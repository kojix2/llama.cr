<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="utf-8" />
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1.0">
<meta name="generator" content="Crystal Docs 1.16.3">
<meta name="crystal_docs.project_version" content="main">
<meta name="crystal_docs.project_name" content="llama">



<link href="css/style.css" rel="stylesheet" type="text/css" />
<script type="text/javascript" src="js/doc.js"></script>

  <meta name="repository-name" content="llama">
  <title>Llama - llama main</title>
  <script type="text/javascript">
    CrystalDocs.base_path = "";
  </script>
</head>
<body>

<svg class="hidden">
  <symbol id="octicon-link" viewBox="0 0 16 16">
    <path fill="currentColor" fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path>
  </symbol>
</svg>
<input type="checkbox" id="sidebar-btn">
<label for="sidebar-btn" id="sidebar-btn-label">
  <svg class="open" xmlns="http://www.w3.org/2000/svg" height="2em" width="2em" viewBox="0 0 512 512"><title>Open Sidebar</title><path fill="currentColor" d="M80 96v64h352V96H80zm0 112v64h352v-64H80zm0 112v64h352v-64H80z"></path></svg>
  <svg class="close" xmlns="http://www.w3.org/2000/svg" width="2em" height="2em" viewBox="0 0 512 512"><title>Close Sidebar</title><path fill="currentColor" d="m118.6 73.4-45.2 45.2L210.7 256 73.4 393.4l45.2 45.2L256 301.3l137.4 137.3 45.2-45.2L301.3 256l137.3-137.4-45.2-45.2L256 210.7Z"></path></svg>
</label>
<div class="sidebar">
  <div class="sidebar-header">
    <div class="search-box">
      <input type="search" class="search-input" placeholder="Search..." spellcheck="false" aria-label="Search">
    </div>

    <div class="project-summary">
      <h1 class="project-name">
        <a href="index.html">
          llama
        </a>
      </h1>

      <span class="project-version">
        main
      </span>
    </div>
  </div>

  <div class="search-results hidden">
    <ul class="search-list"></ul>
  </div>

  <div class="types-list">
    <ul>
  
  <li class="parent open current" data-id="llama/Llama" data-name="llama">
      <a href="Llama.html">Llama</a>
      
        <ul>
  
  <li class="parent " data-id="llama/Llama/AdapterLora" data-name="llama::adapterlora">
      <a href="Llama/AdapterLora.html">AdapterLora</a>
      
        <ul>
  
  <li class=" " data-id="llama/Llama/AdapterLora/Error" data-name="llama::adapterlora::error">
      <a href="Llama/AdapterLora/Error.html">Error</a>
      
    </li>
  
</ul>

      
    </li>
  
  <li class="parent " data-id="llama/Llama/Batch" data-name="llama::batch">
      <a href="Llama/Batch.html">Batch</a>
      
        <ul>
  
  <li class=" " data-id="llama/Llama/Batch/Error" data-name="llama::batch::error">
      <a href="Llama/Batch/Error.html">Error</a>
      
    </li>
  
</ul>

      
    </li>
  
  <li class=" " data-id="llama/Llama/ChatMessage" data-name="llama::chatmessage">
      <a href="Llama/ChatMessage.html">ChatMessage</a>
      
    </li>
  
  <li class="parent " data-id="llama/Llama/Context" data-name="llama::context">
      <a href="Llama/Context.html">Context</a>
      
        <ul>
  
  <li class=" " data-id="llama/Llama/Context/Error" data-name="llama::context::error">
      <a href="Llama/Context/Error.html">Error</a>
      
    </li>
  
  <li class=" " data-id="llama/Llama/Context/TokenizationError" data-name="llama::context::tokenizationerror">
      <a href="Llama/Context/TokenizationError.html">TokenizationError</a>
      
    </li>
  
</ul>

      
    </li>
  
  <li class=" " data-id="llama/Llama/Error" data-name="llama::error">
      <a href="Llama/Error.html">Error</a>
      
    </li>
  
  <li class="parent " data-id="llama/Llama/Memory" data-name="llama::memory">
      <a href="Llama/Memory.html">Memory</a>
      
        <ul>
  
  <li class=" " data-id="llama/Llama/Memory/Error" data-name="llama::memory::error">
      <a href="Llama/Memory/Error.html">Error</a>
      
    </li>
  
</ul>

      
    </li>
  
  <li class="parent " data-id="llama/Llama/Model" data-name="llama::model">
      <a href="Llama/Model.html">Model</a>
      
        <ul>
  
  <li class=" " data-id="llama/Llama/Model/Error" data-name="llama::model::error">
      <a href="Llama/Model/Error.html">Error</a>
      
    </li>
  
</ul>

      
    </li>
  
  <li class="parent " data-id="llama/Llama/Sampler" data-name="llama::sampler">
      <a href="Llama/Sampler.html">Sampler</a>
      
        <ul>
  
  <li class=" " data-id="llama/Llama/Sampler/Base" data-name="llama::sampler::base">
      <a href="Llama/Sampler/Base.html">Base</a>
      
    </li>
  
  <li class=" " data-id="llama/Llama/Sampler/Dist" data-name="llama::sampler::dist">
      <a href="Llama/Sampler/Dist.html">Dist</a>
      
    </li>
  
  <li class=" " data-id="llama/Llama/Sampler/Error" data-name="llama::sampler::error">
      <a href="Llama/Sampler/Error.html">Error</a>
      
    </li>
  
  <li class=" " data-id="llama/Llama/Sampler/Grammar" data-name="llama::sampler::grammar">
      <a href="Llama/Sampler/Grammar.html">Grammar</a>
      
    </li>
  
  <li class=" " data-id="llama/Llama/Sampler/GrammarLazyPatterns" data-name="llama::sampler::grammarlazypatterns">
      <a href="Llama/Sampler/GrammarLazyPatterns.html">GrammarLazyPatterns</a>
      
    </li>
  
  <li class=" " data-id="llama/Llama/Sampler/Greedy" data-name="llama::sampler::greedy">
      <a href="Llama/Sampler/Greedy.html">Greedy</a>
      
    </li>
  
  <li class=" " data-id="llama/Llama/Sampler/Infill" data-name="llama::sampler::infill">
      <a href="Llama/Sampler/Infill.html">Infill</a>
      
    </li>
  
  <li class=" " data-id="llama/Llama/Sampler/MinP" data-name="llama::sampler::minp">
      <a href="Llama/Sampler/MinP.html">MinP</a>
      
    </li>
  
  <li class=" " data-id="llama/Llama/Sampler/Mirostat" data-name="llama::sampler::mirostat">
      <a href="Llama/Sampler/Mirostat.html">Mirostat</a>
      
    </li>
  
  <li class=" " data-id="llama/Llama/Sampler/MirostatV2" data-name="llama::sampler::mirostatv2">
      <a href="Llama/Sampler/MirostatV2.html">MirostatV2</a>
      
    </li>
  
  <li class=" " data-id="llama/Llama/Sampler/Penalties" data-name="llama::sampler::penalties">
      <a href="Llama/Sampler/Penalties.html">Penalties</a>
      
    </li>
  
  <li class=" " data-id="llama/Llama/Sampler/Temp" data-name="llama::sampler::temp">
      <a href="Llama/Sampler/Temp.html">Temp</a>
      
    </li>
  
  <li class=" " data-id="llama/Llama/Sampler/TempExt" data-name="llama::sampler::tempext">
      <a href="Llama/Sampler/TempExt.html">TempExt</a>
      
    </li>
  
  <li class=" " data-id="llama/Llama/Sampler/TokenizationError" data-name="llama::sampler::tokenizationerror">
      <a href="Llama/Sampler/TokenizationError.html">TokenizationError</a>
      
    </li>
  
  <li class=" " data-id="llama/Llama/Sampler/TopK" data-name="llama::sampler::topk">
      <a href="Llama/Sampler/TopK.html">TopK</a>
      
    </li>
  
  <li class=" " data-id="llama/Llama/Sampler/TopNSigma" data-name="llama::sampler::topnsigma">
      <a href="Llama/Sampler/TopNSigma.html">TopNSigma</a>
      
    </li>
  
  <li class=" " data-id="llama/Llama/Sampler/TopP" data-name="llama::sampler::topp">
      <a href="Llama/Sampler/TopP.html">TopP</a>
      
    </li>
  
  <li class=" " data-id="llama/Llama/Sampler/Typical" data-name="llama::sampler::typical">
      <a href="Llama/Sampler/Typical.html">Typical</a>
      
    </li>
  
  <li class=" " data-id="llama/Llama/Sampler/Xtc" data-name="llama::sampler::xtc">
      <a href="Llama/Sampler/Xtc.html">Xtc</a>
      
    </li>
  
</ul>

      
    </li>
  
  <li class=" " data-id="llama/Llama/SamplerChain" data-name="llama::samplerchain">
      <a href="Llama/SamplerChain.html">SamplerChain</a>
      
    </li>
  
  <li class="parent " data-id="llama/Llama/State" data-name="llama::state">
      <a href="Llama/State.html">State</a>
      
        <ul>
  
  <li class=" " data-id="llama/Llama/State/Error" data-name="llama::state::error">
      <a href="Llama/State/Error.html">Error</a>
      
    </li>
  
</ul>

      
    </li>
  
  <li class=" " data-id="llama/Llama/TokenizationError" data-name="llama::tokenizationerror">
      <a href="Llama/TokenizationError.html">TokenizationError</a>
      
    </li>
  
  <li class=" " data-id="llama/Llama/Vocab" data-name="llama::vocab">
      <a href="Llama/Vocab.html">Vocab</a>
      
    </li>
  
</ul>

      
    </li>
  
</ul>

  </div>
</div>


<div class="main-content">
<h1 class="type-name">

  <span class="kind">
    module
  </span> Llama

</h1>




















  <h2>
    <a id="defined-in" class="anchor" href="#defined-in">
  <svg class="octicon-link" aria-hidden="true">
    <use href="#octicon-link"/>
  </svg>
</a>
    Defined in:
  </h2>
  
    
      <a href="https://github.com/kojix2/llama.cr/blob/4d2b8264e170307bbea4f3f7730728f16b0fdcc3/src/llama.cr#L80" target="_blank">
        llama.cr
      </a>
    
    <br/>
  
    
      <a href="https://github.com/kojix2/llama.cr/blob/4d2b8264e170307bbea4f3f7730728f16b0fdcc3/src/llama/adapter_lora.cr#L3" target="_blank">
        llama/adapter_lora.cr
      </a>
    
    <br/>
  
    
      <a href="https://github.com/kojix2/llama.cr/blob/4d2b8264e170307bbea4f3f7730728f16b0fdcc3/src/llama/adapter_lora/error.cr#L1" target="_blank">
        llama/adapter_lora/error.cr
      </a>
    
    <br/>
  
    
      <a href="https://github.com/kojix2/llama.cr/blob/4d2b8264e170307bbea4f3f7730728f16b0fdcc3/src/llama/batch.cr#L3" target="_blank">
        llama/batch.cr
      </a>
    
    <br/>
  
    
      <a href="https://github.com/kojix2/llama.cr/blob/4d2b8264e170307bbea4f3f7730728f16b0fdcc3/src/llama/batch/error.cr#L3" target="_blank">
        llama/batch/error.cr
      </a>
    
    <br/>
  
    
      <a href="https://github.com/kojix2/llama.cr/blob/4d2b8264e170307bbea4f3f7730728f16b0fdcc3/src/llama/chat.cr#L1" target="_blank">
        llama/chat.cr
      </a>
    
    <br/>
  
    
      <a href="https://github.com/kojix2/llama.cr/blob/4d2b8264e170307bbea4f3f7730728f16b0fdcc3/src/llama/context.cr#L3" target="_blank">
        llama/context.cr
      </a>
    
    <br/>
  
    
      <a href="https://github.com/kojix2/llama.cr/blob/4d2b8264e170307bbea4f3f7730728f16b0fdcc3/src/llama/context/error.cr#L3" target="_blank">
        llama/context/error.cr
      </a>
    
    <br/>
  
    
      <a href="https://github.com/kojix2/llama.cr/blob/4d2b8264e170307bbea4f3f7730728f16b0fdcc3/src/llama/error.cr#L1" target="_blank">
        llama/error.cr
      </a>
    
    <br/>
  
    
      <a href="https://github.com/kojix2/llama.cr/blob/4d2b8264e170307bbea4f3f7730728f16b0fdcc3/src/llama/lib_llama.cr#L1" target="_blank">
        llama/lib_llama.cr
      </a>
    
    <br/>
  
    
      <a href="https://github.com/kojix2/llama.cr/blob/4d2b8264e170307bbea4f3f7730728f16b0fdcc3/src/llama/memory.cr#L3" target="_blank">
        llama/memory.cr
      </a>
    
    <br/>
  
    
      <a href="https://github.com/kojix2/llama.cr/blob/4d2b8264e170307bbea4f3f7730728f16b0fdcc3/src/llama/memory/error.cr#L1" target="_blank">
        llama/memory/error.cr
      </a>
    
    <br/>
  
    
      <a href="https://github.com/kojix2/llama.cr/blob/4d2b8264e170307bbea4f3f7730728f16b0fdcc3/src/llama/model.cr#L3" target="_blank">
        llama/model.cr
      </a>
    
    <br/>
  
    
      <a href="https://github.com/kojix2/llama.cr/blob/4d2b8264e170307bbea4f3f7730728f16b0fdcc3/src/llama/model/error.cr#L3" target="_blank">
        llama/model/error.cr
      </a>
    
    <br/>
  
    
      <a href="https://github.com/kojix2/llama.cr/blob/4d2b8264e170307bbea4f3f7730728f16b0fdcc3/src/llama/sampler.cr#L4" target="_blank">
        llama/sampler.cr
      </a>
    
    <br/>
  
    
      <a href="https://github.com/kojix2/llama.cr/blob/4d2b8264e170307bbea4f3f7730728f16b0fdcc3/src/llama/sampler/base.cr#L1" target="_blank">
        llama/sampler/base.cr
      </a>
    
    <br/>
  
    
      <a href="https://github.com/kojix2/llama.cr/blob/4d2b8264e170307bbea4f3f7730728f16b0fdcc3/src/llama/sampler/dist.cr#L1" target="_blank">
        llama/sampler/dist.cr
      </a>
    
    <br/>
  
    
      <a href="https://github.com/kojix2/llama.cr/blob/4d2b8264e170307bbea4f3f7730728f16b0fdcc3/src/llama/sampler/error.cr#L3" target="_blank">
        llama/sampler/error.cr
      </a>
    
    <br/>
  
    
      <a href="https://github.com/kojix2/llama.cr/blob/4d2b8264e170307bbea4f3f7730728f16b0fdcc3/src/llama/sampler/grammar.cr#L1" target="_blank">
        llama/sampler/grammar.cr
      </a>
    
    <br/>
  
    
      <a href="https://github.com/kojix2/llama.cr/blob/4d2b8264e170307bbea4f3f7730728f16b0fdcc3/src/llama/sampler/grammar_lazy_patterns.cr#L1" target="_blank">
        llama/sampler/grammar_lazy_patterns.cr
      </a>
    
    <br/>
  
    
      <a href="https://github.com/kojix2/llama.cr/blob/4d2b8264e170307bbea4f3f7730728f16b0fdcc3/src/llama/sampler/greedy.cr#L1" target="_blank">
        llama/sampler/greedy.cr
      </a>
    
    <br/>
  
    
      <a href="https://github.com/kojix2/llama.cr/blob/4d2b8264e170307bbea4f3f7730728f16b0fdcc3/src/llama/sampler/infill.cr#L1" target="_blank">
        llama/sampler/infill.cr
      </a>
    
    <br/>
  
    
      <a href="https://github.com/kojix2/llama.cr/blob/4d2b8264e170307bbea4f3f7730728f16b0fdcc3/src/llama/sampler/min_p.cr#L1" target="_blank">
        llama/sampler/min_p.cr
      </a>
    
    <br/>
  
    
      <a href="https://github.com/kojix2/llama.cr/blob/4d2b8264e170307bbea4f3f7730728f16b0fdcc3/src/llama/sampler/mirostat.cr#L1" target="_blank">
        llama/sampler/mirostat.cr
      </a>
    
    <br/>
  
    
      <a href="https://github.com/kojix2/llama.cr/blob/4d2b8264e170307bbea4f3f7730728f16b0fdcc3/src/llama/sampler/mirostat_v2.cr#L1" target="_blank">
        llama/sampler/mirostat_v2.cr
      </a>
    
    <br/>
  
    
      <a href="https://github.com/kojix2/llama.cr/blob/4d2b8264e170307bbea4f3f7730728f16b0fdcc3/src/llama/sampler/penalties.cr#L1" target="_blank">
        llama/sampler/penalties.cr
      </a>
    
    <br/>
  
    
      <a href="https://github.com/kojix2/llama.cr/blob/4d2b8264e170307bbea4f3f7730728f16b0fdcc3/src/llama/sampler/temp.cr#L1" target="_blank">
        llama/sampler/temp.cr
      </a>
    
    <br/>
  
    
      <a href="https://github.com/kojix2/llama.cr/blob/4d2b8264e170307bbea4f3f7730728f16b0fdcc3/src/llama/sampler/temp_ext.cr#L1" target="_blank">
        llama/sampler/temp_ext.cr
      </a>
    
    <br/>
  
    
      <a href="https://github.com/kojix2/llama.cr/blob/4d2b8264e170307bbea4f3f7730728f16b0fdcc3/src/llama/sampler/top_k.cr#L1" target="_blank">
        llama/sampler/top_k.cr
      </a>
    
    <br/>
  
    
      <a href="https://github.com/kojix2/llama.cr/blob/4d2b8264e170307bbea4f3f7730728f16b0fdcc3/src/llama/sampler/top_n_sigma.cr#L1" target="_blank">
        llama/sampler/top_n_sigma.cr
      </a>
    
    <br/>
  
    
      <a href="https://github.com/kojix2/llama.cr/blob/4d2b8264e170307bbea4f3f7730728f16b0fdcc3/src/llama/sampler/top_p.cr#L1" target="_blank">
        llama/sampler/top_p.cr
      </a>
    
    <br/>
  
    
      <a href="https://github.com/kojix2/llama.cr/blob/4d2b8264e170307bbea4f3f7730728f16b0fdcc3/src/llama/sampler/typical.cr#L1" target="_blank">
        llama/sampler/typical.cr
      </a>
    
    <br/>
  
    
      <a href="https://github.com/kojix2/llama.cr/blob/4d2b8264e170307bbea4f3f7730728f16b0fdcc3/src/llama/sampler/xtc.cr#L1" target="_blank">
        llama/sampler/xtc.cr
      </a>
    
    <br/>
  
    
      <a href="https://github.com/kojix2/llama.cr/blob/4d2b8264e170307bbea4f3f7730728f16b0fdcc3/src/llama/sampler_chain.cr#L1" target="_blank">
        llama/sampler_chain.cr
      </a>
    
    <br/>
  
    
      <a href="https://github.com/kojix2/llama.cr/blob/4d2b8264e170307bbea4f3f7730728f16b0fdcc3/src/llama/state.cr#L3" target="_blank">
        llama/state.cr
      </a>
    
    <br/>
  
    
      <a href="https://github.com/kojix2/llama.cr/blob/4d2b8264e170307bbea4f3f7730728f16b0fdcc3/src/llama/state/error.cr#L3" target="_blank">
        llama/state/error.cr
      </a>
    
    <br/>
  
    
      <a href="https://github.com/kojix2/llama.cr/blob/4d2b8264e170307bbea4f3f7730728f16b0fdcc3/src/llama/vocab.cr#L3" target="_blank">
        llama/vocab.cr
      </a>
    
    <br/>
  



  
    <h2>
      <a id="constant-summary" class="anchor" href="#constant-summary">
  <svg class="octicon-link" aria-hidden="true">
    <use href="#octicon-link"/>
  </svg>
</a>
      Constant Summary
    </h2>
  
  <dl>
    
      <dt class="entry-const" id="DEFAULT_SEED">
        <strong>DEFAULT_SEED</strong> = <code><span class="t">LibLlama</span><span class="t">::</span><span class="t">LLAMA_DEFAULT_SEED</span></code>
      </dt>
      
      <dd class="entry-const-doc">
        <p>==== Native constants (wrapped for user convenience) ====</p>
      </dd>
      
    
      <dt class="entry-const" id="FILE_MAGIC_GGLA">
        <strong>FILE_MAGIC_GGLA</strong> = <code><span class="t">LibLlama</span><span class="t">::</span><span class="t">LLAMA_FILE_MAGIC_GGLA</span></code>
      </dt>
      
    
      <dt class="entry-const" id="FILE_MAGIC_GGSN">
        <strong>FILE_MAGIC_GGSN</strong> = <code><span class="t">LibLlama</span><span class="t">::</span><span class="t">LLAMA_FILE_MAGIC_GGSN</span></code>
      </dt>
      
    
      <dt class="entry-const" id="FILE_MAGIC_GGSQ">
        <strong>FILE_MAGIC_GGSQ</strong> = <code><span class="t">LibLlama</span><span class="t">::</span><span class="t">LLAMA_FILE_MAGIC_GGSQ</span></code>
      </dt>
      
    
      <dt class="entry-const" id="LLAMA_CPP_COMPATIBLE_VERSION">
        <strong>LLAMA_CPP_COMPATIBLE_VERSION</strong> = <code>(read_file(<span class="s">&quot;/home/runner/work/llama.cr/llama.cr/src/LLAMA_VERSION&quot;</span>)).chomp</code>
      </dt>
      
    
      <dt class="entry-const" id="LOG_LEVEL_DEBUG">
        <strong>LOG_LEVEL_DEBUG</strong> = <code><span class="n">0</span></code>
      </dt>
      
      <dd class="entry-const-doc">
        <p>Log level constants (from llama.cpp / ggml)</p>
      </dd>
      
    
      <dt class="entry-const" id="LOG_LEVEL_ERROR">
        <strong>LOG_LEVEL_ERROR</strong> = <code><span class="n">3</span></code>
      </dt>
      
    
      <dt class="entry-const" id="LOG_LEVEL_INFO">
        <strong>LOG_LEVEL_INFO</strong> = <code><span class="n">1</span></code>
      </dt>
      
    
      <dt class="entry-const" id="LOG_LEVEL_NONE">
        <strong>LOG_LEVEL_NONE</strong> = <code><span class="n">4</span></code>
      </dt>
      
    
      <dt class="entry-const" id="LOG_LEVEL_WARNING">
        <strong>LOG_LEVEL_WARNING</strong> = <code><span class="n">2</span></code>
      </dt>
      
    
      <dt class="entry-const" id="SESSION_MAGIC">
        <strong>SESSION_MAGIC</strong> = <code><span class="t">LibLlama</span><span class="t">::</span><span class="t">LLAMA_SESSION_MAGIC</span></code>
      </dt>
      
    
      <dt class="entry-const" id="SESSION_VERSION">
        <strong>SESSION_VERSION</strong> = <code><span class="t">LibLlama</span><span class="t">::</span><span class="t">LLAMA_SESSION_VERSION</span></code>
      </dt>
      
    
      <dt class="entry-const" id="TOKEN_NULL">
        <strong>TOKEN_NULL</strong> = <code><span class="t">LibLlama</span><span class="t">::</span><span class="t">LLAMA_TOKEN_NULL</span></code>
      </dt>
      
    
      <dt class="entry-const" id="VERSION">
        <strong>VERSION</strong> = <code><span class="o">{{</span> (<span class="s">`shards version /home/runner/work/llama.cr/llama.cr/src`</span>).chomp.stringify }}</code>
      </dt>
      
    
  </dl>



  

  
  <h2>
    <a id="class-method-summary" class="anchor" href="#class-method-summary">
  <svg class="octicon-link" aria-hidden="true">
    <use href="#octicon-link"/>
  </svg>
</a>
    Class Method Summary
  </h2>
  <ul class="list-summary">
    
      <li class="entry-summary">
        <a href="#apply_chat_template%28template%3AString%7CNil%2Cmessages%3AArray%28ChatMessage%29%2Cadd_assistant%3ABool%3Dtrue%29%3AString-class-method" class="signature"><strong>.apply_chat_template</strong>(template : String | Nil, messages : Array(ChatMessage), add_assistant : Bool = <span class="n">true</span>) : String</a>
        
          <div class="summary"><p>Applies a chat template to a list of messages</p></div>
        
      </li>
    
      <li class="entry-summary">
        <a href="#builtin_chat_templates%3AArray%28String%29-class-method" class="signature"><strong>.builtin_chat_templates</strong> : Array(String)</a>
        
          <div class="summary"><p>Gets the list of built-in chat templates</p></div>
        
      </li>
    
      <li class="entry-summary">
        <a href="#error_message%28code%3AInt32%29%3AString-class-method" class="signature"><strong>.error_message</strong>(code : Int32) : String</a>
        
      </li>
    
      <li class="entry-summary">
        <a href="#format_error%28message%3AString%2Ccode%3AInt32%7CNil%3Dnil%2Ccontext%3AString%7CNil%3Dnil%29%3AString-class-method" class="signature"><strong>.format_error</strong>(message : String, code : Int32 | Nil = <span class="n">nil</span>, context : String | Nil = <span class="n">nil</span>) : String</a>
        
      </li>
    
      <li class="entry-summary">
        <a href="#generate%28model_path%3AString%2Cprompt%3AString%2Cmax_tokens%3AInt32%3D128%2Ctemperature%3AFloat32%3D0.8%29%3AString-class-method" class="signature"><strong>.generate</strong>(model_path : String, prompt : String, max_tokens : Int32 = <span class="n">128</span>, temperature : Float32 = <span class="n">0.8</span>) : String</a>
        
          <div class="summary"><p>Generates text from a prompt using a model</p></div>
        
      </li>
    
      <li class="entry-summary">
        <a href="#init-class-method" class="signature"><strong>.init</strong></a>
        
          <div class="summary"><p>Thread-safe, idempotent initialization of the llama.cpp backend.</p></div>
        
      </li>
    
      <li class="entry-summary">
        <a href="#log_level-class-method" class="signature"><strong>.log_level</strong></a>
        
          <div class="summary"><p>Get the current log level</p></div>
        
      </li>
    
      <li class="entry-summary">
        <a href="#log_level%3D%28level%3AInt32%29-class-method" class="signature"><strong>.log_level=</strong>(level : Int32)</a>
        
          <div class="summary"><p>Set the log level</p></div>
        
      </li>
    
      <li class="entry-summary">
        <a href="#log_set%28%26block%3AInt32%2CString-%3E%29-class-method" class="signature"><strong>.log_set</strong>(&block : Int32, String -> )</a>
        
          <div class="summary"><p>Set a custom log callback</p></div>
        
      </li>
    
      <li class="entry-summary">
        <a href="#measure_ms%28%26%29-class-method" class="signature"><strong>.measure_ms</strong>(&)</a>
        
          <div class="summary"><p>Measures elapsed time in milliseconds for a block using llama.cpp's clock.</p></div>
        
      </li>
    
      <li class="entry-summary">
        <a href="#process_escapes%28text%3AString%29%3AString-class-method" class="signature"><strong>.process_escapes</strong>(text : String) : String</a>
        
          <div class="summary"><p>Process escape sequences in a string</p></div>
        
      </li>
    
      <li class="entry-summary">
        <a href="#system_info%3AString-class-method" class="signature"><strong>.system_info</strong> : String</a>
        
          <div class="summary"><p>Returns the llama.cpp system information</p></div>
        
      </li>
    
      <li class="entry-summary">
        <a href="#time_ms%3AInt64-class-method" class="signature"><strong>.time_ms</strong> : Int64</a>
        
          <div class="summary"><p>Returns the current time in milliseconds since the Unix epoch (llama.cpp compatible).</p></div>
        
      </li>
    
      <li class="entry-summary">
        <a href="#time_us%3AInt64-class-method" class="signature"><strong>.time_us</strong> : Int64</a>
        
          <div class="summary"><p>Returns the current time in microseconds since the Unix epoch (llama.cpp compatible).</p></div>
        
      </li>
    
      <li class="entry-summary">
        <a href="#tokenize_and_format%28vocab%3AVocab%2Ctext%3AString%2Cadd_bos%3ABool%3Dtrue%2Cparse_special%3ABool%3Dtrue%2Cids_only%3ABool%3Dfalse%29%3AString-class-method" class="signature"><strong>.tokenize_and_format</strong>(vocab : Vocab, text : String, add_bos : Bool = <span class="n">true</span>, parse_special : Bool = <span class="n">true</span>, ids_only : Bool = <span class="n">false</span>) : String</a>
        
          <div class="summary"><p>Tokenize text and return formatted output</p></div>
        
      </li>
    
      <li class="entry-summary">
        <a href="#uninit-class-method" class="signature"><strong>.uninit</strong></a>
        
          <div class="summary"><p>Thread-safe, idempotent finalization of the llama.cpp backend.</p></div>
        
      </li>
    
  </ul>


  

  


  <div class="methods-inherited">
    
  </div>

  

  
  <h2>
    <a id="class-method-detail" class="anchor" href="#class-method-detail">
  <svg class="octicon-link" aria-hidden="true">
    <use href="#octicon-link"/>
  </svg>
</a>
    Class Method Detail
  </h2>
  
    <div class="entry-detail" id="apply_chat_template(template:String|Nil,messages:Array(ChatMessage),add_assistant:Bool=true):String-class-method">
      <div class="signature">
        
        def self.<strong>apply_chat_template</strong>(template : String | Nil, messages : Array(<a href="Llama/ChatMessage.html">ChatMessage</a>), add_assistant : Bool = <span class="n">true</span>) : String

        <a class="method-permalink" href="#apply_chat_template%28template%3AString%7CNil%2Cmessages%3AArray%28ChatMessage%29%2Cadd_assistant%3ABool%3Dtrue%29%3AString-class-method">#</a>
      </div>
      
        <div class="doc">
          
          <p>Applies a chat template to a list of messages</p>
<p>Parameters:</p>
<ul>
<li>template: The template string (nil to use model's default)</li>
<li>messages: Array of chat messages</li>
<li>add_assistant: Whether to end with an assistant message prefix</li>
</ul>
<p>Returns:</p>
<ul>
<li>The formatted prompt string</li>
</ul>
<p>Raises:</p>
<ul>
<li>Llama::Error if template application fails</li>
</ul>
        </div>
      
      <br/>
      <div>
        
          [<a href="https://github.com/kojix2/llama.cr/blob/4d2b8264e170307bbea4f3f7730728f16b0fdcc3/src/llama/chat.cr#L39" target="_blank">View source</a>]
        
      </div>
    </div>
  
    <div class="entry-detail" id="builtin_chat_templates:Array(String)-class-method">
      <div class="signature">
        
        def self.<strong>builtin_chat_templates</strong> : Array(String)

        <a class="method-permalink" href="#builtin_chat_templates%3AArray%28String%29-class-method">#</a>
      </div>
      
        <div class="doc">
          
          <p>Gets the list of built-in chat templates</p>
<p>Returns:</p>
<ul>
<li>Array of template names</li>
</ul>
        </div>
      
      <br/>
      <div>
        
          [<a href="https://github.com/kojix2/llama.cr/blob/4d2b8264e170307bbea4f3f7730728f16b0fdcc3/src/llama/chat.cr#L85" target="_blank">View source</a>]
        
      </div>
    </div>
  
    <div class="entry-detail" id="error_message(code:Int32):String-class-method">
      <div class="signature">
        
        def self.<strong>error_message</strong>(code : Int32) : String

        <a class="method-permalink" href="#error_message%28code%3AInt32%29%3AString-class-method">#</a>
      </div>
      
      <br/>
      <div>
        
          [<a href="https://github.com/kojix2/llama.cr/blob/4d2b8264e170307bbea4f3f7730728f16b0fdcc3/src/llama/error.cr#L38" target="_blank">View source</a>]
        
      </div>
    </div>
  
    <div class="entry-detail" id="format_error(message:String,code:Int32|Nil=nil,context:String|Nil=nil):String-class-method">
      <div class="signature">
        
        def self.<strong>format_error</strong>(message : String, code : Int32 | Nil = <span class="n">nil</span>, context : String | Nil = <span class="n">nil</span>) : String

        <a class="method-permalink" href="#format_error%28message%3AString%2Ccode%3AInt32%7CNil%3Dnil%2Ccontext%3AString%7CNil%3Dnil%29%3AString-class-method">#</a>
      </div>
      
      <br/>
      <div>
        
          [<a href="https://github.com/kojix2/llama.cr/blob/4d2b8264e170307bbea4f3f7730728f16b0fdcc3/src/llama/error.cr#L42" target="_blank">View source</a>]
        
      </div>
    </div>
  
    <div class="entry-detail" id="generate(model_path:String,prompt:String,max_tokens:Int32=128,temperature:Float32=0.8):String-class-method">
      <div class="signature">
        
        def self.<strong>generate</strong>(model_path : String, prompt : String, max_tokens : Int32 = <span class="n">128</span>, temperature : Float32 = <span class="n">0.8</span>) : String

        <a class="method-permalink" href="#generate%28model_path%3AString%2Cprompt%3AString%2Cmax_tokens%3AInt32%3D128%2Ctemperature%3AFloat32%3D0.8%29%3AString-class-method">#</a>
      </div>
      
        <div class="doc">
          
          <p>Generates text from a prompt using a model</p>
<p>This is a convenience method that loads a model, creates a context,
and generates text in a single call.</p>
<pre><code class="language-crystal">response <span class="o">=</span> <span class="t">Llama</span>.generate(
  <span class="s">&quot;/path/to/model.gguf&quot;</span>,
  <span class="s">&quot;Once upon a time&quot;</span>,
  max_tokens: <span class="n">100</span>,
  temperature: <span class="n">0.7</span>
)
puts response</code></pre>
<p>Parameters:</p>
<ul>
<li>model_path: Path to the model file (.gguf format)</li>
<li>prompt: The input prompt</li>
<li>max_tokens: Maximum number of tokens to generate (must be positive)</li>
<li>temperature: Sampling temperature (0.0 = greedy, 1.0 = more random)</li>
</ul>
<p>Returns:</p>
<ul>
<li>The generated text</li>
</ul>
<p>Raises:</p>
<ul>
<li>ArgumentError if parameters are invalid</li>
<li>Llama::Model::Error if model loading fails</li>
<li>Llama::Context::Error if text generation fails</li>
</ul>
        </div>
      
      <br/>
      <div>
        
          [<a href="https://github.com/kojix2/llama.cr/blob/4d2b8264e170307bbea4f3f7730728f16b0fdcc3/src/llama.cr#L273" target="_blank">View source</a>]
        
      </div>
    </div>
  
    <div class="entry-detail" id="init-class-method">
      <div class="signature">
        
        def self.<strong>init</strong>

        <a class="method-permalink" href="#init-class-method">#</a>
      </div>
      
        <div class="doc">
          
          <p>Thread-safe, idempotent initialization of the llama.cpp backend.
You do not need to call this manually in most cases.</p>
        </div>
      
      <br/>
      <div>
        
          [<a href="https://github.com/kojix2/llama.cr/blob/4d2b8264e170307bbea4f3f7730728f16b0fdcc3/src/llama.cr#L285" target="_blank">View source</a>]
        
      </div>
    </div>
  
    <div class="entry-detail" id="log_level-class-method">
      <div class="signature">
        
        def self.<strong>log_level</strong>

        <a class="method-permalink" href="#log_level-class-method">#</a>
      </div>
      
        <div class="doc">
          
          <p>Get the current log level</p>
<p>Returns:</p>
<ul>
<li>The current log level</li>
</ul>
        </div>
      
      <br/>
      <div>
        
          [<a href="https://github.com/kojix2/llama.cr/blob/4d2b8264e170307bbea4f3f7730728f16b0fdcc3/src/llama.cr#L125" target="_blank">View source</a>]
        
      </div>
    </div>
  
    <div class="entry-detail" id="log_level=(level:Int32)-class-method">
      <div class="signature">
        
        def self.<strong>log_level=</strong>(level : Int32)

        <a class="method-permalink" href="#log_level%3D%28level%3AInt32%29-class-method">#</a>
      </div>
      
        <div class="doc">
          
          <p>Set the log level</p>
<p>Parameters:</p>
<ul>
<li>level : Int32 - log level (0=DEBUG, 1=INFO, 2=WARNING, 3=ERROR, 4=NONE)</li>
</ul>
<p>Example:
Llama.log_level = Llama::LOG_LEVEL_ERROR  # Only show errors
Llama.log_level = Llama::LOG_LEVEL_NONE   # Disable all logging</p>
        </div>
      
      <br/>
      <div>
        
          [<a href="https://github.com/kojix2/llama.cr/blob/4d2b8264e170307bbea4f3f7730728f16b0fdcc3/src/llama.cr#L116" target="_blank">View source</a>]
        
      </div>
    </div>
  
    <div class="entry-detail" id="log_set(&amp;block:Int32,String-&gt;)-class-method">
      <div class="signature">
        
        def self.<strong>log_set</strong>(&block : Int32, String -> )

        <a class="method-permalink" href="#log_set%28%26block%3AInt32%2CString-%3E%29-class-method">#</a>
      </div>
      
        <div class="doc">
          
          <p>Set a custom log callback</p>
<p>The block receives:</p>
<ul>
<li>level : Int32 - log level (0=DEBUG, 1=INFO, 2=WARNING, 3=ERROR)</li>
<li>message : String - log message</li>
</ul>
<p>Example:
Llama.log_set do |level, message|
if level &gt;= Llama::LOG_LEVEL_ERROR
STDERR.print message
end
end</p>
        </div>
      
      <br/>
      <div>
        
          [<a href="https://github.com/kojix2/llama.cr/blob/4d2b8264e170307bbea4f3f7730728f16b0fdcc3/src/llama.cr#L148" target="_blank">View source</a>]
        
      </div>
    </div>
  
    <div class="entry-detail" id="measure_ms(&amp;)-class-method">
      <div class="signature">
        
        def self.<strong>measure_ms</strong>(&)

        <a class="method-permalink" href="#measure_ms%28%26%29-class-method">#</a>
      </div>
      
        <div class="doc">
          
          <p>Measures elapsed time in milliseconds for a block using llama.cpp's clock.</p>
<pre><code class="language-crystal">elapsed <span class="o">=</span> <span class="t">Llama</span>.measure_ms <span class="k">do</span>
  <span class="c"># ... code to measure ...</span>
<span class="k">end</span>
puts <span class="s">&quot;Elapsed: </span><span class="i">#{</span>elapsed<span class="i">}</span><span class="s"> ms&quot;</span></code></pre>
<p>Returns:</p>
<ul>
<li>Float64: elapsed milliseconds</li>
</ul>
        </div>
      
      <br/>
      <div>
        
          [<a href="https://github.com/kojix2/llama.cr/blob/4d2b8264e170307bbea4f3f7730728f16b0fdcc3/src/llama.cr#L380" target="_blank">View source</a>]
        
      </div>
    </div>
  
    <div class="entry-detail" id="process_escapes(text:String):String-class-method">
      <div class="signature">
        
        def self.<strong>process_escapes</strong>(text : String) : String

        <a class="method-permalink" href="#process_escapes%28text%3AString%29%3AString-class-method">#</a>
      </div>
      
        <div class="doc">
          
          <p>Process escape sequences in a string</p>
<p>This method processes common escape sequences like \n, \t, etc.
in a string, converting them to their actual character representations.</p>
<pre><code class="language-crystal">text <span class="o">=</span> <span class="t">Llama</span>.process_escapes(<span class="s">&quot;Hello\\nWorld&quot;</span>)
puts text <span class="c"># Prints &quot;Hello&quot; and &quot;World&quot; on separate lines</span></code></pre>
<p>Parameters:</p>
<ul>
<li>text: The input string containing escape sequences</li>
</ul>
<p>Returns:</p>
<ul>
<li>A new string with escape sequences processed</li>
</ul>
        </div>
      
      <br/>
      <div>
        
          [<a href="https://github.com/kojix2/llama.cr/blob/4d2b8264e170307bbea4f3f7730728f16b0fdcc3/src/llama.cr#L196" target="_blank">View source</a>]
        
      </div>
    </div>
  
    <div class="entry-detail" id="system_info:String-class-method">
      <div class="signature">
        
        def self.<strong>system_info</strong> : String

        <a class="method-permalink" href="#system_info%3AString-class-method">#</a>
      </div>
      
        <div class="doc">
          
          <p>Returns the llama.cpp system information</p>
<p>This method provides information about the llama.cpp build,
including BLAS configuration, CPU features, and GPU support.</p>
<pre><code class="language-crystal">info <span class="o">=</span> <span class="t">Llama</span>.system_info
puts info</code></pre>
<p>Returns:</p>
<ul>
<li>A string containing system information</li>
</ul>
        </div>
      
      <br/>
      <div>
        
          [<a href="https://github.com/kojix2/llama.cr/blob/4d2b8264e170307bbea4f3f7730728f16b0fdcc3/src/llama.cr#L175" target="_blank">View source</a>]
        
      </div>
    </div>
  
    <div class="entry-detail" id="time_ms:Int64-class-method">
      <div class="signature">
        
        def self.<strong>time_ms</strong> : Int64

        <a class="method-permalink" href="#time_ms%3AInt64-class-method">#</a>
      </div>
      
        <div class="doc">
          
          <p>Returns the current time in milliseconds since the Unix epoch (llama.cpp compatible).</p>
<pre><code class="language-crystal">t0 <span class="o">=</span> <span class="t">Llama</span>.time_ms
<span class="c"># ... some processing ...</span>
t1 <span class="o">=</span> <span class="t">Llama</span>.time_ms
elapsed <span class="o">=</span> t1 <span class="o">-</span> t0
puts <span class="s">&quot;Elapsed: </span><span class="i">#{</span>elapsed<span class="i">}</span><span class="s"> ms&quot;</span></code></pre>
<p>Returns:</p>
<ul>
<li>Int64: milliseconds since epoch</li>
</ul>
        </div>
      
      <br/>
      <div>
        
          [<a href="https://github.com/kojix2/llama.cr/blob/4d2b8264e170307bbea4f3f7730728f16b0fdcc3/src/llama.cr#L365" target="_blank">View source</a>]
        
      </div>
    </div>
  
    <div class="entry-detail" id="time_us:Int64-class-method">
      <div class="signature">
        
        def self.<strong>time_us</strong> : Int64

        <a class="method-permalink" href="#time_us%3AInt64-class-method">#</a>
      </div>
      
        <div class="doc">
          
          <p>Returns the current time in microseconds since the Unix epoch (llama.cpp compatible).</p>
<p>This is a high-level wrapper for LibLlama.llama_time_us.</p>
<pre><code class="language-crystal">t0 <span class="o">=</span> <span class="t">Llama</span>.time_us
<span class="c"># ... some processing ...</span>
t1 <span class="o">=</span> <span class="t">Llama</span>.time_us
elapsed_ms <span class="o">=</span> (t1 <span class="o">-</span> t0) <span class="o">/</span> <span class="n">1000.0</span>
puts <span class="s">&quot;Elapsed: </span><span class="i">#{</span>elapsed_ms<span class="i">}</span><span class="s"> ms&quot;</span></code></pre>
<p>Returns:</p>
<ul>
<li>Int64: microseconds since epoch</li>
</ul>
        </div>
      
      <br/>
      <div>
        
          [<a href="https://github.com/kojix2/llama.cr/blob/4d2b8264e170307bbea4f3f7730728f16b0fdcc3/src/llama.cr#L349" target="_blank">View source</a>]
        
      </div>
    </div>
  
    <div class="entry-detail" id="tokenize_and_format(vocab:Vocab,text:String,add_bos:Bool=true,parse_special:Bool=true,ids_only:Bool=false):String-class-method">
      <div class="signature">
        
        def self.<strong>tokenize_and_format</strong>(vocab : <a href="Llama/Vocab.html">Vocab</a>, text : String, add_bos : Bool = <span class="n">true</span>, parse_special : Bool = <span class="n">true</span>, ids_only : Bool = <span class="n">false</span>) : String

        <a class="method-permalink" href="#tokenize_and_format%28vocab%3AVocab%2Ctext%3AString%2Cadd_bos%3ABool%3Dtrue%2Cparse_special%3ABool%3Dtrue%2Cids_only%3ABool%3Dfalse%29%3AString-class-method">#</a>
      </div>
      
        <div class="doc">
          
          <p>Tokenize text and return formatted output</p>
<p>This is a convenience method that tokenizes text and returns
a formatted string representation of the tokens.</p>
<pre><code class="language-crystal">model <span class="o">=</span> <span class="t">Llama</span><span class="t">::</span><span class="t">Model</span>.new(<span class="s">&quot;/path/to/model.gguf&quot;</span>)
result <span class="o">=</span> <span class="t">Llama</span>.tokenize_and_format(model.vocab, <span class="s">&quot;Hello, world!&quot;</span>, ids_only: <span class="n">true</span>)
puts result <span class="c"># Prints &quot;[1, 2, 3, ...]&quot;</span></code></pre>
<p>Parameters:</p>
<ul>
<li>vocab: The vocabulary to use for tokenization</li>
<li>text: The text to tokenize</li>
<li>add_bos: Whether to add BOS token (default: true)</li>
<li>parse_special: Whether to parse special tokens (default: true)</li>
<li>ids_only: Whether to return only token IDs (default: false)</li>
</ul>
<p>Returns:</p>
<ul>
<li>A formatted string representation of the tokens</li>
</ul>
        </div>
      
      <br/>
      <div>
        
          [<a href="https://github.com/kojix2/llama.cr/blob/4d2b8264e170307bbea4f3f7730728f16b0fdcc3/src/llama.cr#L229" target="_blank">View source</a>]
        
      </div>
    </div>
  
    <div class="entry-detail" id="uninit-class-method">
      <div class="signature">
        
        def self.<strong>uninit</strong>

        <a class="method-permalink" href="#uninit-class-method">#</a>
      </div>
      
        <div class="doc">
          
          <p>Thread-safe, idempotent finalization of the llama.cpp backend.
Call this if you want to explicitly release all backend resources before program exit.</p>
        </div>
      
      <br/>
      <div>
        
          [<a href="https://github.com/kojix2/llama.cr/blob/4d2b8264e170307bbea4f3f7730728f16b0fdcc3/src/llama.cr#L326" target="_blank">View source</a>]
        
      </div>
    </div>
  


  

  


</div>

</body>
</html>
