name: examples

on:
  push:
  pull_request:
  workflow_dispatch:

jobs:
  build:
    strategy:
      fail-fast: false
      matrix:
        os: [ubuntu-latest, macos-latest]

    runs-on: ${{ matrix.os }}

    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Set up Crystal
        uses: crystal-lang/install-crystal@v1
        with:
          crystal: latest

      - name: Install dependencies (Ubuntu)
        if: runner.os == 'Linux'
        run: |
          sudo apt-get update
          sudo apt-get install -y cmake

      - name: Install dependencies (macOS)
        if: runner.os == 'macOS'
        run: |
          brew install cmake

      - name: Set llama.cpp version
        id: set_version
        run: |
          echo "LLAMA_VERSION=$(cat LLAMA_VERSION)" >> $GITHUB_ENV

      - name: Install llama.cpp (Ubuntu)
        if: runner.os == 'Linux'
        run: |
          curl -L https://github.com/ggml-org/llama.cpp/releases/download/${{ env.LLAMA_VERSION }}/llama-${{ env.LLAMA_VERSION }}-bin-ubuntu-x64.zip -o llama.zip
          unzip llama.zip
          sudo mkdir -p /usr/local/lib
          sudo cp build/bin/*.so /usr/local/lib/
          sudo ldconfig
      - name: Install llama.cpp (macOS)
        if: runner.os == 'macOS'
        run: |
          curl -L https://github.com/ggml-org/llama.cpp/releases/download/${{ env.LLAMA_VERSION }}/llama-${{ env.LLAMA_VERSION }}-bin-macos-arm64.zip -o llama.zip
          unzip llama.zip
          sudo mkdir -p /usr/local/lib
          sudo cp build/bin/*.dylib /usr/local/lib/

      # Set the DYLD_LIBRARY_PATH for macOS to include the installed library path
      - name: Set library path (macOS)
        if: runner.os == 'macOS'
        run: echo 'DYLD_LIBRARY_PATH=/usr/local/lib' >> $GITHUB_ENV

      - name: Install dependencies
        run: shards install

      - name: Cache model files
        id: cache-models
        uses: actions/cache@v4
        with:
          path: models
          key: ${{ runner.os }}-models-tinyllama-v2

      - name: Download test model (if not cached)
        if: steps.cache-models.outputs.cache-hit != 'true'
        run: |
          mkdir -p models
          curl -L https://huggingface.co/TheBloke/TinyLlama-1.1B-Chat-v1.0-GGUF/resolve/main/tinyllama-1.1b-chat-v1.0.Q4_K_M.gguf -o models/tiny_model.gguf

      - name: Build examples
        run: |
          crystal build examples/simple.cr -o simple_example
          crystal build examples/chat.cr -o chat_example
          crystal build examples/tokenize.cr -o tokenize_example

      - name: Run simple example
        run: ./simple_example --model models/tiny_model.gguf

      - name: Run chat example
        run: ./chat_example --model models/tiny_model.gguf

      - name: Run tokenization example
        run: ./tokenize_example --model models/tiny_model.gguf --prompt "Hello, world!"
