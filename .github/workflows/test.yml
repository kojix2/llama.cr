name: test

on:
  push:
  pull_request:
  workflow_dispatch:

jobs:
  test:
    strategy:
      fail-fast: false
      matrix:
        os: [ubuntu-latest, macos-latest]
        spec_file:
          - spec/llama_spec.cr
          - spec/context_spec.cr
          - spec/state_spec.cr
          - spec/batch_spec.cr
          - spec/sampler_spec.cr
          - spec/vocab_spec.cr
          - spec/model_spec.cr
          - spec/adapter_lora_spec.cr
          - spec/context_lora_spec.cr

    runs-on: ${{ matrix.os }}

    steps:
      - uses: actions/checkout@v4

      - name: Install Crystal
        uses: crystal-lang/install-crystal@v1

      - name: Install dependencies (Ubuntu)
        if: runner.os == 'Linux'
        run: |
          sudo apt-get update
          sudo apt-get install -y cmake

      - name: Install dependencies (macOS)
        if: runner.os == 'macOS'
        run: |
          brew install cmake

      - name: Set llama.cpp version
        id: set_version
        run: |
          echo "LLAMA_VERSION=$(cat LLAMA_VERSION)" >> $GITHUB_ENV

      - name: Install llama.cpp (Ubuntu)
        if: runner.os == 'Linux'
        run: |
          curl -L https://github.com/ggml-org/llama.cpp/releases/download/${{ env.LLAMA_VERSION }}/llama-${{ env.LLAMA_VERSION }}-bin-ubuntu-x64.zip -o llama.zip
          unzip llama.zip
          sudo mkdir -p /usr/local/lib
          sudo cp build/bin/*.so /usr/local/lib/
          sudo ldconfig

      - name: Install llama.cpp (macOS)
        if: runner.os == 'macOS'
        run: |
          curl -L https://github.com/ggml-org/llama.cpp/releases/download/${{ env.LLAMA_VERSION }}/llama-${{ env.LLAMA_VERSION }}-bin-macos-arm64.zip -o llama.zip
          unzip llama.zip
          sudo mkdir -p /usr/local/lib
          sudo cp build/bin/*.dylib /usr/local/lib/

      - name: Set library path (macOS)
        if: runner.os == 'macOS'
        run: echo 'DYLD_LIBRARY_PATH=/usr/local/lib' >> $GITHUB_ENV

      - name: Install dependencies
        run: shards install

      - name: Build
        run: crystal build --no-codegen src/llama.cr

      - name: Cache model files
        id: cache-models
        uses: actions/cache@v4
        with:
          path: models
          key: ${{ runner.os }}-models-tinyllama-v2

      - name: Download test model (if not cached)
        if: steps.cache-models.outputs.cache-hit != 'true'
        run: |
          mkdir -p models
          curl -L https://huggingface.co/TheBloke/TinyLlama-1.1B-Chat-v1.0-GGUF/resolve/main/tinyllama-1.1b-chat-v1.0.Q4_K_M.gguf -o models/tiny_model.gguf

      - name: Run spec file ${{ matrix.spec_file }}
        env:
          CRYSTAL_OPTS: --error-trace
          MODEL_PATH: models/tiny_model.gguf
          LLAMA_CPP_DIR: ${{ github.workspace }}
          GC_DEBUG: "1"
        run: |
          echo "Running ${{ matrix.spec_file }}"
          echo "Model path: $MODEL_PATH"
          echo "LLAMA_CPP_DIR: $LLAMA_CPP_DIR"
          echo "Working directory: $(pwd)"
          crystal spec "${{ matrix.spec_file }}"

      - name: Run static analysis
        run: crystal tool format --check
