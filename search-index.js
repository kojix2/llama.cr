crystal_doc_search_index_callback({"repository_name":"llama","body":"# llama.cr\n\n[![test](https://github.com/kojix2/llama.cr/actions/workflows/test.yml/badge.svg)](https://github.com/kojix2/llama.cr/actions/workflows/test.yml)\n[![docs](https://img.shields.io/badge/docs-latest-blue.svg)](https://kojix2.github.io/llama.cr)\n[![Lines of Code](https://img.shields.io/endpoint?url=https%3A%2F%2Ftokei.kojix2.net%2Fbadge%2Fgithub%2Fkojix2%2Fllama.cr%2Flines)](https://tokei.kojix2.net/github/kojix2/llama.cr)\n\nCrystal bindings for [llama.cpp](https://github.com/ggml-org/llama.cpp), a C/C++ implementation of LLaMA, Falcon, GPT-2, and other large language models.\n\nPlease check the [LLAMA_VERSION](LLAMA_VERSION) file for the current compatible version of llama.cpp.\n\nThis project is under active development and may change rapidly.\n\n## Features\n\n- Low-level bindings to the llama.cpp C API\n- High-level Crystal wrapper classes for easy usage\n- Memory management for C resources\n- Simple text generation interface\n- Advanced sampling methods (Min-P, Typical, Mirostat, etc.)\n- Batch processing for efficient token handling\n- KV cache management for optimized inference\n- State saving and loading\n\n## Installation\n\n### Prerequisites\n\nYou need to have llama.cpp compiled and installed on your system:\n\n```bash\ngit clone https://github.com/ggml-org/llama.cpp.git\ncd llama.cpp\nmkdir build && cd build\ncmake ..\ncmake --build . --config Release\nsudo cmake --install .\nsudo ldconfig\n```\n\nAlternatively, you can specify the library location without installing:\n\n```bash\ncrystal build examples/simple.cr --link-flags=\"-L/path/to/llama.cpp/build/bin\"\nLD_LIBRARY_PATH=/path/to/llama.cpp/build/bin ./simple /path/to/model.gguf \"Your prompt here\"\n```\n\n### Obtaining GGUF Model Files\n\nYou'll need a model file in GGUF format. For testing, smaller quantized models (1-3B parameters) with Q4_K_M quantization are recommended.\n\nPopular options:\n\n- [TinyLlama 1.1B](https://huggingface.co/TheBloke/TinyLlama-1.1B-Chat-v1.0-GGUF)\n- [Llama 3 8B Instruct](https://huggingface.co/TheBloke/Llama-3-8B-Instruct-GGUF)\n- [Mistral 7B Instruct v0.2](https://huggingface.co/TheBloke/Mistral-7B-Instruct-v0.2-GGUF)\n\n### Adding to Your Project\n\nAdd the dependency to your `shard.yml`:\n\n```yaml\ndependencies:\n  llama:\n    github: kojix2/llama.cr\n```\n\nThen run `shards install`.\n\n## Usage\n\n### Basic Text Generation\n\n```crystal\nrequire \"llama\"\n\n# Load a model\nmodel = Llama::Model.new(\"/path/to/model.gguf\")\n\n# Create a context\ncontext = model.context\n\n# Generate text\nresponse = context.generate(\"Once upon a time\", max_tokens: 100, temperature: 0.8)\nputs response\n\n# Or use the convenience method\nresponse = Llama.generate(\"/path/to/model.gguf\", \"Once upon a time\")\nputs response\n```\n\n### Advanced Sampling\n\n```crystal\nrequire \"llama\"\n\nmodel = Llama::Model.new(\"/path/to/model.gguf\")\ncontext = model.context\n\n# Create a sampler chain with multiple sampling methods\nchain = Llama::Sampler::Chain.new\nchain.add(Llama::Sampler::TopK.new(40))\nchain.add(Llama::Sampler::MinP.new(0.05, 1))\nchain.add(Llama::Sampler::Temp.new(0.8))\nchain.add(Llama::Sampler::Dist.new(42))\n\n# Generate text with the custom sampler chain\nresult = context.generate_with_sampler(\"Write a short poem about AI:\", chain, 150)\nputs result\n```\n\n### Chat Conversations\n\n```crystal\nrequire \"llama\"\nrequire \"llama/chat\"\n\nmodel = Llama::Model.new(\"/path/to/model.gguf\")\ncontext = model.context\n\n# Create a chat conversation\nmessages = [\n  Llama::ChatMessage.new(\"system\", \"You are a helpful assistant.\"),\n  Llama::ChatMessage.new(\"user\", \"Hello, who are you?\")\n]\n\n# Generate a response\nresponse = context.chat(messages)\nputs \"Assistant: #{response}\"\n\n# Continue the conversation\nmessages << Llama::ChatMessage.new(\"assistant\", response)\nmessages << Llama::ChatMessage.new(\"user\", \"Tell me a joke\")\nresponse = context.chat(messages)\nputs \"Assistant: #{response}\"\n```\n\n### Embeddings\n\n```crystal\nrequire \"llama\"\n\nmodel = Llama::Model.new(\"/path/to/model.gguf\")\n\n# Create a context with embeddings enabled\ncontext = model.context(embeddings: true)\n\n# Get embeddings for text\ntext = \"Hello, world!\"\ntokens = model.vocab.tokenize(text)\nbatch = Llama::Batch.get_one(tokens)\ncontext.decode(batch)\nembeddings = context.get_embeddings_seq(0)\n\nputs \"Embedding dimension: #{embeddings.size}\"\n```\n\n### Utilities\n\n#### System Info\n\n```crystal\nputs Llama.system_info\n```\n\n#### Tokenization Utility\n\n```crystal\nmodel = Llama::Model.new(\"/path/to/model.gguf\")\nputs Llama.tokenize_and_format(model.vocab, \"Hello, world!\", ids_only: true)\n```\n\n## Examples\n\nThe `examples` directory contains sample code demonstrating various features:\n\n- `simple.cr` - Basic text generation\n- `chat.cr` - Chat conversations with models\n- `tokenize.cr` - Tokenization and vocabulary features\n\n## API Documentation\n\nSee [kojix2.github.io/llama.cr](https://kojix2.github.io/llama.cr) for full API docs.\n\n### Core Classes\n\n- **Llama::Model** - Represents a loaded LLaMA model\n- **Llama::Context** - Handles inference state for a model\n- **Llama::Vocab** - Provides access to the model's vocabulary\n- **Llama::Batch** - Manages batches of tokens for efficient processing\n- **Llama::KvCache** - Controls the key-value cache for optimized inference\n- **Llama::State** - Handles saving and loading model state\n- **Llama::Sampler::Chain** - Combines multiple sampling methods\n\n### Samplers\n\n- **Llama::Sampler::TopK** - Keeps only the top K most likely tokens\n- **Llama::Sampler::TopP** - Nucleus sampling (keeps tokens until cumulative probability exceeds P)\n- **Llama::Sampler::Temp** - Applies temperature to logits\n- **Llama::Sampler::Dist** - Samples from the final probability distribution\n- **Llama::Sampler::MinP** - Keeps tokens with probability >= P \\* max_probability\n- **Llama::Sampler::Typical** - Selects tokens based on their \"typicality\" (entropy)\n- **Llama::Sampler::Mirostat** - Dynamically adjusts sampling to maintain target entropy\n- **Llama::Sampler::Penalties** - Applies penalties to reduce repetition\n\n## Development\n\nSee [DEVELOPMENT.md](DEVELOPMENT.md) for development guidelines.\n\n## Contributing\n\n1. Fork it (<https://github.com/kojix2/llama.cr/fork>)\n2. Create your feature branch (`git checkout -b my-new-feature`)\n3. Commit your changes (`git commit -am 'Add some feature'`)\n4. Push to the branch (`git push origin my-new-feature`)\n5. Create a new Pull Request\n\n## License\n\nThis project is available under the MIT License. See the LICENSE file for more info.\n","program":{"html_id":"llama/toplevel","path":"toplevel.html","kind":"module","full_name":"Top Level Namespace","name":"Top Level Namespace","abstract":false,"locations":[],"repository_name":"llama","program":true,"enum":false,"alias":false,"const":false,"types":[{"html_id":"llama/Llama","path":"Llama.html","kind":"module","full_name":"Llama","name":"Llama","abstract":false,"locations":[{"filename":"src/llama.cr","line_number":79,"url":"https://github.com/kojix2/llama.cr/blob/038f4ed10509841d5e14fe7055fbc1a21dac6a15/src/llama.cr#L79"},{"filename":"src/llama/batch.cr","line_number":3,"url":"https://github.com/kojix2/llama.cr/blob/038f4ed10509841d5e14fe7055fbc1a21dac6a15/src/llama/batch.cr#L3"},{"filename":"src/llama/batch/error.cr","line_number":3,"url":"https://github.com/kojix2/llama.cr/blob/038f4ed10509841d5e14fe7055fbc1a21dac6a15/src/llama/batch/error.cr#L3"},{"filename":"src/llama/chat.cr","line_number":1,"url":"https://github.com/kojix2/llama.cr/blob/038f4ed10509841d5e14fe7055fbc1a21dac6a15/src/llama/chat.cr#L1"},{"filename":"src/llama/context.cr","line_number":3,"url":"https://github.com/kojix2/llama.cr/blob/038f4ed10509841d5e14fe7055fbc1a21dac6a15/src/llama/context.cr#L3"},{"filename":"src/llama/context/error.cr","line_number":3,"url":"https://github.com/kojix2/llama.cr/blob/038f4ed10509841d5e14fe7055fbc1a21dac6a15/src/llama/context/error.cr#L3"},{"filename":"src/llama/error.cr","line_number":1,"url":"https://github.com/kojix2/llama.cr/blob/038f4ed10509841d5e14fe7055fbc1a21dac6a15/src/llama/error.cr#L1"},{"filename":"src/llama/kv_cache.cr","line_number":3,"url":"https://github.com/kojix2/llama.cr/blob/038f4ed10509841d5e14fe7055fbc1a21dac6a15/src/llama/kv_cache.cr#L3"},{"filename":"src/llama/kv_cache/error.cr","line_number":3,"url":"https://github.com/kojix2/llama.cr/blob/038f4ed10509841d5e14fe7055fbc1a21dac6a15/src/llama/kv_cache/error.cr#L3"},{"filename":"src/llama/lib_llama.cr","line_number":1,"url":"https://github.com/kojix2/llama.cr/blob/038f4ed10509841d5e14fe7055fbc1a21dac6a15/src/llama/lib_llama.cr#L1"},{"filename":"src/llama/model.cr","line_number":3,"url":"https://github.com/kojix2/llama.cr/blob/038f4ed10509841d5e14fe7055fbc1a21dac6a15/src/llama/model.cr#L3"},{"filename":"src/llama/model/error.cr","line_number":3,"url":"https://github.com/kojix2/llama.cr/blob/038f4ed10509841d5e14fe7055fbc1a21dac6a15/src/llama/model/error.cr#L3"},{"filename":"src/llama/sampler.cr","line_number":3,"url":"https://github.com/kojix2/llama.cr/blob/038f4ed10509841d5e14fe7055fbc1a21dac6a15/src/llama/sampler.cr#L3"},{"filename":"src/llama/sampler/base.cr","line_number":1,"url":"https://github.com/kojix2/llama.cr/blob/038f4ed10509841d5e14fe7055fbc1a21dac6a15/src/llama/sampler/base.cr#L1"},{"filename":"src/llama/sampler/chain.cr","line_number":1,"url":"https://github.com/kojix2/llama.cr/blob/038f4ed10509841d5e14fe7055fbc1a21dac6a15/src/llama/sampler/chain.cr#L1"},{"filename":"src/llama/sampler/dist.cr","line_number":1,"url":"https://github.com/kojix2/llama.cr/blob/038f4ed10509841d5e14fe7055fbc1a21dac6a15/src/llama/sampler/dist.cr#L1"},{"filename":"src/llama/sampler/error.cr","line_number":3,"url":"https://github.com/kojix2/llama.cr/blob/038f4ed10509841d5e14fe7055fbc1a21dac6a15/src/llama/sampler/error.cr#L3"},{"filename":"src/llama/sampler/grammar.cr","line_number":1,"url":"https://github.com/kojix2/llama.cr/blob/038f4ed10509841d5e14fe7055fbc1a21dac6a15/src/llama/sampler/grammar.cr#L1"},{"filename":"src/llama/sampler/grammar_lazy_patterns.cr","line_number":1,"url":"https://github.com/kojix2/llama.cr/blob/038f4ed10509841d5e14fe7055fbc1a21dac6a15/src/llama/sampler/grammar_lazy_patterns.cr#L1"},{"filename":"src/llama/sampler/greedy.cr","line_number":1,"url":"https://github.com/kojix2/llama.cr/blob/038f4ed10509841d5e14fe7055fbc1a21dac6a15/src/llama/sampler/greedy.cr#L1"},{"filename":"src/llama/sampler/infill.cr","line_number":1,"url":"https://github.com/kojix2/llama.cr/blob/038f4ed10509841d5e14fe7055fbc1a21dac6a15/src/llama/sampler/infill.cr#L1"},{"filename":"src/llama/sampler/min_p.cr","line_number":1,"url":"https://github.com/kojix2/llama.cr/blob/038f4ed10509841d5e14fe7055fbc1a21dac6a15/src/llama/sampler/min_p.cr#L1"},{"filename":"src/llama/sampler/mirostat.cr","line_number":1,"url":"https://github.com/kojix2/llama.cr/blob/038f4ed10509841d5e14fe7055fbc1a21dac6a15/src/llama/sampler/mirostat.cr#L1"},{"filename":"src/llama/sampler/mirostat_v2.cr","line_number":1,"url":"https://github.com/kojix2/llama.cr/blob/038f4ed10509841d5e14fe7055fbc1a21dac6a15/src/llama/sampler/mirostat_v2.cr#L1"},{"filename":"src/llama/sampler/penalties.cr","line_number":1,"url":"https://github.com/kojix2/llama.cr/blob/038f4ed10509841d5e14fe7055fbc1a21dac6a15/src/llama/sampler/penalties.cr#L1"},{"filename":"src/llama/sampler/temp.cr","line_number":1,"url":"https://github.com/kojix2/llama.cr/blob/038f4ed10509841d5e14fe7055fbc1a21dac6a15/src/llama/sampler/temp.cr#L1"},{"filename":"src/llama/sampler/temp_ext.cr","line_number":1,"url":"https://github.com/kojix2/llama.cr/blob/038f4ed10509841d5e14fe7055fbc1a21dac6a15/src/llama/sampler/temp_ext.cr#L1"},{"filename":"src/llama/sampler/top_k.cr","line_number":1,"url":"https://github.com/kojix2/llama.cr/blob/038f4ed10509841d5e14fe7055fbc1a21dac6a15/src/llama/sampler/top_k.cr#L1"},{"filename":"src/llama/sampler/top_n_sigma.cr","line_number":1,"url":"https://github.com/kojix2/llama.cr/blob/038f4ed10509841d5e14fe7055fbc1a21dac6a15/src/llama/sampler/top_n_sigma.cr#L1"},{"filename":"src/llama/sampler/top_p.cr","line_number":1,"url":"https://github.com/kojix2/llama.cr/blob/038f4ed10509841d5e14fe7055fbc1a21dac6a15/src/llama/sampler/top_p.cr#L1"},{"filename":"src/llama/sampler/typical.cr","line_number":1,"url":"https://github.com/kojix2/llama.cr/blob/038f4ed10509841d5e14fe7055fbc1a21dac6a15/src/llama/sampler/typical.cr#L1"},{"filename":"src/llama/sampler/xtc.cr","line_number":1,"url":"https://github.com/kojix2/llama.cr/blob/038f4ed10509841d5e14fe7055fbc1a21dac6a15/src/llama/sampler/xtc.cr#L1"},{"filename":"src/llama/state.cr","line_number":3,"url":"https://github.com/kojix2/llama.cr/blob/038f4ed10509841d5e14fe7055fbc1a21dac6a15/src/llama/state.cr#L3"},{"filename":"src/llama/state/error.cr","line_number":3,"url":"https://github.com/kojix2/llama.cr/blob/038f4ed10509841d5e14fe7055fbc1a21dac6a15/src/llama/state/error.cr#L3"},{"filename":"src/llama/vocab.cr","line_number":1,"url":"https://github.com/kojix2/llama.cr/blob/038f4ed10509841d5e14fe7055fbc1a21dac6a15/src/llama/vocab.cr#L1"}],"repository_name":"llama","program":false,"enum":false,"alias":false,"const":false,"constants":[{"id":"LLAMA_CPP_COMPATIBLE_VERSION","name":"LLAMA_CPP_COMPATIBLE_VERSION","value":"(read_file(\"/home/runner/work/llama.cr/llama.cr/src/LLAMA_VERSION\")).chomp"},{"id":"LOG_LEVEL_DEBUG","name":"LOG_LEVEL_DEBUG","value":"0","doc":"Log level constants (from llama.cpp / ggml)","summary":"<p>Log level constants (from llama.cpp / ggml)</p>"},{"id":"LOG_LEVEL_ERROR","name":"LOG_LEVEL_ERROR","value":"3"},{"id":"LOG_LEVEL_INFO","name":"LOG_LEVEL_INFO","value":"1"},{"id":"LOG_LEVEL_NONE","name":"LOG_LEVEL_NONE","value":"4"},{"id":"LOG_LEVEL_WARNING","name":"LOG_LEVEL_WARNING","value":"2"},{"id":"VERSION","name":"VERSION","value":"\"0.1.0\""}],"class_methods":[{"html_id":"apply_chat_template(template:String|Nil,messages:Array(ChatMessage),add_assistant:Bool=true):String-class-method","name":"apply_chat_template","doc":"Applies a chat template to a list of messages\n\nParameters:\n- template: The template string (nil to use model's default)\n- messages: Array of chat messages\n- add_assistant: Whether to end with an assistant message prefix\n\nReturns:\n- The formatted prompt string\n\nRaises:\n- Llama::Error if template application fails","summary":"<p>Applies a chat template to a list of messages</p>","abstract":false,"args":[{"name":"template","external_name":"template","restriction":"String | ::Nil"},{"name":"messages","external_name":"messages","restriction":"Array(ChatMessage)"},{"name":"add_assistant","default_value":"true","external_name":"add_assistant","restriction":"Bool"}],"args_string":"(template : String | Nil, messages : Array(ChatMessage), add_assistant : Bool = true) : String","args_html":"(template : String | Nil, messages : Array(<a href=\"Llama/ChatMessage.html\">ChatMessage</a>), add_assistant : Bool = <span class=\"n\">true</span>) : String","location":{"filename":"src/llama/chat.cr","line_number":39,"url":"https://github.com/kojix2/llama.cr/blob/038f4ed10509841d5e14fe7055fbc1a21dac6a15/src/llama/chat.cr#L39"},"def":{"name":"apply_chat_template","args":[{"name":"template","external_name":"template","restriction":"String | ::Nil"},{"name":"messages","external_name":"messages","restriction":"Array(ChatMessage)"},{"name":"add_assistant","default_value":"true","external_name":"add_assistant","restriction":"Bool"}],"return_type":"String","visibility":"Public","body":"c_messages = messages.map(&.to_unsafe)\n\n\nestimated_size = messages.sum do |msg|\n  msg.content.size + msg.role.size\nend * 2\nbuffer = Pointer(LibC::Char).malloc(estimated_size)\n\n\nresult = LibLlama.llama_chat_apply_template((template || \"\").to_unsafe, c_messages.to_unsafe, messages.size, add_assistant, buffer, estimated_size)\n\n\nif result > estimated_size\n  buffer = Pointer(LibC::Char).malloc(result)\n  result = LibLlama.llama_chat_apply_template((template || \"\").to_unsafe, c_messages.to_unsafe, messages.size, add_assistant, buffer, result)\nend\n\n\nif result < 0\n  raise(Error.new(\"Failed to apply chat template\"))\nend\n\n\nString.new(buffer, result)\n"},"external_var":false},{"html_id":"builtin_chat_templates:Array(String)-class-method","name":"builtin_chat_templates","doc":"Gets the list of built-in chat templates\n\nReturns:\n- Array of template names","summary":"<p>Gets the list of built-in chat templates</p>","abstract":false,"location":{"filename":"src/llama/chat.cr","line_number":85,"url":"https://github.com/kojix2/llama.cr/blob/038f4ed10509841d5e14fe7055fbc1a21dac6a15/src/llama/chat.cr#L85"},"def":{"name":"builtin_chat_templates","return_type":"Array(String)","visibility":"Public","body":"output = Pointer(::Pointer(LibC::Char)).malloc(100)\ncount = LibLlama.llama_chat_builtin_templates(output, 100)\n\nresult = [] of String\ncount.times do |i|\n  result << (String.new(output[i]))\nend\n\nresult\n"},"external_var":false},{"html_id":"error_message(code:Int32):String-class-method","name":"error_message","abstract":false,"args":[{"name":"code","external_name":"code","restriction":"Int32"}],"args_string":"(code : Int32) : String","args_html":"(code : Int32) : String","location":{"filename":"src/llama/error.cr","line_number":38,"url":"https://github.com/kojix2/llama.cr/blob/038f4ed10509841d5e14fe7055fbc1a21dac6a15/src/llama/error.cr#L38"},"def":{"name":"error_message","args":[{"name":"code","external_name":"code","restriction":"Int32"}],"return_type":"String","visibility":"Public","body":"Error.error_message(code)"},"external_var":false},{"html_id":"format_error(message:String,code:Int32|Nil=nil,context:String|Nil=nil):String-class-method","name":"format_error","abstract":false,"args":[{"name":"message","external_name":"message","restriction":"String"},{"name":"code","default_value":"nil","external_name":"code","restriction":"Int32 | ::Nil"},{"name":"context","default_value":"nil","external_name":"context","restriction":"String | ::Nil"}],"args_string":"(message : String, code : Int32 | Nil = nil, context : String | Nil = nil) : String","args_html":"(message : String, code : Int32 | Nil = <span class=\"n\">nil</span>, context : String | Nil = <span class=\"n\">nil</span>) : String","location":{"filename":"src/llama/error.cr","line_number":42,"url":"https://github.com/kojix2/llama.cr/blob/038f4ed10509841d5e14fe7055fbc1a21dac6a15/src/llama/error.cr#L42"},"def":{"name":"format_error","args":[{"name":"message","external_name":"message","restriction":"String"},{"name":"code","default_value":"nil","external_name":"code","restriction":"Int32 | ::Nil"},{"name":"context","default_value":"nil","external_name":"context","restriction":"String | ::Nil"}],"return_type":"String","visibility":"Public","body":"Error.format_error(message, code, context)"},"external_var":false},{"html_id":"generate(model_path:String,prompt:String,max_tokens:Int32=128,temperature:Float32=0.8):String-class-method","name":"generate","doc":"Generates text from a prompt using a model\n\nThis is a convenience method that loads a model, creates a context,\nand generates text in a single call.\n\n```\nresponse = Llama.generate(\n  \"/path/to/model.gguf\",\n  \"Once upon a time\",\n  max_tokens: 100,\n  temperature: 0.7\n)\nputs response\n```\n\nParameters:\n- model_path: Path to the model file (.gguf format)\n- prompt: The input prompt\n- max_tokens: Maximum number of tokens to generate (must be positive)\n- temperature: Sampling temperature (0.0 = greedy, 1.0 = more random)\n\nReturns:\n- The generated text\n\nRaises:\n- ArgumentError if parameters are invalid\n- Llama::Model::Error if model loading fails\n- Llama::Context::Error if text generation fails","summary":"<p>Generates text from a prompt using a model</p>","abstract":false,"args":[{"name":"model_path","external_name":"model_path","restriction":"String"},{"name":"prompt","external_name":"prompt","restriction":"String"},{"name":"max_tokens","default_value":"128","external_name":"max_tokens","restriction":"Int32"},{"name":"temperature","default_value":"0.8","external_name":"temperature","restriction":"Float32"}],"args_string":"(model_path : String, prompt : String, max_tokens : Int32 = 128, temperature : Float32 = 0.8) : String","args_html":"(model_path : String, prompt : String, max_tokens : Int32 = <span class=\"n\">128</span>, temperature : Float32 = <span class=\"n\">0.8</span>) : String","location":{"filename":"src/llama.cr","line_number":261,"url":"https://github.com/kojix2/llama.cr/blob/038f4ed10509841d5e14fe7055fbc1a21dac6a15/src/llama.cr#L261"},"def":{"name":"generate","args":[{"name":"model_path","external_name":"model_path","restriction":"String"},{"name":"prompt","external_name":"prompt","restriction":"String"},{"name":"max_tokens","default_value":"128","external_name":"max_tokens","restriction":"Int32"},{"name":"temperature","default_value":"0.8","external_name":"temperature","restriction":"Float32"}],"return_type":"String","visibility":"Public","body":"if max_tokens <= 0\n  raise(ArgumentError.new(\"max_tokens must be positive\"))\nend\nif temperature < 0\n  raise(ArgumentError.new(\"temperature must be non-negative\"))\nend\n\nmodel = Model.new(model_path)\ncontext = model.context\ncontext.generate(prompt, max_tokens, temperature)\n"},"external_var":false},{"html_id":"init-class-method","name":"init","doc":"Thread-safe, idempotent initialization of the llama.cpp backend.\nYou do not need to call this manually in most cases.","summary":"<p>Thread-safe, idempotent initialization of the llama.cpp backend.</p>","abstract":false,"location":{"filename":"src/llama.cr","line_number":273,"url":"https://github.com/kojix2/llama.cr/blob/038f4ed10509841d5e14fe7055fbc1a21dac6a15/src/llama.cr#L273"},"def":{"name":"init","visibility":"Public","body":"@@backend_mutex.synchronize do\n  if @@backend_initialized\n  else\n    LibLlama.llama_backend_init\n    @@backend_initialized = true\n  end\nend"},"external_var":false},{"html_id":"log_level-class-method","name":"log_level","doc":"Get the current log level\n\nReturns:\n- The current log level","summary":"<p>Get the current log level</p>","abstract":false,"location":{"filename":"src/llama.cr","line_number":115,"url":"https://github.com/kojix2/llama.cr/blob/038f4ed10509841d5e14fe7055fbc1a21dac6a15/src/llama.cr#L115"},"def":{"name":"log_level","visibility":"Public","body":"@@log_level"},"external_var":false},{"html_id":"log_level=(level:Int32)-class-method","name":"log_level=","doc":"Set the log level\n\nParameters:\n- level : Int32 - log level (0=DEBUG, 1=INFO, 2=WARNING, 3=ERROR, 4=NONE)\n\nExample:\n  Llama.log_level = Llama::LOG_LEVEL_ERROR  # Only show errors\n  Llama.log_level = Llama::LOG_LEVEL_NONE   # Disable all logging","summary":"<p>Set the log level</p>","abstract":false,"args":[{"name":"level","external_name":"level","restriction":"Int32"}],"args_string":"(level : Int32)","args_html":"(level : Int32)","location":{"filename":"src/llama.cr","line_number":106,"url":"https://github.com/kojix2/llama.cr/blob/038f4ed10509841d5e14fe7055fbc1a21dac6a15/src/llama.cr#L106"},"def":{"name":"log_level=","args":[{"name":"level","external_name":"level","restriction":"Int32"}],"visibility":"Public","body":"@@log_level = level\nsetup_default_logger\n"},"external_var":false},{"html_id":"log_set(&block:Int32,String->)-class-method","name":"log_set","doc":"Set a custom log callback\n\nThe block receives:\n- level : Int32 - log level (0=DEBUG, 1=INFO, 2=WARNING, 3=ERROR)\n- message : String - log message\n\nExample:\n  Llama.log_set do |level, message|\n    if level >= Llama::LOG_LEVEL_ERROR\n      STDERR.print message\n    end\n  end","summary":"<p>Set a custom log callback</p>","abstract":false,"location":{"filename":"src/llama.cr","line_number":138,"url":"https://github.com/kojix2/llama.cr/blob/038f4ed10509841d5e14fe7055fbc1a21dac6a15/src/llama.cr#L138"},"def":{"name":"log_set","yields":2,"block_arity":2,"block_arg":{"name":"block","external_name":"block","restriction":"(Int32, String ->)"},"visibility":"Public","body":"boxed = Box.box(block)\n@@log_box = boxed\n\nLibLlama.llama_log_set(->(level : Int32, text : ::Pointer(LibC::Char), user_data : ::Pointer(Void)) do\n  user_callback = Box(Proc(Int32, String, Nil)).unbox(user_data)\n  msg = String.new(text)\n  user_callback.call(level, msg)\n  nil\nend, boxed)\n"},"external_var":false},{"html_id":"process_escapes(text:String):String-class-method","name":"process_escapes","doc":"Process escape sequences in a string\n\nThis method processes common escape sequences like \\n, \\t, etc.\nin a string, converting them to their actual character representations.\n\n```\ntext = Llama.process_escapes(\"Hello\\\\nWorld\")\nputs text # Prints \"Hello\" and \"World\" on separate lines\n```\n\nParameters:\n- text: The input string containing escape sequences\n\nReturns:\n- A new string with escape sequences processed","summary":"<p>Process escape sequences in a string</p>","abstract":false,"args":[{"name":"text","external_name":"text","restriction":"String"}],"args_string":"(text : String) : String","args_html":"(text : String) : String","location":{"filename":"src/llama.cr","line_number":184,"url":"https://github.com/kojix2/llama.cr/blob/038f4ed10509841d5e14fe7055fbc1a21dac6a15/src/llama.cr#L184"},"def":{"name":"process_escapes","args":[{"name":"text","external_name":"text","restriction":"String"}],"return_type":"String","visibility":"Public","body":"text.gsub(/\\\\([nrt\\\\\"])/) do |match|\n  case match[1]\n  when 'n'\n    \"\\n\"\n  when 'r'\n    \"\\r\"\n  when 't'\n    \"\\t\"\n  when '\\\\'\n    \"\\\\\"\n  when '\"'\n    \"\\\"\"\n  else\n    match\n  end\nend"},"external_var":false},{"html_id":"system_info:String-class-method","name":"system_info","doc":"Returns the llama.cpp system information\n\nThis method provides information about the llama.cpp build,\nincluding BLAS configuration, CPU features, and GPU support.\n\n```\ninfo = Llama.system_info\nputs info\n```\n\nReturns:\n- A string containing system information","summary":"<p>Returns the llama.cpp system information</p>","abstract":false,"location":{"filename":"src/llama.cr","line_number":165,"url":"https://github.com/kojix2/llama.cr/blob/038f4ed10509841d5e14fe7055fbc1a21dac6a15/src/llama.cr#L165"},"def":{"name":"system_info","return_type":"String","visibility":"Public","body":"String.new(LibLlama.llama_print_system_info)"},"external_var":false},{"html_id":"tokenize_and_format(vocab:Vocab,text:String,add_bos:Bool=true,parse_special:Bool=true,ids_only:Bool=false):String-class-method","name":"tokenize_and_format","doc":"Tokenize text and return formatted output\n\nThis is a convenience method that tokenizes text and returns\na formatted string representation of the tokens.\n\n```\nmodel = Llama::Model.new(\"/path/to/model.gguf\")\nresult = Llama.tokenize_and_format(model.vocab, \"Hello, world!\", ids_only: true)\nputs result # Prints \"[1, 2, 3, ...]\"\n```\n\nParameters:\n- vocab: The vocabulary to use for tokenization\n- text: The text to tokenize\n- add_bos: Whether to add BOS token (default: true)\n- parse_special: Whether to parse special tokens (default: true)\n- ids_only: Whether to return only token IDs (default: false)\n\nReturns:\n- A formatted string representation of the tokens","summary":"<p>Tokenize text and return formatted output</p>","abstract":false,"args":[{"name":"vocab","external_name":"vocab","restriction":"Vocab"},{"name":"text","external_name":"text","restriction":"String"},{"name":"add_bos","default_value":"true","external_name":"add_bos","restriction":"Bool"},{"name":"parse_special","default_value":"true","external_name":"parse_special","restriction":"Bool"},{"name":"ids_only","default_value":"false","external_name":"ids_only","restriction":"Bool"}],"args_string":"(vocab : Vocab, text : String, add_bos : Bool = true, parse_special : Bool = true, ids_only : Bool = false) : String","args_html":"(vocab : <a href=\"Llama/Vocab.html\">Vocab</a>, text : String, add_bos : Bool = <span class=\"n\">true</span>, parse_special : Bool = <span class=\"n\">true</span>, ids_only : Bool = <span class=\"n\">false</span>) : String","location":{"filename":"src/llama.cr","line_number":217,"url":"https://github.com/kojix2/llama.cr/blob/038f4ed10509841d5e14fe7055fbc1a21dac6a15/src/llama.cr#L217"},"def":{"name":"tokenize_and_format","args":[{"name":"vocab","external_name":"vocab","restriction":"Vocab"},{"name":"text","external_name":"text","restriction":"String"},{"name":"add_bos","default_value":"true","external_name":"add_bos","restriction":"Bool"},{"name":"parse_special","default_value":"true","external_name":"parse_special","restriction":"Bool"},{"name":"ids_only","default_value":"false","external_name":"ids_only","restriction":"Bool"}],"return_type":"String","visibility":"Public","body":"tokens = vocab.tokenize(text, add_bos, parse_special)\n\nif ids_only\n  (\"[\" + (tokens.map(&.to_s).join(\", \"))) + \"]\"\nelse\n  tokens.map do |t|\n    vocab.format_token(t)\n  end.join(\"\\n\")\nend\n"},"external_var":false},{"html_id":"uninit-class-method","name":"uninit","doc":"Thread-safe, idempotent finalization of the llama.cpp backend.\nCall this if you want to explicitly release all backend resources before program exit.","summary":"<p>Thread-safe, idempotent finalization of the llama.cpp backend.</p>","abstract":false,"location":{"filename":"src/llama.cr","line_number":284,"url":"https://github.com/kojix2/llama.cr/blob/038f4ed10509841d5e14fe7055fbc1a21dac6a15/src/llama.cr#L284"},"def":{"name":"uninit","visibility":"Public","body":"@@backend_mutex.synchronize do\n  if @@backend_initialized\n    LibLlama.llama_backend_free\n    @@backend_initialized = false\n  end\nend"},"external_var":false}],"types":[{"html_id":"llama/Llama/Batch","path":"Llama/Batch.html","kind":"class","full_name":"Llama::Batch","name":"Batch","abstract":false,"superclass":{"html_id":"llama/Reference","kind":"class","full_name":"Reference","name":"Reference"},"ancestors":[{"html_id":"llama/Reference","kind":"class","full_name":"Reference","name":"Reference"},{"html_id":"llama/Object","kind":"class","full_name":"Object","name":"Object"}],"locations":[{"filename":"src/llama/batch.cr","line_number":6,"url":"https://github.com/kojix2/llama.cr/blob/038f4ed10509841d5e14fe7055fbc1a21dac6a15/src/llama/batch.cr#L6"},{"filename":"src/llama/batch/error.cr","line_number":4,"url":"https://github.com/kojix2/llama.cr/blob/038f4ed10509841d5e14fe7055fbc1a21dac6a15/src/llama/batch/error.cr#L4"}],"repository_name":"llama","program":false,"enum":false,"alias":false,"const":false,"namespace":{"html_id":"llama/Llama","kind":"module","full_name":"Llama","name":"Llama"},"doc":"Wrapper for the llama_batch structure\nProvides methods for managing batches of tokens for efficient processing","summary":"<p>Wrapper for the llama_batch structure Provides methods for managing batches of tokens for efficient processing</p>","constructors":[{"html_id":"for_embeddings(embeddings:Array(Array(Float32)),seq_ids:Array(Int32)|Nil=nil,n_seq_max:Int32=8):Batch-class-method","name":"for_embeddings","doc":"Creates a batch for embeddings with optional parameters\n\nParameters:\n- embeddings: Array of embedding vectors\n- seq_ids: Sequence IDs to use for all embeddings (default: nil)\n- n_seq_max: Maximum number of sequence IDs per token (default: 8)\n\nReturns:\n- A new Batch instance configured with the provided embeddings\n\nRaises:\n- ArgumentError if embeddings array is empty or contains empty embeddings\n- Llama::Batch::Error if batch creation fails","summary":"<p>Creates a batch for embeddings with optional parameters</p>","abstract":false,"args":[{"name":"embeddings","external_name":"embeddings","restriction":"Array(Array(Float32))"},{"name":"seq_ids","default_value":"nil","external_name":"seq_ids","restriction":"Array(Int32) | ::Nil"},{"name":"n_seq_max","default_value":"8","external_name":"n_seq_max","restriction":"Int32"}],"args_string":"(embeddings : Array(Array(Float32)), seq_ids : Array(Int32) | Nil = nil, n_seq_max : Int32 = 8) : Batch","args_html":"(embeddings : Array(Array(Float32)), seq_ids : Array(Int32) | Nil = <span class=\"n\">nil</span>, n_seq_max : Int32 = <span class=\"n\">8</span>) : <a href=\"../Llama/Batch.html\">Batch</a>","location":{"filename":"src/llama/batch.cr","line_number":314,"url":"https://github.com/kojix2/llama.cr/blob/038f4ed10509841d5e14fe7055fbc1a21dac6a15/src/llama/batch.cr#L314"},"def":{"name":"for_embeddings","args":[{"name":"embeddings","external_name":"embeddings","restriction":"Array(Array(Float32))"},{"name":"seq_ids","default_value":"nil","external_name":"seq_ids","restriction":"Array(Int32) | ::Nil"},{"name":"n_seq_max","default_value":"8","external_name":"n_seq_max","restriction":"Int32"}],"return_type":"Batch","visibility":"Public","body":"if embeddings.empty?\n  raise(ArgumentError.new(\"Embeddings array cannot be empty\"))\nend\n\nif embeddings.first.empty?\n  raise(ArgumentError.new(\"Embedding vectors cannot be empty\"))\nend\n\nbegin\n  embd_size = embeddings.first.size\n  batch = Batch.new(embeddings.size, embd_size, n_seq_max)\n\n  embeddings.each_with_index do |embedding, i|\n    if embedding.size != embd_size\n      error_msg = Llama.format_error(\"Inconsistent embedding dimensions\", nil, \"expected: #{embd_size}, got: #{embedding.size} at index #{i}\")\n      raise(Batch::Error.new(error_msg))\n    end\n\n    batch.set_embedding(i, embedding, i, seq_ids)\n  end\n\n  batch\nrescue ex : Batch::Error | ArgumentError | IndexError\n  raise(ex)\nrescue ex\n  error_msg = Llama.format_error(\"Failed to create batch for embeddings\", -3, \"embeddings size: #{embeddings.size}, embd_size: #{embeddings.first.size}, error: #{ex.message}\")\n  raise(Batch::Error.new(error_msg))\nend\n"},"external_var":false},{"html_id":"for_tokens(tokens:Array(Int32),compute_logits_for_last:Bool=true,seq_ids:Array(Int32)|Nil=nil,n_seq_max:Int32=8):Batch-class-method","name":"for_tokens","doc":"Creates a batch for a sequence of tokens with optional parameters\n\nParameters:\n- tokens: Array of token IDs\n- compute_logits_for_last: Whether to compute logits only for the last token\n- seq_ids: Sequence IDs to use for all tokens (default: nil)\n- n_seq_max: Maximum number of sequence IDs per token (default: 8)\n\nReturns:\n- A new Batch instance configured with the provided tokens\n\nRaises:\n- ArgumentError if tokens array is empty\n- Llama::Batch::Error if batch creation fails","summary":"<p>Creates a batch for a sequence of tokens with optional parameters</p>","abstract":false,"args":[{"name":"tokens","external_name":"tokens","restriction":"Array(Int32)"},{"name":"compute_logits_for_last","default_value":"true","external_name":"compute_logits_for_last","restriction":"Bool"},{"name":"seq_ids","default_value":"nil","external_name":"seq_ids","restriction":"Array(Int32) | ::Nil"},{"name":"n_seq_max","default_value":"8","external_name":"n_seq_max","restriction":"Int32"}],"args_string":"(tokens : Array(Int32), compute_logits_for_last : Bool = true, seq_ids : Array(Int32) | Nil = nil, n_seq_max : Int32 = 8) : Batch","args_html":"(tokens : Array(Int32), compute_logits_for_last : Bool = <span class=\"n\">true</span>, seq_ids : Array(Int32) | Nil = <span class=\"n\">nil</span>, n_seq_max : Int32 = <span class=\"n\">8</span>) : <a href=\"../Llama/Batch.html\">Batch</a>","location":{"filename":"src/llama/batch.cr","line_number":251,"url":"https://github.com/kojix2/llama.cr/blob/038f4ed10509841d5e14fe7055fbc1a21dac6a15/src/llama/batch.cr#L251"},"def":{"name":"for_tokens","args":[{"name":"tokens","external_name":"tokens","restriction":"Array(Int32)"},{"name":"compute_logits_for_last","default_value":"true","external_name":"compute_logits_for_last","restriction":"Bool"},{"name":"seq_ids","default_value":"nil","external_name":"seq_ids","restriction":"Array(Int32) | ::Nil"},{"name":"n_seq_max","default_value":"8","external_name":"n_seq_max","restriction":"Int32"}],"return_type":"Batch","visibility":"Public","body":"if tokens.empty?\n  raise(ArgumentError.new(\"Tokens array cannot be empty\"))\nend\n\nbegin\n  handle, has_crystal_token = self.crystal_llama_batch_get_one(tokens.to_unsafe, tokens.size, n_seq_max)\n\n  batch = Batch.new(handle, owned: true)\n\n  batch.has_crystal_token = has_crystal_token\n\n\n  tokens.size.times do |i|\n    batch.to_unsafe.pos[i] = i\n\n\n    if seq_ids.nil? || seq_ids.empty?\n      batch.to_unsafe.n_seq_id[i] = 1\n      batch.to_unsafe.seq_id[i][0] = 0\n    else\n      num_seq_ids = Math.min(seq_ids.size, n_seq_max)\n      batch.to_unsafe.n_seq_id[i] = num_seq_ids\n\n      num_seq_ids.times do |j|\n        batch.to_unsafe.seq_id[i][j] = seq_ids[j]\n      end\n    end\n\n\n    needs_logits = compute_logits_for_last ? (i == (tokens.size - 1)) : true\n    batch.to_unsafe.logits[i] = needs_logits ? 1_i8 : 0_i8\n  end\n\n  batch\nrescue ex : Batch::Error | ArgumentError | IndexError\n  raise(ex)\nrescue ex\n  error_msg = Llama.format_error(\"Failed to create batch for tokens\", -3, \"tokens size: #{tokens.size}, error: #{ex.message}\")\n  raise(Batch::Error.new(error_msg))\nend\n"},"external_var":false},{"html_id":"get_one(tokens:Array(Int32)):Batch-class-method","name":"get_one","doc":"Creates a new Batch for a single sequence of tokens\n\nParameters:\n- tokens: Array of token IDs\n\nReturns:\n- A new Batch instance\n\nRaises:\n- Llama::Batch::Error if the batch cannot be created","summary":"<p>Creates a new Batch for a single sequence of tokens</p>","abstract":false,"args":[{"name":"tokens","external_name":"tokens","restriction":"Array(Int32)"}],"args_string":"(tokens : Array(Int32)) : Batch","args_html":"(tokens : Array(Int32)) : <a href=\"../Llama/Batch.html\">Batch</a>","location":{"filename":"src/llama/batch.cr","line_number":70,"url":"https://github.com/kojix2/llama.cr/blob/038f4ed10509841d5e14fe7055fbc1a21dac6a15/src/llama/batch.cr#L70"},"def":{"name":"get_one","args":[{"name":"tokens","external_name":"tokens","restriction":"Array(Int32)"}],"return_type":"Batch","visibility":"Public","body":"if tokens.empty?\n  handle = LibLlama::LlamaBatch.new\n  handle.n_tokens = 0\n  return Batch.new(handle)\nend\n\ntokens_ptr = tokens.to_unsafe\nhandle = LibLlama.llama_batch_get_one(tokens_ptr, tokens.size)\n\nif handle.n_tokens == 0\n  error_msg = Llama.format_error(\"Failed to create batch from tokens\", -3, \"tokens size: #{tokens.size}\")\n  raise(Batch::Error.new(error_msg))\nend\n\nBatch.new(handle)\n"},"external_var":false},{"html_id":"new(n_tokens:Int32,embd:Int32=0,n_seq_max:Int32=8)-class-method","name":"new","doc":"Creates a new Batch instance with the specified parameters\n\nParameters:\n- n_tokens: Maximum number of tokens this batch can hold\n- embd: Embedding dimension (0 for token-based batch, >0 for embedding-based batch)\n- n_seq_max: Maximum number of sequence IDs per token (default: 8)\nRaises:\n- ArgumentError if parameters are invalid\n- Llama::Batch::Error if the batch cannot be created","summary":"<p>Creates a new Batch instance with the specified parameters</p>","abstract":false,"args":[{"name":"n_tokens","external_name":"n_tokens","restriction":"Int32"},{"name":"embd","default_value":"0","external_name":"embd","restriction":"Int32"},{"name":"n_seq_max","default_value":"8","external_name":"n_seq_max","restriction":"Int32"}],"args_string":"(n_tokens : Int32, embd : Int32 = 0, n_seq_max : Int32 = 8)","args_html":"(n_tokens : Int32, embd : Int32 = <span class=\"n\">0</span>, n_seq_max : Int32 = <span class=\"n\">8</span>)","location":{"filename":"src/llama/batch.cr","line_number":16,"url":"https://github.com/kojix2/llama.cr/blob/038f4ed10509841d5e14fe7055fbc1a21dac6a15/src/llama/batch.cr#L16"},"def":{"name":"new","args":[{"name":"n_tokens","external_name":"n_tokens","restriction":"Int32"},{"name":"embd","default_value":"0","external_name":"embd","restriction":"Int32"},{"name":"n_seq_max","default_value":"8","external_name":"n_seq_max","restriction":"Int32"}],"visibility":"Public","body":"_ = allocate\n_.initialize(n_tokens, embd, n_seq_max)\nif _.responds_to?(:finalize)\n  ::GC.add_finalizer(_)\nend\n_\n"},"external_var":false},{"html_id":"new(handle:LibLlama::LlamaBatch,owned:Bool=false,n_seq_max:Int32=8)-class-method","name":"new","doc":"Creates a new Batch instance from a raw llama_batch structure\n\nNote: This constructor is intended for internal use.\nThe batch created this way is not owned by this wrapper and will not be freed.","summary":"<p>Creates a new Batch instance from a raw llama_batch structure</p>","abstract":false,"args":[{"name":"handle","external_name":"handle","restriction":"LibLlama::LlamaBatch"},{"name":"owned","default_value":"false","external_name":"owned","restriction":"::Bool"},{"name":"n_seq_max","default_value":"8","external_name":"n_seq_max","restriction":"Int32"}],"args_string":"(handle : LibLlama::LlamaBatch, owned : Bool = false, n_seq_max : Int32 = 8)","args_html":"(handle : LibLlama::LlamaBatch, owned : Bool = <span class=\"n\">false</span>, n_seq_max : Int32 = <span class=\"n\">8</span>)","location":{"filename":"src/llama/batch.cr","line_number":49,"url":"https://github.com/kojix2/llama.cr/blob/038f4ed10509841d5e14fe7055fbc1a21dac6a15/src/llama/batch.cr#L49"},"def":{"name":"new","args":[{"name":"handle","external_name":"handle","restriction":"LibLlama::LlamaBatch"},{"name":"owned","default_value":"false","external_name":"owned","restriction":"::Bool"},{"name":"n_seq_max","default_value":"8","external_name":"n_seq_max","restriction":"Int32"}],"visibility":"Public","body":"_ = allocate\n_.initialize(handle, owned, n_seq_max)\nif _.responds_to?(:finalize)\n  ::GC.add_finalizer(_)\nend\n_\n"},"external_var":false}],"instance_methods":[{"html_id":"add_tokens(tokens:Array(Int32),pos_offset:Int32=0,seq_ids:Array(Int32)|Nil=nil,compute_logits:Bool=true)-instance-method","name":"add_tokens","doc":"Adds multiple tokens to the batch\n\nParameters:\n- tokens: Array of token IDs to add\n- pos_offset: Position offset for the tokens (default: 0)\n- seq_ids: Sequence IDs for all tokens (default: [0])\n- compute_logits: Whether to compute logits for all tokens (default: true)\n\nRaises:\n- ArgumentError if tokens array is empty\n- IndexError if the batch doesn't have enough space\n- Llama::Batch::Error if memory allocation fails","summary":"<p>Adds multiple tokens to the batch</p>","abstract":false,"args":[{"name":"tokens","external_name":"tokens","restriction":"Array(Int32)"},{"name":"pos_offset","default_value":"0","external_name":"pos_offset","restriction":"Int32"},{"name":"seq_ids","default_value":"nil","external_name":"seq_ids","restriction":"Array(Int32) | ::Nil"},{"name":"compute_logits","default_value":"true","external_name":"compute_logits","restriction":"Bool"}],"args_string":"(tokens : Array(Int32), pos_offset : Int32 = 0, seq_ids : Array(Int32) | Nil = nil, compute_logits : Bool = true)","args_html":"(tokens : Array(Int32), pos_offset : Int32 = <span class=\"n\">0</span>, seq_ids : Array(Int32) | Nil = <span class=\"n\">nil</span>, compute_logits : Bool = <span class=\"n\">true</span>)","location":{"filename":"src/llama/batch.cr","line_number":111,"url":"https://github.com/kojix2/llama.cr/blob/038f4ed10509841d5e14fe7055fbc1a21dac6a15/src/llama/batch.cr#L111"},"def":{"name":"add_tokens","args":[{"name":"tokens","external_name":"tokens","restriction":"Array(Int32)"},{"name":"pos_offset","default_value":"0","external_name":"pos_offset","restriction":"Int32"},{"name":"seq_ids","default_value":"nil","external_name":"seq_ids","restriction":"Array(Int32) | ::Nil"},{"name":"compute_logits","default_value":"true","external_name":"compute_logits","restriction":"Bool"}],"visibility":"Public","body":"if tokens.empty?\n  raise(ArgumentError.new(\"Tokens array cannot be empty\"))\nend\n\nif tokens.size > @handle.n_tokens\n  raise(IndexError.new(\"Batch size (#{@handle.n_tokens}) is too small for #{tokens.size} tokens\"))\nend\n\ntokens.each_with_index do |token, i|\n  set_token(i, token, pos_offset + i, seq_ids, compute_logits)\nend\n"},"external_var":false},{"html_id":"cleanup-instance-method","name":"cleanup","doc":"Explicitly clean up resources\nThis can be called manually to release resources before garbage collection","summary":"<p>Explicitly clean up resources This can be called manually to release resources before garbage collection</p>","abstract":false,"location":{"filename":"src/llama/batch.cr","line_number":365,"url":"https://github.com/kojix2/llama.cr/blob/038f4ed10509841d5e14fe7055fbc1a21dac6a15/src/llama/batch.cr#L365"},"def":{"name":"cleanup","visibility":"Public","body":"if @owned\n  if @has_crystal_token && !@handle.token.null?\n    @handle.token = nil\n  end\n\n  LibLlama.llama_batch_free(@handle)\nend"},"external_var":false},{"html_id":"finalize-instance-method","name":"finalize","doc":"Frees the resources associated with this batch","summary":"<p>Frees the resources associated with this batch</p>","abstract":false,"location":{"filename":"src/llama/batch.cr","line_number":377,"url":"https://github.com/kojix2/llama.cr/blob/038f4ed10509841d5e14fe7055fbc1a21dac6a15/src/llama/batch.cr#L377"},"def":{"name":"finalize","visibility":"Public","body":"cleanup"},"external_var":false},{"html_id":"has_crystal_token=(value:Bool)-instance-method","name":"has_crystal_token=","doc":"Setter for has_crystal_token","summary":"<p>Setter for has_crystal_token</p>","abstract":false,"args":[{"name":"value","external_name":"value","restriction":"Bool"}],"args_string":"(value : Bool)","args_html":"(value : Bool)","location":{"filename":"src/llama/batch.cr","line_number":359,"url":"https://github.com/kojix2/llama.cr/blob/038f4ed10509841d5e14fe7055fbc1a21dac6a15/src/llama/batch.cr#L359"},"def":{"name":"has_crystal_token=","args":[{"name":"value","external_name":"value","restriction":"Bool"}],"visibility":"Public","body":"@has_crystal_token = value"},"external_var":false},{"html_id":"n_tokens:Int32-instance-method","name":"n_tokens","doc":"Returns the number of tokens in this batch","summary":"<p>Returns the number of tokens in this batch</p>","abstract":false,"location":{"filename":"src/llama/batch.cr","line_number":95,"url":"https://github.com/kojix2/llama.cr/blob/038f4ed10509841d5e14fe7055fbc1a21dac6a15/src/llama/batch.cr#L95"},"def":{"name":"n_tokens","return_type":"Int32","visibility":"Public","body":"@handle.n_tokens"},"external_var":false},{"html_id":"set_embedding(i:Int32,embedding:Array(Float32),pos:Int32|Nil=nil,seq_ids:Array(Int32)|Nil=nil,logits:Bool|Nil=nil)-instance-method","name":"set_embedding","doc":"Sets an embedding at the specified index\n\nParameters:\n- i: Index in the batch\n- embedding: Array of embedding values\n- pos: Position of the embedding in the sequence (nil for auto-position)\n- seq_ids: Sequence IDs (nil for default sequence 0)\n- logits: Whether to compute logits for this embedding (nil for default)\n\nRaises:\n- IndexError if the index is out of bounds\n- ArgumentError if the batch is not embedding-based\n- Llama::Batch::Error if memory allocation fails","summary":"<p>Sets an embedding at the specified index</p>","abstract":false,"args":[{"name":"i","external_name":"i","restriction":"Int32"},{"name":"embedding","external_name":"embedding","restriction":"Array(Float32)"},{"name":"pos","default_value":"nil","external_name":"pos","restriction":"Int32 | ::Nil"},{"name":"seq_ids","default_value":"nil","external_name":"seq_ids","restriction":"Array(Int32) | ::Nil"},{"name":"logits","default_value":"nil","external_name":"logits","restriction":"Bool | ::Nil"}],"args_string":"(i : Int32, embedding : Array(Float32), pos : Int32 | Nil = nil, seq_ids : Array(Int32) | Nil = nil, logits : Bool | Nil = nil)","args_html":"(i : Int32, embedding : Array(Float32), pos : Int32 | Nil = <span class=\"n\">nil</span>, seq_ids : Array(Int32) | Nil = <span class=\"n\">nil</span>, logits : Bool | Nil = <span class=\"n\">nil</span>)","location":{"filename":"src/llama/batch.cr","line_number":181,"url":"https://github.com/kojix2/llama.cr/blob/038f4ed10509841d5e14fe7055fbc1a21dac6a15/src/llama/batch.cr#L181"},"def":{"name":"set_embedding","args":[{"name":"i","external_name":"i","restriction":"Int32"},{"name":"embedding","external_name":"embedding","restriction":"Array(Float32)"},{"name":"pos","default_value":"nil","external_name":"pos","restriction":"Int32 | ::Nil"},{"name":"seq_ids","default_value":"nil","external_name":"seq_ids","restriction":"Array(Int32) | ::Nil"},{"name":"logits","default_value":"nil","external_name":"logits","restriction":"Bool | ::Nil"}],"visibility":"Public","body":"if i < 0 || i >= @handle.n_tokens\n  raise(IndexError.new(\"Index out of bounds: #{i} (valid range: 0..#{@handle.n_tokens - 1})\"))\nend\n\nif embedding.empty?\n  raise(ArgumentError.new(\"Embedding array cannot be empty\"))\nend\n\n\nembd_size = embedding.size\nembd_size.times do |j|\n  @handle.embd[(i * embd_size) + j] = embedding[j]\nend\n\n\n@handle.pos[i] = pos || i\n\n\nif seq_ids.nil? || seq_ids.empty?\n  @handle.n_seq_id[i] = 1\n  @handle.seq_id[i][0] = 0\nelse\n  num_seq_ids = Math.min(seq_ids.size, @n_seq_max)\n  @handle.n_seq_id[i] = num_seq_ids\n\n  num_seq_ids.times do |j|\n    @handle.seq_id[i][j] = seq_ids[j]\n  end\nend\n\n\nif logits\n  @handle.logits[i] = logits ? 1_i8 : 0_i8\nend\n"},"external_var":false},{"html_id":"set_token(i:Int32,token:Int32,pos:Int32|Nil=nil,seq_ids:Array(Int32)|Nil=nil,logits:Bool|Nil=nil)-instance-method","name":"set_token","doc":"Sets a token at the specified index\n\nParameters:\n- i: Index in the batch\n- token: Token ID to set\n- pos: Position of the token in the sequence (nil for auto-position)\n- seq_ids: Sequence IDs (nil for default sequence 0)\n- logits: Whether to compute logits for this token (nil for default)\n\nRaises:\n- IndexError if the index is out of bounds\n- Llama::Batch::Error if memory allocation fails","summary":"<p>Sets a token at the specified index</p>","abstract":false,"args":[{"name":"i","external_name":"i","restriction":"Int32"},{"name":"token","external_name":"token","restriction":"Int32"},{"name":"pos","default_value":"nil","external_name":"pos","restriction":"Int32 | ::Nil"},{"name":"seq_ids","default_value":"nil","external_name":"seq_ids","restriction":"Array(Int32) | ::Nil"},{"name":"logits","default_value":"nil","external_name":"logits","restriction":"Bool | ::Nil"}],"args_string":"(i : Int32, token : Int32, pos : Int32 | Nil = nil, seq_ids : Array(Int32) | Nil = nil, logits : Bool | Nil = nil)","args_html":"(i : Int32, token : Int32, pos : Int32 | Nil = <span class=\"n\">nil</span>, seq_ids : Array(Int32) | Nil = <span class=\"n\">nil</span>, logits : Bool | Nil = <span class=\"n\">nil</span>)","location":{"filename":"src/llama/batch.cr","line_number":137,"url":"https://github.com/kojix2/llama.cr/blob/038f4ed10509841d5e14fe7055fbc1a21dac6a15/src/llama/batch.cr#L137"},"def":{"name":"set_token","args":[{"name":"i","external_name":"i","restriction":"Int32"},{"name":"token","external_name":"token","restriction":"Int32"},{"name":"pos","default_value":"nil","external_name":"pos","restriction":"Int32 | ::Nil"},{"name":"seq_ids","default_value":"nil","external_name":"seq_ids","restriction":"Array(Int32) | ::Nil"},{"name":"logits","default_value":"nil","external_name":"logits","restriction":"Bool | ::Nil"}],"visibility":"Public","body":"if i < 0 || i >= @handle.n_tokens\n  raise(IndexError.new(\"Index out of bounds: #{i} (valid range: 0..#{@handle.n_tokens - 1})\"))\nend\n\n\n@handle.token[i] = token\n\n\n@handle.pos[i] = pos || i\n\n\nif seq_ids.nil? || seq_ids.empty?\n  @handle.n_seq_id[i] = 1\n  @handle.seq_id[i][0] = 0\nelse\n  num_seq_ids = Math.min(seq_ids.size, @n_seq_max)\n  @handle.n_seq_id[i] = num_seq_ids\n\n  num_seq_ids.times do |j|\n    @handle.seq_id[i][j] = seq_ids[j]\n  end\nend\n\n\nif logits\n  @handle.logits[i] = logits ? 1_i8 : 0_i8\nend\n"},"external_var":false},{"html_id":"to_unsafe:Llama::LibLlama::LlamaBatch-instance-method","name":"to_unsafe","doc":"Returns the raw pointer to the underlying llama_batch structure","summary":"<p>Returns the raw pointer to the underlying llama_batch structure</p>","abstract":false,"location":{"filename":"src/llama/batch.cr","line_number":354,"url":"https://github.com/kojix2/llama.cr/blob/038f4ed10509841d5e14fe7055fbc1a21dac6a15/src/llama/batch.cr#L354"},"def":{"name":"to_unsafe","visibility":"Public","body":"@handle"},"external_var":false}],"types":[{"html_id":"llama/Llama/Batch/Error","path":"Llama/Batch/Error.html","kind":"class","full_name":"Llama::Batch::Error","name":"Error","abstract":false,"superclass":{"html_id":"llama/Llama/Error","kind":"class","full_name":"Llama::Error","name":"Error"},"ancestors":[{"html_id":"llama/Llama/Error","kind":"class","full_name":"Llama::Error","name":"Error"},{"html_id":"llama/Exception","kind":"class","full_name":"Exception","name":"Exception"},{"html_id":"llama/Reference","kind":"class","full_name":"Reference","name":"Reference"},{"html_id":"llama/Object","kind":"class","full_name":"Object","name":"Object"}],"locations":[{"filename":"src/llama/batch/error.cr","line_number":5,"url":"https://github.com/kojix2/llama.cr/blob/038f4ed10509841d5e14fe7055fbc1a21dac6a15/src/llama/batch/error.cr#L5"}],"repository_name":"llama","program":false,"enum":false,"alias":false,"const":false,"namespace":{"html_id":"llama/Llama/Batch","kind":"class","full_name":"Llama::Batch","name":"Batch"}}]},{"html_id":"llama/Llama/ChatMessage","path":"Llama/ChatMessage.html","kind":"class","full_name":"Llama::ChatMessage","name":"ChatMessage","abstract":false,"superclass":{"html_id":"llama/Reference","kind":"class","full_name":"Reference","name":"Reference"},"ancestors":[{"html_id":"llama/Reference","kind":"class","full_name":"Reference","name":"Reference"},{"html_id":"llama/Object","kind":"class","full_name":"Object","name":"Object"}],"locations":[{"filename":"src/llama/chat.cr","line_number":3,"url":"https://github.com/kojix2/llama.cr/blob/038f4ed10509841d5e14fe7055fbc1a21dac6a15/src/llama/chat.cr#L3"}],"repository_name":"llama","program":false,"enum":false,"alias":false,"const":false,"namespace":{"html_id":"llama/Llama","kind":"module","full_name":"Llama","name":"Llama"},"doc":"Represents a message in a chat conversation","summary":"<p>Represents a message in a chat conversation</p>","constructors":[{"html_id":"new(role:String,content:String)-class-method","name":"new","doc":"Creates a new ChatMessage\n\nParameters:\n- role: The role of the message sender\n- content: The content of the message","summary":"<p>Creates a new ChatMessage</p>","abstract":false,"args":[{"name":"role","external_name":"role","restriction":"String"},{"name":"content","external_name":"content","restriction":"String"}],"args_string":"(role : String, content : String)","args_html":"(role : String, content : String)","location":{"filename":"src/llama/chat.cr","line_number":15,"url":"https://github.com/kojix2/llama.cr/blob/038f4ed10509841d5e14fe7055fbc1a21dac6a15/src/llama/chat.cr#L15"},"def":{"name":"new","args":[{"name":"role","external_name":"role","restriction":"String"},{"name":"content","external_name":"content","restriction":"String"}],"visibility":"Public","body":"_ = allocate\n_.initialize(role, content)\nif _.responds_to?(:finalize)\n  ::GC.add_finalizer(_)\nend\n_\n"},"external_var":false}],"instance_methods":[{"html_id":"content:String-instance-method","name":"content","doc":"The content of the message","summary":"<p>The content of the message</p>","abstract":false,"location":{"filename":"src/llama/chat.cr","line_number":8,"url":"https://github.com/kojix2/llama.cr/blob/038f4ed10509841d5e14fe7055fbc1a21dac6a15/src/llama/chat.cr#L8"},"def":{"name":"content","return_type":"String","visibility":"Public","body":"@content"},"external_var":false},{"html_id":"content=(content:String)-instance-method","name":"content=","doc":"The content of the message","summary":"<p>The content of the message</p>","abstract":false,"args":[{"name":"content","external_name":"content","restriction":"String"}],"args_string":"(content : String)","args_html":"(content : String)","location":{"filename":"src/llama/chat.cr","line_number":8,"url":"https://github.com/kojix2/llama.cr/blob/038f4ed10509841d5e14fe7055fbc1a21dac6a15/src/llama/chat.cr#L8"},"def":{"name":"content=","args":[{"name":"content","external_name":"content","restriction":"String"}],"visibility":"Public","body":"@content = content"},"external_var":false},{"html_id":"role:String-instance-method","name":"role","doc":"The role of the message sender (e.g., \"system\", \"user\", \"assistant\")","summary":"<p>The role of the message sender (e.g., &quot;system&quot;, &quot;user&quot;, &quot;assistant&quot;)</p>","abstract":false,"location":{"filename":"src/llama/chat.cr","line_number":5,"url":"https://github.com/kojix2/llama.cr/blob/038f4ed10509841d5e14fe7055fbc1a21dac6a15/src/llama/chat.cr#L5"},"def":{"name":"role","return_type":"String","visibility":"Public","body":"@role"},"external_var":false},{"html_id":"role=(role:String)-instance-method","name":"role=","doc":"The role of the message sender (e.g., \"system\", \"user\", \"assistant\")","summary":"<p>The role of the message sender (e.g., &quot;system&quot;, &quot;user&quot;, &quot;assistant&quot;)</p>","abstract":false,"args":[{"name":"role","external_name":"role","restriction":"String"}],"args_string":"(role : String)","args_html":"(role : String)","location":{"filename":"src/llama/chat.cr","line_number":5,"url":"https://github.com/kojix2/llama.cr/blob/038f4ed10509841d5e14fe7055fbc1a21dac6a15/src/llama/chat.cr#L5"},"def":{"name":"role=","args":[{"name":"role","external_name":"role","restriction":"String"}],"visibility":"Public","body":"@role = role"},"external_var":false},{"html_id":"to_unsafe:LibLlama::LlamaChatMessage-instance-method","name":"to_unsafe","doc":"Converts to the C structure","summary":"<p>Converts to the C structure</p>","abstract":false,"location":{"filename":"src/llama/chat.cr","line_number":19,"url":"https://github.com/kojix2/llama.cr/blob/038f4ed10509841d5e14fe7055fbc1a21dac6a15/src/llama/chat.cr#L19"},"def":{"name":"to_unsafe","return_type":"LibLlama::LlamaChatMessage","visibility":"Public","body":"msg = LibLlama::LlamaChatMessage.new\nmsg.role = @role.to_unsafe\nmsg.content = @content.to_unsafe\nmsg\n"},"external_var":false}]},{"html_id":"llama/Llama/Context","path":"Llama/Context.html","kind":"class","full_name":"Llama::Context","name":"Context","abstract":false,"superclass":{"html_id":"llama/Reference","kind":"class","full_name":"Reference","name":"Reference"},"ancestors":[{"html_id":"llama/Reference","kind":"class","full_name":"Reference","name":"Reference"},{"html_id":"llama/Object","kind":"class","full_name":"Object","name":"Object"}],"locations":[{"filename":"src/llama/context.cr","line_number":5,"url":"https://github.com/kojix2/llama.cr/blob/038f4ed10509841d5e14fe7055fbc1a21dac6a15/src/llama/context.cr#L5"},{"filename":"src/llama/context/error.cr","line_number":4,"url":"https://github.com/kojix2/llama.cr/blob/038f4ed10509841d5e14fe7055fbc1a21dac6a15/src/llama/context/error.cr#L4"}],"repository_name":"llama","program":false,"enum":false,"alias":false,"const":false,"namespace":{"html_id":"llama/Llama","kind":"module","full_name":"Llama","name":"Llama"},"doc":"Wrapper for the llama_context structure","summary":"<p>Wrapper for the llama_context structure</p>","constructors":[{"html_id":"new(model:Model,n_ctx:UInt32=0,n_batch:UInt32=512,n_threads:Int32=0,n_threads_batch:Int32=0,embeddings:Bool=false,offload_kqv:Bool=false)-class-method","name":"new","doc":"Creates a new Context instance for a model.\n\nParameters:\n- model: The Model to create a context for.\n- n_ctx: Text context (default: 0). The maximum context size. If 0, a minimum context size of 512 is used.\n- n_batch: Logical maximum batch size that can be submitted to llama_decode (default: 512).\n- n_threads: Number of threads to use for generation (default: 0). If 0, uses the number of hardware threads.\n- n_threads_batch: Number of threads to use for batch processing (default: 0). If 0, uses the number of hardware threads.\n- embeddings: Extract embeddings (together with logits) (default: false). If true, extract embeddings (together with logits).\n- offload_kqv: Whether to offload the KQV ops (including the KV cache) to GPU (default: false). Requires a GPU build of llama.cpp.\n\nRaises:\n- Llama::Context::Error if the context cannot be created.","summary":"<p>Creates a new Context instance for a model.</p>","abstract":false,"args":[{"name":"model","external_name":"model","restriction":"Model"},{"name":"n_ctx","default_value":"0","external_name":"n_ctx","restriction":"UInt32"},{"name":"n_batch","default_value":"512","external_name":"n_batch","restriction":"UInt32"},{"name":"n_threads","default_value":"0","external_name":"n_threads","restriction":"Int32"},{"name":"n_threads_batch","default_value":"0","external_name":"n_threads_batch","restriction":"Int32"},{"name":"embeddings","default_value":"false","external_name":"embeddings","restriction":"Bool"},{"name":"offload_kqv","default_value":"false","external_name":"offload_kqv","restriction":"Bool"}],"args_string":"(model : Model, n_ctx : UInt32 = 0, n_batch : UInt32 = 512, n_threads : Int32 = 0, n_threads_batch : Int32 = 0, embeddings : Bool = false, offload_kqv : Bool = false)","args_html":"(model : <a href=\"../Llama/Model.html\">Model</a>, n_ctx : UInt32 = <span class=\"n\">0</span>, n_batch : UInt32 = <span class=\"n\">512</span>, n_threads : Int32 = <span class=\"n\">0</span>, n_threads_batch : Int32 = <span class=\"n\">0</span>, embeddings : Bool = <span class=\"n\">false</span>, offload_kqv : Bool = <span class=\"n\">false</span>)","location":{"filename":"src/llama/context.cr","line_number":19,"url":"https://github.com/kojix2/llama.cr/blob/038f4ed10509841d5e14fe7055fbc1a21dac6a15/src/llama/context.cr#L19"},"def":{"name":"new","args":[{"name":"model","external_name":"model","restriction":"Model"},{"name":"n_ctx","default_value":"0","external_name":"n_ctx","restriction":"UInt32"},{"name":"n_batch","default_value":"512","external_name":"n_batch","restriction":"UInt32"},{"name":"n_threads","default_value":"0","external_name":"n_threads","restriction":"Int32"},{"name":"n_threads_batch","default_value":"0","external_name":"n_threads_batch","restriction":"Int32"},{"name":"embeddings","default_value":"false","external_name":"embeddings","restriction":"Bool"},{"name":"offload_kqv","default_value":"false","external_name":"offload_kqv","restriction":"Bool"}],"visibility":"Public","body":"_ = allocate\n_.initialize(model, n_ctx, n_batch, n_threads, n_threads_batch, embeddings, offload_kqv)\nif _.responds_to?(:finalize)\n  ::GC.add_finalizer(_)\nend\n_\n"},"external_var":false}],"instance_methods":[{"html_id":"chat(messages:Array(ChatMessage),max_tokens:Int32=128,temperature:Float32=0.8,template:String|Nil=nil):String-instance-method","name":"chat","doc":"Generates a response in a chat conversation\n\nParameters:\n- messages: Array of chat messages\n- max_tokens: Maximum number of tokens to generate\n- temperature: Sampling temperature\n- template: Optional chat template (nil to use model's default)\n\nReturns:\n- The generated response text\n\nRaises:\n- ArgumentError if parameters are invalid\n- Llama::Context::Error if text generation fails\n- Llama::TokenizationError if the prompt cannot be tokenized","summary":"<p>Generates a response in a chat conversation</p>","abstract":false,"args":[{"name":"messages","external_name":"messages","restriction":"Array(ChatMessage)"},{"name":"max_tokens","default_value":"128","external_name":"max_tokens","restriction":"Int32"},{"name":"temperature","default_value":"0.8","external_name":"temperature","restriction":"Float32"},{"name":"template","default_value":"nil","external_name":"template","restriction":"String | ::Nil"}],"args_string":"(messages : Array(ChatMessage), max_tokens : Int32 = 128, temperature : Float32 = 0.8, template : String | Nil = nil) : String","args_html":"(messages : Array(<a href=\"../Llama/ChatMessage.html\">ChatMessage</a>), max_tokens : Int32 = <span class=\"n\">128</span>, temperature : Float32 = <span class=\"n\">0.8</span>, template : String | Nil = <span class=\"n\">nil</span>) : String","location":{"filename":"src/llama/context.cr","line_number":102,"url":"https://github.com/kojix2/llama.cr/blob/038f4ed10509841d5e14fe7055fbc1a21dac6a15/src/llama/context.cr#L102"},"def":{"name":"chat","args":[{"name":"messages","external_name":"messages","restriction":"Array(ChatMessage)"},{"name":"max_tokens","default_value":"128","external_name":"max_tokens","restriction":"Int32"},{"name":"temperature","default_value":"0.8","external_name":"temperature","restriction":"Float32"},{"name":"template","default_value":"nil","external_name":"template","restriction":"String | ::Nil"}],"return_type":"String","visibility":"Public","body":"if max_tokens <= 0\n  raise(ArgumentError.new(\"max_tokens must be positive\"))\nend\n\nif temperature < 0\n  raise(ArgumentError.new(\"temperature must be non-negative\"))\nend\n\nif messages.empty?\n  raise(ArgumentError.new(\"messages array cannot be empty\"))\nend\n\n\ntemplate_to_use = template || @model.chat_template\nif template_to_use.nil?\n  error_msg = Llama.format_error(\"No chat template available\", nil, \"model does not provide a default chat template and none was specified\")\n  raise(Context::Error.new(error_msg))\nend\n\nbegin\n  prompt = Llama.apply_chat_template(template_to_use, messages, true)\nrescue ex\n  error_msg = Llama.format_error(\"Failed to apply chat template\", nil, ex.message)\n  raise(Context::Error.new(error_msg))\nend\n\n\ngenerate(prompt, max_tokens, temperature)\n"},"external_var":false},{"html_id":"cleanup-instance-method","name":"cleanup","doc":"Explicitly clean up resources\nThis can be called manually to release resources before garbage collection","summary":"<p>Explicitly clean up resources This can be called manually to release resources before garbage collection</p>","abstract":false,"location":{"filename":"src/llama/context.cr","line_number":76,"url":"https://github.com/kojix2/llama.cr/blob/038f4ed10509841d5e14fe7055fbc1a21dac6a15/src/llama/context.cr#L76"},"def":{"name":"cleanup","visibility":"Public","body":"if @handle && !@handle.null?\n  LibLlama.llama_free(@handle)\nend\n\n\n@kv_cache = nil\n@state = nil\n"},"external_var":false},{"html_id":"decode(batch:LibLlama::LlamaBatch|Batch):Int32-instance-method","name":"decode","doc":"Processes a batch of tokens with the decoder part of the model\n\nParameters:\n- batch: The batch to process (can be a LibLlama::LlamaBatch or a Batch instance)\n\nReturns:\n- 0 on success\n- 1 if no KV slot was found for the batch\n- < 0 on error\n\nRaises:\n- Llama::Batch::Error on error","summary":"<p>Processes a batch of tokens with the decoder part of the model</p>","abstract":false,"args":[{"name":"batch","external_name":"batch","restriction":"LibLlama::LlamaBatch | Batch"}],"args_string":"(batch : LibLlama::LlamaBatch | Batch) : Int32","args_html":"(batch : LibLlama::LlamaBatch | <a href=\"../Llama/Batch.html\">Batch</a>) : Int32","location":{"filename":"src/llama/context.cr","line_number":356,"url":"https://github.com/kojix2/llama.cr/blob/038f4ed10509841d5e14fe7055fbc1a21dac6a15/src/llama/context.cr#L356"},"def":{"name":"decode","args":[{"name":"batch","external_name":"batch","restriction":"LibLlama::LlamaBatch | Batch"}],"return_type":"Int32","visibility":"Public","body":"batch_ptr = batch.is_a?(Batch) ? batch.to_unsafe : batch\nresult = LibLlama.llama_decode(@handle, batch_ptr)\n\nif result < 0\n  error_msg = Llama.format_error(\"Failed to decode batch\", result, \"batch size: #{batch_ptr.n_tokens}\")\n  raise(Batch::Error.new(error_msg))\nend\n\nresult\n"},"external_var":false},{"html_id":"encode(batch:LibLlama::LlamaBatch|Batch):Int32-instance-method","name":"encode","doc":"Processes a batch of tokens with the encoder part of the model\n\nThis function is used for encoder-decoder models to encode the input\nbefore generating text with the decoder.\n\nParameters:\n- batch: The batch to process (can be a LibLlama::LlamaBatch or a Batch instance)\n\nReturns:\n- 0 on success\n- < 0 on error\n\nRaises:\n- Llama::Batch::Error on error","summary":"<p>Processes a batch of tokens with the encoder part of the model</p>","abstract":false,"args":[{"name":"batch","external_name":"batch","restriction":"LibLlama::LlamaBatch | Batch"}],"args_string":"(batch : LibLlama::LlamaBatch | Batch) : Int32","args_html":"(batch : LibLlama::LlamaBatch | <a href=\"../Llama/Batch.html\">Batch</a>) : Int32","location":{"filename":"src/llama/context.cr","line_number":328,"url":"https://github.com/kojix2/llama.cr/blob/038f4ed10509841d5e14fe7055fbc1a21dac6a15/src/llama/context.cr#L328"},"def":{"name":"encode","args":[{"name":"batch","external_name":"batch","restriction":"LibLlama::LlamaBatch | Batch"}],"return_type":"Int32","visibility":"Public","body":"batch_ptr = batch.is_a?(Batch) ? batch.to_unsafe : batch\nresult = LibLlama.llama_encode(@handle, batch_ptr)\n\nif result < 0\n  error_msg = Llama.format_error(\"Failed to encode batch\", result, \"batch size: #{batch_ptr.n_tokens}\")\n  raise(Batch::Error.new(error_msg))\nend\n\nresult\n"},"external_var":false},{"html_id":"finalize-instance-method","name":"finalize","doc":"Frees the resources associated with this context","summary":"<p>Frees the resources associated with this context</p>","abstract":false,"location":{"filename":"src/llama/context.cr","line_number":611,"url":"https://github.com/kojix2/llama.cr/blob/038f4ed10509841d5e14fe7055fbc1a21dac6a15/src/llama/context.cr#L611"},"def":{"name":"finalize","visibility":"Public","body":"cleanup"},"external_var":false},{"html_id":"generate(prompt:String,max_tokens:Int32=128,temperature:Float32=0.8):String-instance-method","name":"generate","doc":"Generates text from a prompt\n\nParameters:\n- prompt: The input prompt\n- max_tokens: Maximum number of tokens to generate (must be positive)\n- temperature: Sampling temperature (0.0 = greedy, 1.0 = more random)\n\nReturns:\n- The generated text\n\nRaises:\n- ArgumentError if parameters are invalid\n- Llama::Context::Error if text generation fails\n- Llama::TokenizationError if the prompt cannot be tokenized","summary":"<p>Generates text from a prompt</p>","abstract":false,"args":[{"name":"prompt","external_name":"prompt","restriction":"String"},{"name":"max_tokens","default_value":"128","external_name":"max_tokens","restriction":"Int32"},{"name":"temperature","default_value":"0.8","external_name":"temperature","restriction":"Float32"}],"args_string":"(prompt : String, max_tokens : Int32 = 128, temperature : Float32 = 0.8) : String","args_html":"(prompt : String, max_tokens : Int32 = <span class=\"n\">128</span>, temperature : Float32 = <span class=\"n\">0.8</span>) : String","location":{"filename":"src/llama/context.cr","line_number":405,"url":"https://github.com/kojix2/llama.cr/blob/038f4ed10509841d5e14fe7055fbc1a21dac6a15/src/llama/context.cr#L405"},"def":{"name":"generate","args":[{"name":"prompt","external_name":"prompt","restriction":"String"},{"name":"max_tokens","default_value":"128","external_name":"max_tokens","restriction":"Int32"},{"name":"temperature","default_value":"0.8","external_name":"temperature","restriction":"Float32"}],"return_type":"String","visibility":"Public","body":"if max_tokens <= 0\n  raise(ArgumentError.new(\"max_tokens must be positive\"))\nend\n\nif temperature < 0\n  raise(ArgumentError.new(\"temperature must be non-negative\"))\nend\n\nif prompt.empty?\n  raise(ArgumentError.new(\"prompt cannot be empty\"))\nend\n\n\ngenerate_internal(prompt, max_tokens) do |logits|\n  sample_token(logits, temperature)\nend\n"},"external_var":false},{"html_id":"generate_with_sampler(prompt:String,sampler:SamplerChain,max_tokens:Int32=128):String-instance-method","name":"generate_with_sampler","doc":"Generates text using a sampler chain\n\nParameters:\n- prompt: The input prompt\n- sampler: The sampler chain to use\n- max_tokens: Maximum number of tokens to generate (must be positive)\n\nReturns:\n- The generated text\n\nRaises:\n- ArgumentError if parameters are invalid\n- Llama::Context::Error if text generation fails\n- Llama::TokenizationError if the prompt cannot be tokenized\n- Llama::Sampler::Error if sampling fails","summary":"<p>Generates text using a sampler chain</p>","abstract":false,"args":[{"name":"prompt","external_name":"prompt","restriction":"String"},{"name":"sampler","external_name":"sampler","restriction":"SamplerChain"},{"name":"max_tokens","default_value":"128","external_name":"max_tokens","restriction":"Int32"}],"args_string":"(prompt : String, sampler : SamplerChain, max_tokens : Int32 = 128) : String","args_html":"(prompt : String, sampler : SamplerChain, max_tokens : Int32 = <span class=\"n\">128</span>) : String","location":{"filename":"src/llama/context.cr","line_number":283,"url":"https://github.com/kojix2/llama.cr/blob/038f4ed10509841d5e14fe7055fbc1a21dac6a15/src/llama/context.cr#L283"},"def":{"name":"generate_with_sampler","args":[{"name":"prompt","external_name":"prompt","restriction":"String"},{"name":"sampler","external_name":"sampler","restriction":"SamplerChain"},{"name":"max_tokens","default_value":"128","external_name":"max_tokens","restriction":"Int32"}],"return_type":"String","visibility":"Public","body":"if max_tokens <= 0\n  raise(ArgumentError.new(\"max_tokens must be positive\"))\nend\n\nif prompt.empty?\n  raise(ArgumentError.new(\"prompt cannot be empty\"))\nend\n\n\ngenerate_internal(prompt, max_tokens) do |logits|\n  begin\n    token = sampler.sample(self)\n\n\n    sampler.accept(token)\n\n    token\n  rescue ex\n    error_msg = Llama.format_error(\"Sampling failed\", -9, ex.message)\n    raise(Sampler::Error.new(error_msg))\n  end\nend\n"},"external_var":false},{"html_id":"get_embeddings:Array(Float32)|Nil-instance-method","name":"get_embeddings","doc":"Gets all output token embeddings\nOnly available when embeddings mode is enabled\n\nReturns:\n- An array of embeddings, or nil if embeddings are not available\n\nRaises:\n- Llama::Context::Error if embeddings mode is not enabled","summary":"<p>Gets all output token embeddings Only available when embeddings mode is enabled</p>","abstract":false,"location":{"filename":"src/llama/context.cr","line_number":657,"url":"https://github.com/kojix2/llama.cr/blob/038f4ed10509841d5e14fe7055fbc1a21dac6a15/src/llama/context.cr#L657"},"def":{"name":"get_embeddings","return_type":"Array(Float32) | ::Nil","visibility":"Public","body":"ptr = LibLlama.llama_get_embeddings(@handle)\nif ptr.null?\n  return nil\nend\n\n\nn_embd = @model.n_embd\n\n\n\n\nn_outputs = 1\n\n\nresult = Array(Float32).new(n_outputs * n_embd)\n(n_outputs * n_embd).times do |i|\n  result << ptr[i]\nend\n\nresult\n"},"external_var":false},{"html_id":"get_embeddings_ith(i:Int32):Array(Float32)|Nil-instance-method","name":"get_embeddings_ith","doc":"Gets the embeddings for a specific token\n\nParameters:\n- i: The token index (negative indices can be used to access in reverse order)\n\nReturns:\n- An array of embedding values, or nil if not available\n\nRaises:\n- Llama::Context::Error if embeddings mode is not enabled","summary":"<p>Gets the embeddings for a specific token</p>","abstract":false,"args":[{"name":"i","external_name":"i","restriction":"Int32"}],"args_string":"(i : Int32) : Array(Float32) | Nil","args_html":"(i : Int32) : Array(Float32) | Nil","location":{"filename":"src/llama/context.cr","line_number":688,"url":"https://github.com/kojix2/llama.cr/blob/038f4ed10509841d5e14fe7055fbc1a21dac6a15/src/llama/context.cr#L688"},"def":{"name":"get_embeddings_ith","args":[{"name":"i","external_name":"i","restriction":"Int32"}],"return_type":"Array(Float32) | ::Nil","visibility":"Public","body":"ptr = LibLlama.llama_get_embeddings_ith(@handle, i)\nif ptr.null?\n  return nil\nend\n\n\nn_embd = @model.n_embd\n\n\nresult = Array(Float32).new(n_embd)\nn_embd.times do |j|\n  result << ptr[j]\nend\n\nresult\n"},"external_var":false},{"html_id":"get_embeddings_seq(seq_id:Int32):Array(Float32)|Nil-instance-method","name":"get_embeddings_seq","doc":"Gets the embeddings for a specific sequence\n\nParameters:\n- seq_id: The sequence ID\n\nReturns:\n- An array of embedding values, or nil if not available\n\nRaises:\n- Llama::Context::Error if embeddings mode is not enabled","summary":"<p>Gets the embeddings for a specific sequence</p>","abstract":false,"args":[{"name":"seq_id","external_name":"seq_id","restriction":"Int32"}],"args_string":"(seq_id : Int32) : Array(Float32) | Nil","args_html":"(seq_id : Int32) : Array(Float32) | Nil","location":{"filename":"src/llama/context.cr","line_number":714,"url":"https://github.com/kojix2/llama.cr/blob/038f4ed10509841d5e14fe7055fbc1a21dac6a15/src/llama/context.cr#L714"},"def":{"name":"get_embeddings_seq","args":[{"name":"seq_id","external_name":"seq_id","restriction":"Int32"}],"return_type":"Array(Float32) | ::Nil","visibility":"Public","body":"ptr = LibLlama.llama_get_embeddings_seq(@handle, seq_id)\nif ptr.null?\n  return nil\nend\n\n\nn_embd = @model.n_embd\n\n\nresult = Array(Float32).new(n_embd)\nn_embd.times do |i|\n  result << ptr[i]\nend\n\nresult\n"},"external_var":false},{"html_id":"kv_cache:KvCache-instance-method","name":"kv_cache","doc":"Returns the KV cache for this context\nLazily initializes the KV cache if it doesn't exist yet","summary":"<p>Returns the KV cache for this context Lazily initializes the KV cache if it doesn't exist yet</p>","abstract":false,"location":{"filename":"src/llama/context.cr","line_number":64,"url":"https://github.com/kojix2/llama.cr/blob/038f4ed10509841d5e14fe7055fbc1a21dac6a15/src/llama/context.cr#L64"},"def":{"name":"kv_cache","return_type":"KvCache","visibility":"Public","body":"@kv_cache || (@kv_cache = KvCache.new(LibLlama.llama_get_kv_self(@handle), self))"},"external_var":false},{"html_id":"logits:Pointer(Float32)-instance-method","name":"logits","doc":"Gets the logits for the last token\n\nReturns:\n- A pointer to the logits array","summary":"<p>Gets the logits for the last token</p>","abstract":false,"location":{"filename":"src/llama/context.cr","line_number":376,"url":"https://github.com/kojix2/llama.cr/blob/038f4ed10509841d5e14fe7055fbc1a21dac6a15/src/llama/context.cr#L376"},"def":{"name":"logits","return_type":"Pointer(Float32)","visibility":"Public","body":"ptr = LibLlama.llama_get_logits(@handle)\n\nif ptr.null?\n  error_msg = Llama.format_error(\"Failed to get logits\", nil, \"logits pointer is null\")\n  raise(Context::Error.new(error_msg))\nend\n\nptr\n"},"external_var":false},{"html_id":"pooling_type:LibLlama::LlamaPoolingType-instance-method","name":"pooling_type","doc":"Gets the pooling type used for embeddings\n\nReturns:\n- The pooling type as a PoolingType enum","summary":"<p>Gets the pooling type used for embeddings</p>","abstract":false,"location":{"filename":"src/llama/context.cr","line_number":645,"url":"https://github.com/kojix2/llama.cr/blob/038f4ed10509841d5e14fe7055fbc1a21dac6a15/src/llama/context.cr#L645"},"def":{"name":"pooling_type","return_type":"LibLlama::LlamaPoolingType","visibility":"Public","body":"LibLlama.llama_pooling_type(@handle)"},"external_var":false},{"html_id":"print_perf-instance-method","name":"print_perf","doc":"Print performance information for this context\n\nThis method prints performance statistics about the context to STDERR.\nIt's useful for debugging and performance analysis.","summary":"<p>Print performance information for this context</p>","abstract":false,"location":{"filename":"src/llama/context.cr","line_number":619,"url":"https://github.com/kojix2/llama.cr/blob/038f4ed10509841d5e14fe7055fbc1a21dac6a15/src/llama/context.cr#L619"},"def":{"name":"print_perf","visibility":"Public","body":"LibLlama.llama_perf_context_print(@handle)"},"external_var":false},{"html_id":"process_embeddings(embeddings:Array(Array(Float32)),seq_ids:Array(Int32)|Nil=nil,n_seq_max:Int32=8):Int32-instance-method","name":"process_embeddings","doc":"Process embeddings\n\nParameters:\n- embeddings: Array of embedding vectors\n- seq_ids: Sequence IDs to use for all embeddings\n- n_seq_max: Maximum number of sequence IDs per token (default: 8)\n\nReturns:\n- The result of the decode operation (0 on success)\n\nRaises:\n- Llama::Batch::Error on error","summary":"<p>Process embeddings</p>","abstract":false,"args":[{"name":"embeddings","external_name":"embeddings","restriction":"Array(Array(Float32))"},{"name":"seq_ids","default_value":"nil","external_name":"seq_ids","restriction":"Array(Int32) | ::Nil"},{"name":"n_seq_max","default_value":"8","external_name":"n_seq_max","restriction":"Int32"}],"args_string":"(embeddings : Array(Array(Float32)), seq_ids : Array(Int32) | Nil = nil, n_seq_max : Int32 = 8) : Int32","args_html":"(embeddings : Array(Array(Float32)), seq_ids : Array(Int32) | Nil = <span class=\"n\">nil</span>, n_seq_max : Int32 = <span class=\"n\">8</span>) : Int32","location":{"filename":"src/llama/context.cr","line_number":233,"url":"https://github.com/kojix2/llama.cr/blob/038f4ed10509841d5e14fe7055fbc1a21dac6a15/src/llama/context.cr#L233"},"def":{"name":"process_embeddings","args":[{"name":"embeddings","external_name":"embeddings","restriction":"Array(Array(Float32))"},{"name":"seq_ids","default_value":"nil","external_name":"seq_ids","restriction":"Array(Int32) | ::Nil"},{"name":"n_seq_max","default_value":"8","external_name":"n_seq_max","restriction":"Int32"}],"return_type":"Int32","visibility":"Public","body":"if embeddings.empty?\n  raise(ArgumentError.new(\"embeddings array cannot be empty\"))\nend\n\nbatch = Batch.for_embeddings(embeddings, seq_ids, n_seq_max)\ndecode(batch)\n"},"external_var":false},{"html_id":"process_prompts(prompts:Array(String)):Array(Int32)-instance-method","name":"process_prompts","doc":"Process multiple prompts in batch\n\nParameters:\n- prompts: Array of text prompts to process\n- compute_logits_for_last: Whether to compute logits only for the last token of each prompt\n\nReturns:\n- Array of decode operation results (0 on success)\n\nRaises:\n- Llama::Batch::Error on error\n- Llama::TokenizationError if a prompt cannot be tokenized","summary":"<p>Process multiple prompts in batch</p>","abstract":false,"args":[{"name":"prompts","external_name":"prompts","restriction":"Array(String)"}],"args_string":"(prompts : Array(String)) : Array(Int32)","args_html":"(prompts : Array(String)) : Array(Int32)","location":{"filename":"src/llama/context.cr","line_number":183,"url":"https://github.com/kojix2/llama.cr/blob/038f4ed10509841d5e14fe7055fbc1a21dac6a15/src/llama/context.cr#L183"},"def":{"name":"process_prompts","args":[{"name":"prompts","external_name":"prompts","restriction":"Array(String)"}],"return_type":"Array(Int32)","visibility":"Public","body":"if prompts.empty?\n  raise(ArgumentError.new(\"prompts array cannot be empty\"))\nend\n\nresults = [] of Int32\n\nprompts.each_with_index do |prompt, i|\n  begin\n    tokens = @model.vocab.tokenize(prompt)\n\n    if tokens.empty?\n      error_msg = Llama.format_error(\"Tokenization resulted in empty token array\", -6, \"prompt index: #{i}\")\n      raise(TokenizationError.new(error_msg))\n    end\n\n    batch = Batch.for_tokens(tokens, true, nil, 8)\n    result = decode(batch)\n    results << result\n  rescue ex : TokenizationError\n    raise(ex)\n  rescue ex\n    error_msg = Llama.format_error(\"Failed to process prompt\", -3, \"prompt index: #{i}, error: #{ex.message}\")\n    raise(Batch::Error.new(error_msg))\n  end\nend\n\nresults\n"},"external_var":false},{"html_id":"process_tokens(tokens:Array(Int32),compute_logits_for_last:Bool=true,seq_ids:Array(Int32)|Nil=nil,n_seq_max:Int32=8):Int32-instance-method","name":"process_tokens","doc":"Process a sequence of tokens\n\nParameters:\n- tokens: Array of token IDs to process\n- compute_logits_for_last: Whether to compute logits only for the last token\n- seq_ids: Sequence IDs to use for all tokens\n- n_seq_max: Maximum number of sequence IDs per token (default: 8)\n\nReturns:\n- The result of the decode operation (0 on success)\n\nRaises:\n- Llama::Batch::Error on error","summary":"<p>Process a sequence of tokens</p>","abstract":false,"args":[{"name":"tokens","external_name":"tokens","restriction":"Array(Int32)"},{"name":"compute_logits_for_last","default_value":"true","external_name":"compute_logits_for_last","restriction":"Bool"},{"name":"seq_ids","default_value":"nil","external_name":"seq_ids","restriction":"Array(Int32) | ::Nil"},{"name":"n_seq_max","default_value":"8","external_name":"n_seq_max","restriction":"Int32"}],"args_string":"(tokens : Array(Int32), compute_logits_for_last : Bool = true, seq_ids : Array(Int32) | Nil = nil, n_seq_max : Int32 = 8) : Int32","args_html":"(tokens : Array(Int32), compute_logits_for_last : Bool = <span class=\"n\">true</span>, seq_ids : Array(Int32) | Nil = <span class=\"n\">nil</span>, n_seq_max : Int32 = <span class=\"n\">8</span>) : Int32","location":{"filename":"src/llama/context.cr","line_number":162,"url":"https://github.com/kojix2/llama.cr/blob/038f4ed10509841d5e14fe7055fbc1a21dac6a15/src/llama/context.cr#L162"},"def":{"name":"process_tokens","args":[{"name":"tokens","external_name":"tokens","restriction":"Array(Int32)"},{"name":"compute_logits_for_last","default_value":"true","external_name":"compute_logits_for_last","restriction":"Bool"},{"name":"seq_ids","default_value":"nil","external_name":"seq_ids","restriction":"Array(Int32) | ::Nil"},{"name":"n_seq_max","default_value":"8","external_name":"n_seq_max","restriction":"Int32"}],"return_type":"Int32","visibility":"Public","body":"if tokens.empty?\n  raise(ArgumentError.new(\"tokens array cannot be empty\"))\nend\n\nbatch = Batch.for_tokens(tokens, compute_logits_for_last, seq_ids, n_seq_max)\ndecode(batch)\n"},"external_var":false},{"html_id":"reset_perf-instance-method","name":"reset_perf","doc":"Reset performance counters for this context\n\nThis method resets all performance counters for the context.","summary":"<p>Reset performance counters for this context</p>","abstract":false,"location":{"filename":"src/llama/context.cr","line_number":626,"url":"https://github.com/kojix2/llama.cr/blob/038f4ed10509841d5e14fe7055fbc1a21dac6a15/src/llama/context.cr#L626"},"def":{"name":"reset_perf","visibility":"Public","body":"LibLlama.llama_perf_context_reset(@handle)"},"external_var":false},{"html_id":"set_embeddings(enabled:Bool)-instance-method","name":"set_embeddings","doc":"Sets whether the model is in embeddings mode or not\nIf true, embeddings will be returned but logits will not\n\nParameters:\n- enabled: Whether to enable embeddings mode","summary":"<p>Sets whether the model is in embeddings mode or not If true, embeddings will be returned but logits will not</p>","abstract":false,"args":[{"name":"enabled","external_name":"enabled","restriction":"Bool"}],"args_string":"(enabled : Bool)","args_html":"(enabled : Bool)","location":{"filename":"src/llama/context.cr","line_number":637,"url":"https://github.com/kojix2/llama.cr/blob/038f4ed10509841d5e14fe7055fbc1a21dac6a15/src/llama/context.cr#L637"},"def":{"name":"set_embeddings","args":[{"name":"enabled","external_name":"enabled","restriction":"Bool"}],"visibility":"Public","body":"LibLlama.llama_set_embeddings(@handle, enabled)"},"external_var":false},{"html_id":"state:State-instance-method","name":"state","doc":"Returns the state manager for this context\nLazily initializes the state if it doesn't exist yet","summary":"<p>Returns the state manager for this context Lazily initializes the state if it doesn't exist yet</p>","abstract":false,"location":{"filename":"src/llama/context.cr","line_number":70,"url":"https://github.com/kojix2/llama.cr/blob/038f4ed10509841d5e14fe7055fbc1a21dac6a15/src/llama/context.cr#L70"},"def":{"name":"state","return_type":"State","visibility":"Public","body":"@state || (@state = State.new(self))"},"external_var":false},{"html_id":"to_unsafe:Pointer(Llama::LibLlama::LlamaContext)-instance-method","name":"to_unsafe","doc":"Returns the raw pointer to the underlying llama_context structure","summary":"<p>Returns the raw pointer to the underlying llama_context structure</p>","abstract":false,"location":{"filename":"src/llama/context.cr","line_number":606,"url":"https://github.com/kojix2/llama.cr/blob/038f4ed10509841d5e14fe7055fbc1a21dac6a15/src/llama/context.cr#L606"},"def":{"name":"to_unsafe","visibility":"Public","body":"@handle"},"external_var":false}],"types":[{"html_id":"llama/Llama/Context/Error","path":"Llama/Context/Error.html","kind":"class","full_name":"Llama::Context::Error","name":"Error","abstract":false,"superclass":{"html_id":"llama/Llama/Error","kind":"class","full_name":"Llama::Error","name":"Error"},"ancestors":[{"html_id":"llama/Llama/Error","kind":"class","full_name":"Llama::Error","name":"Error"},{"html_id":"llama/Exception","kind":"class","full_name":"Exception","name":"Exception"},{"html_id":"llama/Reference","kind":"class","full_name":"Reference","name":"Reference"},{"html_id":"llama/Object","kind":"class","full_name":"Object","name":"Object"}],"locations":[{"filename":"src/llama/context/error.cr","line_number":5,"url":"https://github.com/kojix2/llama.cr/blob/038f4ed10509841d5e14fe7055fbc1a21dac6a15/src/llama/context/error.cr#L5"}],"repository_name":"llama","program":false,"enum":false,"alias":false,"const":false,"namespace":{"html_id":"llama/Llama/Context","kind":"class","full_name":"Llama::Context","name":"Context"}},{"html_id":"llama/Llama/Context/TokenizationError","path":"Llama/Context/TokenizationError.html","kind":"class","full_name":"Llama::Context::TokenizationError","name":"TokenizationError","abstract":false,"superclass":{"html_id":"llama/Llama/Error","kind":"class","full_name":"Llama::Error","name":"Error"},"ancestors":[{"html_id":"llama/Llama/Error","kind":"class","full_name":"Llama::Error","name":"Error"},{"html_id":"llama/Exception","kind":"class","full_name":"Exception","name":"Exception"},{"html_id":"llama/Reference","kind":"class","full_name":"Reference","name":"Reference"},{"html_id":"llama/Object","kind":"class","full_name":"Object","name":"Object"}],"locations":[{"filename":"src/llama/context/error.cr","line_number":7,"url":"https://github.com/kojix2/llama.cr/blob/038f4ed10509841d5e14fe7055fbc1a21dac6a15/src/llama/context/error.cr#L7"}],"repository_name":"llama","program":false,"enum":false,"alias":false,"const":false,"namespace":{"html_id":"llama/Llama/Context","kind":"class","full_name":"Llama::Context","name":"Context"}}]},{"html_id":"llama/Llama/Error","path":"Llama/Error.html","kind":"class","full_name":"Llama::Error","name":"Error","abstract":false,"superclass":{"html_id":"llama/Exception","kind":"class","full_name":"Exception","name":"Exception"},"ancestors":[{"html_id":"llama/Exception","kind":"class","full_name":"Exception","name":"Exception"},{"html_id":"llama/Reference","kind":"class","full_name":"Reference","name":"Reference"},{"html_id":"llama/Object","kind":"class","full_name":"Object","name":"Object"}],"locations":[{"filename":"src/llama/error.cr","line_number":2,"url":"https://github.com/kojix2/llama.cr/blob/038f4ed10509841d5e14fe7055fbc1a21dac6a15/src/llama/error.cr#L2"}],"repository_name":"llama","program":false,"enum":false,"alias":false,"const":false,"constants":[{"id":"ERROR_MESSAGES","name":"ERROR_MESSAGES","value":"{-1 => \"General error\", -2 => \"Memory allocation error\", -3 => \"Batch processing error\", -4 => \"Context creation error\", -5 => \"Model loading error\", -6 => \"Tokenization error\", -7 => \"KV cache error\", -8 => \"State management error\", -9 => \"Sampling error\", -10 => \"Invalid parameter error\", -11 => \"File I/O error\", -12 => \"Network error\", -13 => \"GPU error\", -14 => \"Timeout error\", -15 => \"Unsupported operation error\"}"}],"subclasses":[{"html_id":"llama/Llama/Batch/Error","kind":"class","full_name":"Llama::Batch::Error","name":"Error"},{"html_id":"llama/Llama/Context/Error","kind":"class","full_name":"Llama::Context::Error","name":"Error"},{"html_id":"llama/Llama/Context/TokenizationError","kind":"class","full_name":"Llama::Context::TokenizationError","name":"TokenizationError"},{"html_id":"llama/Llama/KvCache/Error","kind":"class","full_name":"Llama::KvCache::Error","name":"Error"},{"html_id":"llama/Llama/Model/Error","kind":"class","full_name":"Llama::Model::Error","name":"Error"},{"html_id":"llama/Llama/Sampler/Error","kind":"class","full_name":"Llama::Sampler::Error","name":"Error"},{"html_id":"llama/Llama/State/Error","kind":"class","full_name":"Llama::State::Error","name":"Error"}],"namespace":{"html_id":"llama/Llama","kind":"module","full_name":"Llama","name":"Llama"},"class_methods":[{"html_id":"error_message(code:Int32):String-class-method","name":"error_message","abstract":false,"args":[{"name":"code","external_name":"code","restriction":"Int32"}],"args_string":"(code : Int32) : String","args_html":"(code : Int32) : String","location":{"filename":"src/llama/error.cr","line_number":21,"url":"https://github.com/kojix2/llama.cr/blob/038f4ed10509841d5e14fe7055fbc1a21dac6a15/src/llama/error.cr#L21"},"def":{"name":"error_message","args":[{"name":"code","external_name":"code","restriction":"Int32"}],"return_type":"String","visibility":"Public","body":"ERROR_MESSAGES[code]? || \"Unknown error (code: #{code})\""},"external_var":false},{"html_id":"format_error(message:String,code:Int32|Nil=nil,context:String|Nil=nil):String-class-method","name":"format_error","abstract":false,"args":[{"name":"message","external_name":"message","restriction":"String"},{"name":"code","default_value":"nil","external_name":"code","restriction":"Int32 | ::Nil"},{"name":"context","default_value":"nil","external_name":"context","restriction":"String | ::Nil"}],"args_string":"(message : String, code : Int32 | Nil = nil, context : String | Nil = nil) : String","args_html":"(message : String, code : Int32 | Nil = <span class=\"n\">nil</span>, context : String | Nil = <span class=\"n\">nil</span>) : String","location":{"filename":"src/llama/error.cr","line_number":25,"url":"https://github.com/kojix2/llama.cr/blob/038f4ed10509841d5e14fe7055fbc1a21dac6a15/src/llama/error.cr#L25"},"def":{"name":"format_error","args":[{"name":"message","external_name":"message","restriction":"String"},{"name":"code","default_value":"nil","external_name":"code","restriction":"Int32 | ::Nil"},{"name":"context","default_value":"nil","external_name":"context","restriction":"String | ::Nil"}],"return_type":"String","visibility":"Public","body":"result = message\nif code\n  error_msg = error_message(code)\n  result = result + \" - #{error_msg} (code: #{code})\"\nend\nif context\n  result = result + \" [#{context}]\"\nend\nresult\n"},"external_var":false}]},{"html_id":"llama/Llama/KvCache","path":"Llama/KvCache.html","kind":"class","full_name":"Llama::KvCache","name":"KvCache","abstract":false,"superclass":{"html_id":"llama/Reference","kind":"class","full_name":"Reference","name":"Reference"},"ancestors":[{"html_id":"llama/Reference","kind":"class","full_name":"Reference","name":"Reference"},{"html_id":"llama/Object","kind":"class","full_name":"Object","name":"Object"}],"locations":[{"filename":"src/llama/kv_cache.cr","line_number":6,"url":"https://github.com/kojix2/llama.cr/blob/038f4ed10509841d5e14fe7055fbc1a21dac6a15/src/llama/kv_cache.cr#L6"},{"filename":"src/llama/kv_cache/error.cr","line_number":4,"url":"https://github.com/kojix2/llama.cr/blob/038f4ed10509841d5e14fe7055fbc1a21dac6a15/src/llama/kv_cache/error.cr#L4"}],"repository_name":"llama","program":false,"enum":false,"alias":false,"const":false,"namespace":{"html_id":"llama/Llama","kind":"module","full_name":"Llama","name":"Llama"},"doc":"Wrapper for the llama_kv_cache structure\nProvides methods for managing the KV (Key-Value) cache in LLaMA models","summary":"<p>Wrapper for the llama_kv_cache structure Provides methods for managing the KV (Key-Value) cache in LLaMA models</p>","constructors":[{"html_id":"new(handle:Pointer(LibLlama::LlamaKvCache),ctx:Context)-class-method","name":"new","doc":"Creates a new KvCache instance from a raw pointer\n\nNote: This constructor is intended for internal use.\nUsers should obtain KvCache instances through Context#kv_cache.\n\nTo avoid circular references, we store the context pointer rather than the context object\n\nRaises:\n- Llama::KvCache::Error if the handle is null","summary":"<p>Creates a new KvCache instance from a raw pointer</p>","abstract":false,"args":[{"name":"handle","external_name":"handle","restriction":"::Pointer(LibLlama::LlamaKvCache)"},{"name":"ctx","external_name":"ctx","restriction":"Context"}],"args_string":"(handle : Pointer(LibLlama::LlamaKvCache), ctx : Context)","args_html":"(handle : Pointer(LibLlama::LlamaKvCache), ctx : <a href=\"../Llama/Context.html\">Context</a>)","location":{"filename":"src/llama/kv_cache.cr","line_number":16,"url":"https://github.com/kojix2/llama.cr/blob/038f4ed10509841d5e14fe7055fbc1a21dac6a15/src/llama/kv_cache.cr#L16"},"def":{"name":"new","args":[{"name":"handle","external_name":"handle","restriction":"::Pointer(LibLlama::LlamaKvCache)"},{"name":"ctx","external_name":"ctx","restriction":"Context"}],"visibility":"Public","body":"_ = allocate\n_.initialize(handle, ctx)\nif _.responds_to?(:finalize)\n  ::GC.add_finalizer(_)\nend\n_\n"},"external_var":false}],"instance_methods":[{"html_id":"can_shift?:Bool-instance-method","name":"can_shift?","doc":"Checks if the context supports KV cache shifting\n\nReturns:\n- true if the context supports KV cache shifting, false otherwise\n\nRaises:\n- Llama::KvCache::Error if the operation fails","summary":"<p>Checks if the context supports KV cache shifting</p>","abstract":false,"location":{"filename":"src/llama/kv_cache.cr","line_number":296,"url":"https://github.com/kojix2/llama.cr/blob/038f4ed10509841d5e14fe7055fbc1a21dac6a15/src/llama/kv_cache.cr#L296"},"def":{"name":"can_shift?","return_type":"Bool","visibility":"Public","body":"begin\n  LibLlama.llama_kv_self_can_shift(ctx_ptr)\nrescue ex\n  error_msg = Llama.format_error(\"Failed to check if context supports KV cache shifting\", -7, ex.message)\n  raise(KvCache::Error.new(error_msg))\nend"},"external_var":false},{"html_id":"clear-instance-method","name":"clear","doc":"Clears the KV cache\nThis removes all tokens from the cache and resets its state\n\nRaises:\n- Llama::KvCache::Error if the operation fails","summary":"<p>Clears the KV cache This removes all tokens from the cache and resets its state</p>","abstract":false,"location":{"filename":"src/llama/kv_cache.cr","line_number":57,"url":"https://github.com/kojix2/llama.cr/blob/038f4ed10509841d5e14fe7055fbc1a21dac6a15/src/llama/kv_cache.cr#L57"},"def":{"name":"clear","visibility":"Public","body":"begin\n  LibLlama.llama_kv_self_clear(ctx_ptr)\nrescue ex\n  error_msg = Llama.format_error(\"Failed to clear KV cache\", -7, ex.message)\n  raise(KvCache::Error.new(error_msg))\nend"},"external_var":false},{"html_id":"defrag-instance-method","name":"defrag","doc":"Defragments the KV cache\nThis will be applied lazily on next decode or explicitly with update\n\nRaises:\n- Llama::KvCache::Error if the operation fails","summary":"<p>Defragments the KV cache This will be applied lazily on next decode or explicitly with update</p>","abstract":false,"location":{"filename":"src/llama/kv_cache.cr","line_number":276,"url":"https://github.com/kojix2/llama.cr/blob/038f4ed10509841d5e14fe7055fbc1a21dac6a15/src/llama/kv_cache.cr#L276"},"def":{"name":"defrag","visibility":"Public","body":"begin\n  LibLlama.llama_kv_self_defrag(ctx_ptr)\nrescue ex\n  error_msg = Llama.format_error(\"Failed to defragment KV cache\", -7, ex.message)\n  raise(KvCache::Error.new(error_msg))\nend"},"external_var":false},{"html_id":"finalize-instance-method","name":"finalize","doc":"Frees the resources associated with this KV cache","summary":"<p>Frees the resources associated with this KV cache</p>","abstract":false,"location":{"filename":"src/llama/kv_cache.cr","line_number":333,"url":"https://github.com/kojix2/llama.cr/blob/038f4ed10509841d5e14fe7055fbc1a21dac6a15/src/llama/kv_cache.cr#L333"},"def":{"name":"finalize","visibility":"Public","body":"@ctx_ptr = Pointer(LibLlama::LlamaContext).null"},"external_var":false},{"html_id":"n_tokens:Int32-instance-method","name":"n_tokens","doc":"Returns the number of tokens in the KV cache\nIf a KV cell has multiple sequences assigned to it, it will be counted multiple times\n\nReturns:\n- The number of tokens in the KV cache\n\nRaises:\n- Llama::KvCache::Error if the operation fails","summary":"<p>Returns the number of tokens in the KV cache If a KV cell has multiple sequences assigned to it, it will be counted multiple times</p>","abstract":false,"location":{"filename":"src/llama/kv_cache.cr","line_number":78,"url":"https://github.com/kojix2/llama.cr/blob/038f4ed10509841d5e14fe7055fbc1a21dac6a15/src/llama/kv_cache.cr#L78"},"def":{"name":"n_tokens","return_type":"Int32","visibility":"Public","body":"begin\n  LibLlama.llama_kv_self_n_tokens(ctx_ptr)\nrescue ex\n  error_msg = Llama.format_error(\"Failed to get number of tokens in KV cache\", -7, ex.message)\n  raise(KvCache::Error.new(error_msg))\nend"},"external_var":false},{"html_id":"seq_add(seq_id:Int32,p0:Int32,p1:Int32,delta:Int32)-instance-method","name":"seq_add","doc":"Adds a relative position delta to tokens in a sequence\n\nParameters:\n- seq_id: The sequence ID to modify\n- p0: Start position (p0 < 0 means start from 0)\n- p1: End position (p1 < 0 means end at infinity)\n- delta: The position delta to add\n\nRaises:\n- Llama::KvCache::Error if the operation fails","summary":"<p>Adds a relative position delta to tokens in a sequence</p>","abstract":false,"args":[{"name":"seq_id","external_name":"seq_id","restriction":"Int32"},{"name":"p0","external_name":"p0","restriction":"Int32"},{"name":"p1","external_name":"p1","restriction":"Int32"},{"name":"delta","external_name":"delta","restriction":"Int32"}],"args_string":"(seq_id : Int32, p0 : Int32, p1 : Int32, delta : Int32)","args_html":"(seq_id : Int32, p0 : Int32, p1 : Int32, delta : Int32)","location":{"filename":"src/llama/kv_cache.cr","line_number":194,"url":"https://github.com/kojix2/llama.cr/blob/038f4ed10509841d5e14fe7055fbc1a21dac6a15/src/llama/kv_cache.cr#L194"},"def":{"name":"seq_add","args":[{"name":"seq_id","external_name":"seq_id","restriction":"Int32"},{"name":"p0","external_name":"p0","restriction":"Int32"},{"name":"p1","external_name":"p1","restriction":"Int32"},{"name":"delta","external_name":"delta","restriction":"Int32"}],"visibility":"Public","body":"begin\n  LibLlama.llama_kv_self_seq_add(ctx_ptr, seq_id, p0, p1, delta)\nrescue ex\n  error_msg = Llama.format_error(\"Failed to add position delta to sequence in KV cache\", -7, \"seq_id: #{seq_id}, p0: #{p0}, p1: #{p1}, delta: #{delta}, error: #{ex.message}\")\n  raise(KvCache::Error.new(error_msg))\nend"},"external_var":false},{"html_id":"seq_cp(seq_id_src:Int32,seq_id_dst:Int32,p0:Int32,p1:Int32)-instance-method","name":"seq_cp","doc":"Copies tokens from one sequence to another in the KV cache\n\nParameters:\n- seq_id_src: Source sequence ID\n- seq_id_dst: Destination sequence ID\n- p0: Start position (p0 < 0 means start from 0)\n- p1: End position (p1 < 0 means end at infinity)\n\nRaises:\n- Llama::KvCache::Error if the operation fails","summary":"<p>Copies tokens from one sequence to another in the KV cache</p>","abstract":false,"args":[{"name":"seq_id_src","external_name":"seq_id_src","restriction":"Int32"},{"name":"seq_id_dst","external_name":"seq_id_dst","restriction":"Int32"},{"name":"p0","external_name":"p0","restriction":"Int32"},{"name":"p1","external_name":"p1","restriction":"Int32"}],"args_string":"(seq_id_src : Int32, seq_id_dst : Int32, p0 : Int32, p1 : Int32)","args_html":"(seq_id_src : Int32, seq_id_dst : Int32, p0 : Int32, p1 : Int32)","location":{"filename":"src/llama/kv_cache.cr","line_number":151,"url":"https://github.com/kojix2/llama.cr/blob/038f4ed10509841d5e14fe7055fbc1a21dac6a15/src/llama/kv_cache.cr#L151"},"def":{"name":"seq_cp","args":[{"name":"seq_id_src","external_name":"seq_id_src","restriction":"Int32"},{"name":"seq_id_dst","external_name":"seq_id_dst","restriction":"Int32"},{"name":"p0","external_name":"p0","restriction":"Int32"},{"name":"p1","external_name":"p1","restriction":"Int32"}],"visibility":"Public","body":"begin\n  LibLlama.llama_kv_self_seq_cp(ctx_ptr, seq_id_src, seq_id_dst, p0, p1)\nrescue ex\n  error_msg = Llama.format_error(\"Failed to copy sequence in KV cache\", -7, \"seq_id_src: #{seq_id_src}, seq_id_dst: #{seq_id_dst}, p0: #{p0}, p1: #{p1}, error: #{ex.message}\")\n  raise(KvCache::Error.new(error_msg))\nend"},"external_var":false},{"html_id":"seq_div(seq_id:Int32,p0:Int32,p1:Int32,d:Int32)-instance-method","name":"seq_div","doc":"Divides the positions of tokens in a sequence by a factor\n\nParameters:\n- seq_id: The sequence ID to modify\n- p0: Start position (p0 < 0 means start from 0)\n- p1: End position (p1 < 0 means end at infinity)\n- d: The divisor (must be > 1)\n\nRaises:\n- ArgumentError if the divisor is not greater than 1\n- Llama::KvCache::Error if the operation fails","summary":"<p>Divides the positions of tokens in a sequence by a factor</p>","abstract":false,"args":[{"name":"seq_id","external_name":"seq_id","restriction":"Int32"},{"name":"p0","external_name":"p0","restriction":"Int32"},{"name":"p1","external_name":"p1","restriction":"Int32"},{"name":"d","external_name":"d","restriction":"Int32"}],"args_string":"(seq_id : Int32, p0 : Int32, p1 : Int32, d : Int32)","args_html":"(seq_id : Int32, p0 : Int32, p1 : Int32, d : Int32)","location":{"filename":"src/llama/kv_cache.cr","line_number":218,"url":"https://github.com/kojix2/llama.cr/blob/038f4ed10509841d5e14fe7055fbc1a21dac6a15/src/llama/kv_cache.cr#L218"},"def":{"name":"seq_div","args":[{"name":"seq_id","external_name":"seq_id","restriction":"Int32"},{"name":"p0","external_name":"p0","restriction":"Int32"},{"name":"p1","external_name":"p1","restriction":"Int32"},{"name":"d","external_name":"d","restriction":"Int32"}],"visibility":"Public","body":"if d <= 1\n  raise(ArgumentError.new(\"Divisor must be greater than 1\"))\nend\n\nbegin\n  LibLlama.llama_kv_self_seq_div(ctx_ptr, seq_id, p0, p1, d)\nrescue ex\n  error_msg = Llama.format_error(\"Failed to divide positions in sequence in KV cache\", -7, \"seq_id: #{seq_id}, p0: #{p0}, p1: #{p1}, d: #{d}, error: #{ex.message}\")\n  raise(KvCache::Error.new(error_msg))\nend\n"},"external_var":false},{"html_id":"seq_keep(seq_id:Int32)-instance-method","name":"seq_keep","doc":"Keeps only the specified sequence in the KV cache, removing all others\n\nParameters:\n- seq_id: The sequence ID to keep\n\nRaises:\n- Llama::KvCache::Error if the operation fails","summary":"<p>Keeps only the specified sequence in the KV cache, removing all others</p>","abstract":false,"args":[{"name":"seq_id","external_name":"seq_id","restriction":"Int32"}],"args_string":"(seq_id : Int32)","args_html":"(seq_id : Int32)","location":{"filename":"src/llama/kv_cache.cr","line_number":171,"url":"https://github.com/kojix2/llama.cr/blob/038f4ed10509841d5e14fe7055fbc1a21dac6a15/src/llama/kv_cache.cr#L171"},"def":{"name":"seq_keep","args":[{"name":"seq_id","external_name":"seq_id","restriction":"Int32"}],"visibility":"Public","body":"begin\n  LibLlama.llama_kv_self_seq_keep(ctx_ptr, seq_id)\nrescue ex\n  error_msg = Llama.format_error(\"Failed to keep sequence in KV cache\", -7, \"seq_id: #{seq_id}, error: #{ex.message}\")\n  raise(KvCache::Error.new(error_msg))\nend"},"external_var":false},{"html_id":"seq_pos_max(seq_id:Int32):Int32-instance-method","name":"seq_pos_max","doc":"Returns the maximum position in a sequence\n\nParameters:\n- seq_id: The sequence ID to query\n\nReturns:\n- The maximum position in the sequence\n\nRaises:\n- Llama::KvCache::Error if the operation fails","summary":"<p>Returns the maximum position in a sequence</p>","abstract":false,"args":[{"name":"seq_id","external_name":"seq_id","restriction":"Int32"}],"args_string":"(seq_id : Int32) : Int32","args_html":"(seq_id : Int32) : Int32","location":{"filename":"src/llama/kv_cache.cr","line_number":245,"url":"https://github.com/kojix2/llama.cr/blob/038f4ed10509841d5e14fe7055fbc1a21dac6a15/src/llama/kv_cache.cr#L245"},"def":{"name":"seq_pos_max","args":[{"name":"seq_id","external_name":"seq_id","restriction":"Int32"}],"return_type":"Int32","visibility":"Public","body":"begin\n  result = LibLlama.llama_kv_self_seq_pos_max(ctx_ptr, seq_id)\n\n  if result < 0\n    error_msg = Llama.format_error(\"Failed to get maximum position in sequence\", -7, \"seq_id: #{seq_id}, result: #{result}\")\n    raise(KvCache::Error.new(error_msg))\n  end\n\n  result\nrescue ex : KvCache::Error\n  raise(ex)\nrescue ex\n  error_msg = Llama.format_error(\"Failed to get maximum position in sequence\", -7, \"seq_id: #{seq_id}, error: #{ex.message}\")\n  raise(KvCache::Error.new(error_msg))\nend"},"external_var":false},{"html_id":"seq_rm(seq_id:Int32,p0:Int32,p1:Int32):Bool-instance-method","name":"seq_rm","doc":"Removes tokens from a sequence in the KV cache\n\nParameters:\n- seq_id: The sequence ID to remove tokens from (seq_id < 0 matches any sequence)\n- p0: Start position (p0 < 0 means start from 0)\n- p1: End position (p1 < 0 means end at infinity)\n\nReturns:\n- true if successful, false if a partial sequence cannot be removed\n  (removing a whole sequence never fails)\n\nRaises:\n- Llama::KvCache::Error if the operation fails","summary":"<p>Removes tokens from a sequence in the KV cache</p>","abstract":false,"args":[{"name":"seq_id","external_name":"seq_id","restriction":"Int32"},{"name":"p0","external_name":"p0","restriction":"Int32"},{"name":"p1","external_name":"p1","restriction":"Int32"}],"args_string":"(seq_id : Int32, p0 : Int32, p1 : Int32) : Bool","args_html":"(seq_id : Int32, p0 : Int32, p1 : Int32) : Bool","location":{"filename":"src/llama/kv_cache.cr","line_number":125,"url":"https://github.com/kojix2/llama.cr/blob/038f4ed10509841d5e14fe7055fbc1a21dac6a15/src/llama/kv_cache.cr#L125"},"def":{"name":"seq_rm","args":[{"name":"seq_id","external_name":"seq_id","restriction":"Int32"},{"name":"p0","external_name":"p0","restriction":"Int32"},{"name":"p1","external_name":"p1","restriction":"Int32"}],"return_type":"Bool","visibility":"Public","body":"begin\n  result = LibLlama.llama_kv_self_seq_rm(ctx_ptr, seq_id, p0, p1)\n\n\n  return result\nrescue ex\n  error_msg = Llama.format_error(\"Failed to remove sequence from KV cache\", -7, \"seq_id: #{seq_id}, p0: #{p0}, p1: #{p1}, error: #{ex.message}\")\n  raise(KvCache::Error.new(error_msg))\nend"},"external_var":false},{"html_id":"to_unsafe:Pointer(Llama::LibLlama::LlamaKvCache)-instance-method","name":"to_unsafe","doc":"Returns the raw pointer to the underlying llama_kv_cache structure","summary":"<p>Returns the raw pointer to the underlying llama_kv_cache structure</p>","abstract":false,"location":{"filename":"src/llama/kv_cache.cr","line_number":328,"url":"https://github.com/kojix2/llama.cr/blob/038f4ed10509841d5e14fe7055fbc1a21dac6a15/src/llama/kv_cache.cr#L328"},"def":{"name":"to_unsafe","visibility":"Public","body":"@handle"},"external_var":false},{"html_id":"update-instance-method","name":"update","doc":"Applies pending KV cache updates\nThis includes K-shifts, defragmentation, etc.\n\nRaises:\n- Llama::KvCache::Error if the operation fails","summary":"<p>Applies pending KV cache updates This includes K-shifts, defragmentation, etc.</p>","abstract":false,"location":{"filename":"src/llama/kv_cache.cr","line_number":314,"url":"https://github.com/kojix2/llama.cr/blob/038f4ed10509841d5e14fe7055fbc1a21dac6a15/src/llama/kv_cache.cr#L314"},"def":{"name":"update","visibility":"Public","body":"begin\n  LibLlama.llama_kv_self_update(ctx_ptr)\nrescue ex\n  error_msg = Llama.format_error(\"Failed to update KV cache\", -7, ex.message)\n  raise(KvCache::Error.new(error_msg))\nend"},"external_var":false},{"html_id":"used_cells:Int32-instance-method","name":"used_cells","doc":"Returns the number of used KV cells\nA cell is considered used if it has at least one sequence assigned to it\n\nReturns:\n- The number of used KV cells\n\nRaises:\n- Llama::KvCache::Error if the operation fails","summary":"<p>Returns the number of used KV cells A cell is considered used if it has at least one sequence assigned to it</p>","abstract":false,"location":{"filename":"src/llama/kv_cache.cr","line_number":99,"url":"https://github.com/kojix2/llama.cr/blob/038f4ed10509841d5e14fe7055fbc1a21dac6a15/src/llama/kv_cache.cr#L99"},"def":{"name":"used_cells","return_type":"Int32","visibility":"Public","body":"begin\n  LibLlama.llama_kv_self_used_cells(ctx_ptr)\nrescue ex\n  error_msg = Llama.format_error(\"Failed to get number of used cells in KV cache\", -7, ex.message)\n  raise(KvCache::Error.new(error_msg))\nend"},"external_var":false}],"types":[{"html_id":"llama/Llama/KvCache/Error","path":"Llama/KvCache/Error.html","kind":"class","full_name":"Llama::KvCache::Error","name":"Error","abstract":false,"superclass":{"html_id":"llama/Llama/Error","kind":"class","full_name":"Llama::Error","name":"Error"},"ancestors":[{"html_id":"llama/Llama/Error","kind":"class","full_name":"Llama::Error","name":"Error"},{"html_id":"llama/Exception","kind":"class","full_name":"Exception","name":"Exception"},{"html_id":"llama/Reference","kind":"class","full_name":"Reference","name":"Reference"},{"html_id":"llama/Object","kind":"class","full_name":"Object","name":"Object"}],"locations":[{"filename":"src/llama/kv_cache/error.cr","line_number":5,"url":"https://github.com/kojix2/llama.cr/blob/038f4ed10509841d5e14fe7055fbc1a21dac6a15/src/llama/kv_cache/error.cr#L5"}],"repository_name":"llama","program":false,"enum":false,"alias":false,"const":false,"namespace":{"html_id":"llama/Llama/KvCache","kind":"class","full_name":"Llama::KvCache","name":"KvCache"}}]},{"html_id":"llama/Llama/Model","path":"Llama/Model.html","kind":"class","full_name":"Llama::Model","name":"Model","abstract":false,"superclass":{"html_id":"llama/Reference","kind":"class","full_name":"Reference","name":"Reference"},"ancestors":[{"html_id":"llama/Reference","kind":"class","full_name":"Reference","name":"Reference"},{"html_id":"llama/Object","kind":"class","full_name":"Object","name":"Object"}],"locations":[{"filename":"src/llama/model.cr","line_number":5,"url":"https://github.com/kojix2/llama.cr/blob/038f4ed10509841d5e14fe7055fbc1a21dac6a15/src/llama/model.cr#L5"},{"filename":"src/llama/model/error.cr","line_number":4,"url":"https://github.com/kojix2/llama.cr/blob/038f4ed10509841d5e14fe7055fbc1a21dac6a15/src/llama/model/error.cr#L4"}],"repository_name":"llama","program":false,"enum":false,"alias":false,"const":false,"namespace":{"html_id":"llama/Llama","kind":"module","full_name":"Llama","name":"Llama"},"doc":"Wrapper for the llama_model structure","summary":"<p>Wrapper for the llama_model structure</p>","constructors":[{"html_id":"new(path:String,n_gpu_layers:Int32=0,use_mmap:Bool=true,use_mlock:Bool=false,vocab_only:Bool=false)-class-method","name":"new","doc":"Creates a new Model instance by loading a model from a file.\n\nParameters:\n- path: Path to the model file (.gguf format).\n- n_gpu_layers: Number of layers to store in VRAM (default: 0). If 0, all layers are loaded to the CPU.\n- use_mmap: Use mmap if possible (default: true). Reduces memory usage.\n- use_mlock: Force the system to keep the model in RAM (default: false). May improve performance but increases memory usage.\n- vocab_only: Only load the vocabulary, no weights (default: false). Useful for inspecting the vocabulary.\n\nRaises:\n- Llama::Model::Error if the model cannot be loaded.","summary":"<p>Creates a new Model instance by loading a model from a file.</p>","abstract":false,"args":[{"name":"path","external_name":"path","restriction":"String"},{"name":"n_gpu_layers","default_value":"0","external_name":"n_gpu_layers","restriction":"Int32"},{"name":"use_mmap","default_value":"true","external_name":"use_mmap","restriction":"Bool"},{"name":"use_mlock","default_value":"false","external_name":"use_mlock","restriction":"Bool"},{"name":"vocab_only","default_value":"false","external_name":"vocab_only","restriction":"Bool"}],"args_string":"(path : String, n_gpu_layers : Int32 = 0, use_mmap : Bool = true, use_mlock : Bool = false, vocab_only : Bool = false)","args_html":"(path : String, n_gpu_layers : Int32 = <span class=\"n\">0</span>, use_mmap : Bool = <span class=\"n\">true</span>, use_mlock : Bool = <span class=\"n\">false</span>, vocab_only : Bool = <span class=\"n\">false</span>)","location":{"filename":"src/llama/model.cr","line_number":17,"url":"https://github.com/kojix2/llama.cr/blob/038f4ed10509841d5e14fe7055fbc1a21dac6a15/src/llama/model.cr#L17"},"def":{"name":"new","args":[{"name":"path","external_name":"path","restriction":"String"},{"name":"n_gpu_layers","default_value":"0","external_name":"n_gpu_layers","restriction":"Int32"},{"name":"use_mmap","default_value":"true","external_name":"use_mmap","restriction":"Bool"},{"name":"use_mlock","default_value":"false","external_name":"use_mlock","restriction":"Bool"},{"name":"vocab_only","default_value":"false","external_name":"vocab_only","restriction":"Bool"}],"visibility":"Public","body":"_ = allocate\n_.initialize(path, n_gpu_layers, use_mmap, use_mlock, vocab_only)\nif _.responds_to?(:finalize)\n  ::GC.add_finalizer(_)\nend\n_\n"},"external_var":false}],"instance_methods":[{"html_id":"chat_template(name:String|Nil=nil):String|Nil-instance-method","name":"chat_template","doc":"Gets the default chat template for this model\n\nParameters:\n- name: Optional template name (nil for default)\n\nReturns:\n- The chat template string, or nil if not available","summary":"<p>Gets the default chat template for this model</p>","abstract":false,"args":[{"name":"name","default_value":"nil","external_name":"name","restriction":"String | ::Nil"}],"args_string":"(name : String | Nil = nil) : String | Nil","args_html":"(name : String | Nil = <span class=\"n\">nil</span>) : String | Nil","location":{"filename":"src/llama/model.cr","line_number":52,"url":"https://github.com/kojix2/llama.cr/blob/038f4ed10509841d5e14fe7055fbc1a21dac6a15/src/llama/model.cr#L52"},"def":{"name":"chat_template","args":[{"name":"name","default_value":"nil","external_name":"name","restriction":"String | ::Nil"}],"return_type":"String | ::Nil","visibility":"Public","body":"ptr = LibLlama.llama_model_chat_template(@handle, name.nil? ? nil : name.to_unsafe)\nif ptr.null?\n  return nil\nend\nString.new(ptr)\n"},"external_var":false},{"html_id":"cleanup-instance-method","name":"cleanup","doc":"Explicitly clean up resources\nThis can be called manually to release resources before garbage collection","summary":"<p>Explicitly clean up resources This can be called manually to release resources before garbage collection</p>","abstract":false,"location":{"filename":"src/llama/model.cr","line_number":143,"url":"https://github.com/kojix2/llama.cr/blob/038f4ed10509841d5e14fe7055fbc1a21dac6a15/src/llama/model.cr#L143"},"def":{"name":"cleanup","visibility":"Public","body":"if @handle && !@handle.null?\n  LibLlama.llama_model_free(@handle)\nend"},"external_var":false},{"html_id":"context(*args,**options):Context-instance-method","name":"context","doc":"Creates a new Context for this model\n\nThis method delegates to Context.new, passing self as the model parameter\nand forwarding all other arguments.\n\nReturns:\n- A new Context instance\n\nRaises:\n- Llama::Context::Error if the context cannot be created","summary":"<p>Creates a new Context for this model</p>","abstract":false,"args":[{"name":"args","external_name":"args","restriction":""}],"args_string":"(*args, **options) : Context","args_html":"(*args, **options) : <a href=\"../Llama/Context.html\">Context</a>","location":{"filename":"src/llama/model.cr","line_number":132,"url":"https://github.com/kojix2/llama.cr/blob/038f4ed10509841d5e14fe7055fbc1a21dac6a15/src/llama/model.cr#L132"},"def":{"name":"context","args":[{"name":"args","external_name":"args","restriction":""}],"double_splat":{"name":"options","external_name":"options","restriction":""},"splat_index":0,"return_type":"Context","visibility":"Public","body":"Context.new(self, *args, **options)"},"external_var":false},{"html_id":"decoder_start_token:Int32-instance-method","name":"decoder_start_token","doc":"Returns the token that must be provided to the decoder to start generating output\nFor encoder-decoder models, returns the decoder start token\nFor other models, returns -1","summary":"<p>Returns the token that must be provided to the decoder to start generating output For encoder-decoder models, returns the decoder start token For other models, returns -1</p>","abstract":false,"location":{"filename":"src/llama/model.cr","line_number":118,"url":"https://github.com/kojix2/llama.cr/blob/038f4ed10509841d5e14fe7055fbc1a21dac6a15/src/llama/model.cr#L118"},"def":{"name":"decoder_start_token","return_type":"Int32","visibility":"Public","body":"LibLlama.llama_model_decoder_start_token(@handle)"},"external_var":false},{"html_id":"description:String-instance-method","name":"description","doc":"Gets a string describing the model type\n\nReturns:\n- A description of the model","summary":"<p>Gets a string describing the model type</p>","abstract":false,"location":{"filename":"src/llama/model.cr","line_number":228,"url":"https://github.com/kojix2/llama.cr/blob/038f4ed10509841d5e14fe7055fbc1a21dac6a15/src/llama/model.cr#L228"},"def":{"name":"description","return_type":"String","visibility":"Public","body":"buf_size = 1024\nbuf = Pointer(LibC::Char).malloc(buf_size)\nresult = LibLlama.llama_model_desc(@handle, buf, buf_size)\n\nif result < 0\n  \"Unknown model\"\nelse\n  String.new(buf, result)\nend\n"},"external_var":false},{"html_id":"finalize-instance-method","name":"finalize","doc":"Frees the resources associated with this model","summary":"<p>Frees the resources associated with this model</p>","abstract":false,"location":{"filename":"src/llama/model.cr","line_number":150,"url":"https://github.com/kojix2/llama.cr/blob/038f4ed10509841d5e14fe7055fbc1a21dac6a15/src/llama/model.cr#L150"},"def":{"name":"finalize","visibility":"Public","body":"cleanup"},"external_var":false},{"html_id":"has_decoder?:Bool-instance-method","name":"has_decoder?","doc":"Returns whether the model contains a decoder","summary":"<p>Returns whether the model contains a decoder</p>","abstract":false,"location":{"filename":"src/llama/model.cr","line_number":101,"url":"https://github.com/kojix2/llama.cr/blob/038f4ed10509841d5e14fe7055fbc1a21dac6a15/src/llama/model.cr#L101"},"def":{"name":"has_decoder?","return_type":"Bool","visibility":"Public","body":"LibLlama.llama_model_has_decoder(@handle)"},"external_var":false},{"html_id":"has_encoder?:Bool-instance-method","name":"has_encoder?","doc":"Returns whether the model contains an encoder","summary":"<p>Returns whether the model contains an encoder</p>","abstract":false,"location":{"filename":"src/llama/model.cr","line_number":96,"url":"https://github.com/kojix2/llama.cr/blob/038f4ed10509841d5e14fe7055fbc1a21dac6a15/src/llama/model.cr#L96"},"def":{"name":"has_encoder?","return_type":"Bool","visibility":"Public","body":"LibLlama.llama_model_has_encoder(@handle)"},"external_var":false},{"html_id":"metadata:Hash(String,String)-instance-method","name":"metadata","doc":"Gets all metadata as a hash\n\nReturns:\n- A hash mapping metadata keys to values","summary":"<p>Gets all metadata as a hash</p>","abstract":false,"location":{"filename":"src/llama/model.cr","line_number":253,"url":"https://github.com/kojix2/llama.cr/blob/038f4ed10509841d5e14fe7055fbc1a21dac6a15/src/llama/model.cr#L253"},"def":{"name":"metadata","return_type":"Hash(String, String)","visibility":"Public","body":"result = {} of String => String\ncount = metadata_count\n\ncount.times do |i|\n  key = metadata_key_at(i)\n  val = metadata_value_at(i)\n\n  if key && val\n    result[key] = val\n  end\nend\n\nresult\n"},"external_var":false},{"html_id":"metadata_count:Int32-instance-method","name":"metadata_count","doc":"Gets the number of metadata key/value pairs\n\nReturns:\n- The number of metadata entries","summary":"<p>Gets the number of metadata key/value pairs</p>","abstract":false,"location":{"filename":"src/llama/model.cr","line_number":180,"url":"https://github.com/kojix2/llama.cr/blob/038f4ed10509841d5e14fe7055fbc1a21dac6a15/src/llama/model.cr#L180"},"def":{"name":"metadata_count","return_type":"Int32","visibility":"Public","body":"LibLlama.llama_model_meta_count(@handle)"},"external_var":false},{"html_id":"metadata_key_at(i:Int32):String|Nil-instance-method","name":"metadata_key_at","doc":"Gets a metadata key name by index\n\nParameters:\n- i: The index of the metadata entry\n\nReturns:\n- The key name, or nil if the index is out of bounds","summary":"<p>Gets a metadata key name by index</p>","abstract":false,"args":[{"name":"i","external_name":"i","restriction":"Int32"}],"args_string":"(i : Int32) : String | Nil","args_html":"(i : Int32) : String | Nil","location":{"filename":"src/llama/model.cr","line_number":191,"url":"https://github.com/kojix2/llama.cr/blob/038f4ed10509841d5e14fe7055fbc1a21dac6a15/src/llama/model.cr#L191"},"def":{"name":"metadata_key_at","args":[{"name":"i","external_name":"i","restriction":"Int32"}],"return_type":"String | ::Nil","visibility":"Public","body":"buf_size = 1024\nbuf = Pointer(LibC::Char).malloc(buf_size)\nresult = LibLlama.llama_model_meta_key_by_index(@handle, i, buf, buf_size)\n\nif result < 0\n  nil\nelse\n  String.new(buf, result)\nend\n"},"external_var":false},{"html_id":"metadata_value(key:String):String|Nil-instance-method","name":"metadata_value","doc":"Gets a metadata value as a string by key name\n\nParameters:\n- key: The metadata key to look up\n\nReturns:\n- The metadata value as a string, or nil if not found","summary":"<p>Gets a metadata value as a string by key name</p>","abstract":false,"args":[{"name":"key","external_name":"key","restriction":"String"}],"args_string":"(key : String) : String | Nil","args_html":"(key : String) : String | Nil","location":{"filename":"src/llama/model.cr","line_number":163,"url":"https://github.com/kojix2/llama.cr/blob/038f4ed10509841d5e14fe7055fbc1a21dac6a15/src/llama/model.cr#L163"},"def":{"name":"metadata_value","args":[{"name":"key","external_name":"key","restriction":"String"}],"return_type":"String | ::Nil","visibility":"Public","body":"buf_size = 1024\nbuf = Pointer(LibC::Char).malloc(buf_size)\nresult = LibLlama.llama_model_meta_val_str(@handle, key, buf, buf_size)\n\nif result < 0\n  nil\nelse\n  String.new(buf, result)\nend\n"},"external_var":false},{"html_id":"metadata_value_at(i:Int32):String|Nil-instance-method","name":"metadata_value_at","doc":"Gets a metadata value as a string by index\n\nParameters:\n- i: The index of the metadata entry\n\nReturns:\n- The value as a string, or nil if the index is out of bounds","summary":"<p>Gets a metadata value as a string by index</p>","abstract":false,"args":[{"name":"i","external_name":"i","restriction":"Int32"}],"args_string":"(i : Int32) : String | Nil","args_html":"(i : Int32) : String | Nil","location":{"filename":"src/llama/model.cr","line_number":211,"url":"https://github.com/kojix2/llama.cr/blob/038f4ed10509841d5e14fe7055fbc1a21dac6a15/src/llama/model.cr#L211"},"def":{"name":"metadata_value_at","args":[{"name":"i","external_name":"i","restriction":"Int32"}],"return_type":"String | ::Nil","visibility":"Public","body":"buf_size = 1024\nbuf = Pointer(LibC::Char).malloc(buf_size)\nresult = LibLlama.llama_model_meta_val_str_by_index(@handle, i, buf, buf_size)\n\nif result < 0\n  nil\nelse\n  String.new(buf, result)\nend\n"},"external_var":false},{"html_id":"model_size:UInt64-instance-method","name":"model_size","doc":"Returns the total size of all the tensors in the model in bytes\n\nReturns:\n- The total size of all tensors in the model (in bytes)","summary":"<p>Returns the total size of all the tensors in the model in bytes</p>","abstract":false,"location":{"filename":"src/llama/model.cr","line_number":245,"url":"https://github.com/kojix2/llama.cr/blob/038f4ed10509841d5e14fe7055fbc1a21dac6a15/src/llama/model.cr#L245"},"def":{"name":"model_size","return_type":"UInt64","visibility":"Public","body":"LibLlama.llama_model_size(@handle)"},"external_var":false},{"html_id":"n_embd:Int32-instance-method","name":"n_embd","doc":"Returns the number of embedding dimensions in the model","summary":"<p>Returns the number of embedding dimensions in the model</p>","abstract":false,"location":{"filename":"src/llama/model.cr","line_number":81,"url":"https://github.com/kojix2/llama.cr/blob/038f4ed10509841d5e14fe7055fbc1a21dac6a15/src/llama/model.cr#L81"},"def":{"name":"n_embd","return_type":"Int32","visibility":"Public","body":"LibLlama.llama_model_n_embd(@handle)"},"external_var":false},{"html_id":"n_head:Int32-instance-method","name":"n_head","doc":"Returns the number of attention heads in the model","summary":"<p>Returns the number of attention heads in the model</p>","abstract":false,"location":{"filename":"src/llama/model.cr","line_number":91,"url":"https://github.com/kojix2/llama.cr/blob/038f4ed10509841d5e14fe7055fbc1a21dac6a15/src/llama/model.cr#L91"},"def":{"name":"n_head","return_type":"Int32","visibility":"Public","body":"LibLlama.llama_model_n_head(@handle)"},"external_var":false},{"html_id":"n_layer:Int32-instance-method","name":"n_layer","doc":"Returns the number of layers in the model","summary":"<p>Returns the number of layers in the model</p>","abstract":false,"location":{"filename":"src/llama/model.cr","line_number":86,"url":"https://github.com/kojix2/llama.cr/blob/038f4ed10509841d5e14fe7055fbc1a21dac6a15/src/llama/model.cr#L86"},"def":{"name":"n_layer","return_type":"Int32","visibility":"Public","body":"LibLlama.llama_model_n_layer(@handle)"},"external_var":false},{"html_id":"n_params:UInt64-instance-method","name":"n_params","doc":"Returns the number of parameters in the model","summary":"<p>Returns the number of parameters in the model</p>","abstract":false,"location":{"filename":"src/llama/model.cr","line_number":76,"url":"https://github.com/kojix2/llama.cr/blob/038f4ed10509841d5e14fe7055fbc1a21dac6a15/src/llama/model.cr#L76"},"def":{"name":"n_params","return_type":"UInt64","visibility":"Public","body":"LibLlama.llama_model_n_params(@handle)"},"external_var":false},{"html_id":"recurrent?:Bool-instance-method","name":"recurrent?","doc":"Returns whether the model is recurrent (like Mamba, RWKV, etc.)","summary":"<p>Returns whether the model is recurrent (like Mamba, RWKV, etc.)</p>","abstract":false,"location":{"filename":"src/llama/model.cr","line_number":106,"url":"https://github.com/kojix2/llama.cr/blob/038f4ed10509841d5e14fe7055fbc1a21dac6a15/src/llama/model.cr#L106"},"def":{"name":"recurrent?","return_type":"Bool","visibility":"Public","body":"LibLlama.llama_model_is_recurrent(@handle)"},"external_var":false},{"html_id":"rope_freq_scale_train:Float32-instance-method","name":"rope_freq_scale_train","doc":"Returns the model's RoPE frequency scaling factor","summary":"<p>Returns the model's RoPE frequency scaling factor</p>","abstract":false,"location":{"filename":"src/llama/model.cr","line_number":111,"url":"https://github.com/kojix2/llama.cr/blob/038f4ed10509841d5e14fe7055fbc1a21dac6a15/src/llama/model.cr#L111"},"def":{"name":"rope_freq_scale_train","return_type":"Float32","visibility":"Public","body":"LibLlama.llama_model_rope_freq_scale_train(@handle)"},"external_var":false},{"html_id":"to_unsafe:Pointer(Llama::LibLlama::LlamaModel)-instance-method","name":"to_unsafe","doc":"Returns the raw pointer to the underlying llama_model structure","summary":"<p>Returns the raw pointer to the underlying llama_model structure</p>","abstract":false,"location":{"filename":"src/llama/model.cr","line_number":137,"url":"https://github.com/kojix2/llama.cr/blob/038f4ed10509841d5e14fe7055fbc1a21dac6a15/src/llama/model.cr#L137"},"def":{"name":"to_unsafe","visibility":"Public","body":"@handle"},"external_var":false},{"html_id":"vocab:Vocab-instance-method","name":"vocab","doc":"Returns the vocabulary associated with this model","summary":"<p>Returns the vocabulary associated with this model</p>","abstract":false,"location":{"filename":"src/llama/model.cr","line_number":62,"url":"https://github.com/kojix2/llama.cr/blob/038f4ed10509841d5e14fe7055fbc1a21dac6a15/src/llama/model.cr#L62"},"def":{"name":"vocab","return_type":"Vocab","visibility":"Public","body":"vocab_ptr = LibLlama.llama_model_get_vocab(@handle)\nif vocab_ptr.null?\n  error_msg = Llama.format_error(\"Failed to get vocabulary\", -5, \"model may be corrupted\")\n  raise(Model::Error.new(error_msg))\nend\nVocab.new(vocab_ptr)\n"},"external_var":false}],"types":[{"html_id":"llama/Llama/Model/Error","path":"Llama/Model/Error.html","kind":"class","full_name":"Llama::Model::Error","name":"Error","abstract":false,"superclass":{"html_id":"llama/Llama/Error","kind":"class","full_name":"Llama::Error","name":"Error"},"ancestors":[{"html_id":"llama/Llama/Error","kind":"class","full_name":"Llama::Error","name":"Error"},{"html_id":"llama/Exception","kind":"class","full_name":"Exception","name":"Exception"},{"html_id":"llama/Reference","kind":"class","full_name":"Reference","name":"Reference"},{"html_id":"llama/Object","kind":"class","full_name":"Object","name":"Object"}],"locations":[{"filename":"src/llama/model/error.cr","line_number":5,"url":"https://github.com/kojix2/llama.cr/blob/038f4ed10509841d5e14fe7055fbc1a21dac6a15/src/llama/model/error.cr#L5"}],"repository_name":"llama","program":false,"enum":false,"alias":false,"const":false,"namespace":{"html_id":"llama/Llama/Model","kind":"class","full_name":"Llama::Model","name":"Model"}}]},{"html_id":"llama/Llama/Sampler","path":"Llama/Sampler.html","kind":"module","full_name":"Llama::Sampler","name":"Sampler","abstract":false,"locations":[{"filename":"src/llama/sampler.cr","line_number":4,"url":"https://github.com/kojix2/llama.cr/blob/038f4ed10509841d5e14fe7055fbc1a21dac6a15/src/llama/sampler.cr#L4"},{"filename":"src/llama/sampler/base.cr","line_number":2,"url":"https://github.com/kojix2/llama.cr/blob/038f4ed10509841d5e14fe7055fbc1a21dac6a15/src/llama/sampler/base.cr#L2"},{"filename":"src/llama/sampler/chain.cr","line_number":2,"url":"https://github.com/kojix2/llama.cr/blob/038f4ed10509841d5e14fe7055fbc1a21dac6a15/src/llama/sampler/chain.cr#L2"},{"filename":"src/llama/sampler/dist.cr","line_number":2,"url":"https://github.com/kojix2/llama.cr/blob/038f4ed10509841d5e14fe7055fbc1a21dac6a15/src/llama/sampler/dist.cr#L2"},{"filename":"src/llama/sampler/error.cr","line_number":4,"url":"https://github.com/kojix2/llama.cr/blob/038f4ed10509841d5e14fe7055fbc1a21dac6a15/src/llama/sampler/error.cr#L4"},{"filename":"src/llama/sampler/grammar.cr","line_number":2,"url":"https://github.com/kojix2/llama.cr/blob/038f4ed10509841d5e14fe7055fbc1a21dac6a15/src/llama/sampler/grammar.cr#L2"},{"filename":"src/llama/sampler/grammar_lazy_patterns.cr","line_number":2,"url":"https://github.com/kojix2/llama.cr/blob/038f4ed10509841d5e14fe7055fbc1a21dac6a15/src/llama/sampler/grammar_lazy_patterns.cr#L2"},{"filename":"src/llama/sampler/greedy.cr","line_number":2,"url":"https://github.com/kojix2/llama.cr/blob/038f4ed10509841d5e14fe7055fbc1a21dac6a15/src/llama/sampler/greedy.cr#L2"},{"filename":"src/llama/sampler/infill.cr","line_number":2,"url":"https://github.com/kojix2/llama.cr/blob/038f4ed10509841d5e14fe7055fbc1a21dac6a15/src/llama/sampler/infill.cr#L2"},{"filename":"src/llama/sampler/min_p.cr","line_number":2,"url":"https://github.com/kojix2/llama.cr/blob/038f4ed10509841d5e14fe7055fbc1a21dac6a15/src/llama/sampler/min_p.cr#L2"},{"filename":"src/llama/sampler/mirostat.cr","line_number":2,"url":"https://github.com/kojix2/llama.cr/blob/038f4ed10509841d5e14fe7055fbc1a21dac6a15/src/llama/sampler/mirostat.cr#L2"},{"filename":"src/llama/sampler/mirostat_v2.cr","line_number":2,"url":"https://github.com/kojix2/llama.cr/blob/038f4ed10509841d5e14fe7055fbc1a21dac6a15/src/llama/sampler/mirostat_v2.cr#L2"},{"filename":"src/llama/sampler/penalties.cr","line_number":2,"url":"https://github.com/kojix2/llama.cr/blob/038f4ed10509841d5e14fe7055fbc1a21dac6a15/src/llama/sampler/penalties.cr#L2"},{"filename":"src/llama/sampler/temp.cr","line_number":2,"url":"https://github.com/kojix2/llama.cr/blob/038f4ed10509841d5e14fe7055fbc1a21dac6a15/src/llama/sampler/temp.cr#L2"},{"filename":"src/llama/sampler/temp_ext.cr","line_number":2,"url":"https://github.com/kojix2/llama.cr/blob/038f4ed10509841d5e14fe7055fbc1a21dac6a15/src/llama/sampler/temp_ext.cr#L2"},{"filename":"src/llama/sampler/top_k.cr","line_number":2,"url":"https://github.com/kojix2/llama.cr/blob/038f4ed10509841d5e14fe7055fbc1a21dac6a15/src/llama/sampler/top_k.cr#L2"},{"filename":"src/llama/sampler/top_n_sigma.cr","line_number":2,"url":"https://github.com/kojix2/llama.cr/blob/038f4ed10509841d5e14fe7055fbc1a21dac6a15/src/llama/sampler/top_n_sigma.cr#L2"},{"filename":"src/llama/sampler/top_p.cr","line_number":2,"url":"https://github.com/kojix2/llama.cr/blob/038f4ed10509841d5e14fe7055fbc1a21dac6a15/src/llama/sampler/top_p.cr#L2"},{"filename":"src/llama/sampler/typical.cr","line_number":2,"url":"https://github.com/kojix2/llama.cr/blob/038f4ed10509841d5e14fe7055fbc1a21dac6a15/src/llama/sampler/typical.cr#L2"},{"filename":"src/llama/sampler/xtc.cr","line_number":2,"url":"https://github.com/kojix2/llama.cr/blob/038f4ed10509841d5e14fe7055fbc1a21dac6a15/src/llama/sampler/xtc.cr#L2"}],"repository_name":"llama","program":false,"enum":false,"alias":false,"const":false,"namespace":{"html_id":"llama/Llama","kind":"module","full_name":"Llama","name":"Llama"},"types":[{"html_id":"llama/Llama/Sampler/Base","path":"Llama/Sampler/Base.html","kind":"class","full_name":"Llama::Sampler::Base","name":"Base","abstract":true,"superclass":{"html_id":"llama/Reference","kind":"class","full_name":"Reference","name":"Reference"},"ancestors":[{"html_id":"llama/Reference","kind":"class","full_name":"Reference","name":"Reference"},{"html_id":"llama/Object","kind":"class","full_name":"Object","name":"Object"}],"locations":[{"filename":"src/llama/sampler/base.cr","line_number":6,"url":"https://github.com/kojix2/llama.cr/blob/038f4ed10509841d5e14fe7055fbc1a21dac6a15/src/llama/sampler/base.cr#L6"}],"repository_name":"llama","program":false,"enum":false,"alias":false,"const":false,"subclasses":[{"html_id":"llama/Llama/Sampler/Chain","kind":"class","full_name":"Llama::Sampler::Chain","name":"Chain"},{"html_id":"llama/Llama/Sampler/Dist","kind":"class","full_name":"Llama::Sampler::Dist","name":"Dist"},{"html_id":"llama/Llama/Sampler/Grammar","kind":"class","full_name":"Llama::Sampler::Grammar","name":"Grammar"},{"html_id":"llama/Llama/Sampler/GrammarLazyPatterns","kind":"class","full_name":"Llama::Sampler::GrammarLazyPatterns","name":"GrammarLazyPatterns"},{"html_id":"llama/Llama/Sampler/Greedy","kind":"class","full_name":"Llama::Sampler::Greedy","name":"Greedy"},{"html_id":"llama/Llama/Sampler/Infill","kind":"class","full_name":"Llama::Sampler::Infill","name":"Infill"},{"html_id":"llama/Llama/Sampler/MinP","kind":"class","full_name":"Llama::Sampler::MinP","name":"MinP"},{"html_id":"llama/Llama/Sampler/Mirostat","kind":"class","full_name":"Llama::Sampler::Mirostat","name":"Mirostat"},{"html_id":"llama/Llama/Sampler/MirostatV2","kind":"class","full_name":"Llama::Sampler::MirostatV2","name":"MirostatV2"},{"html_id":"llama/Llama/Sampler/Penalties","kind":"class","full_name":"Llama::Sampler::Penalties","name":"Penalties"},{"html_id":"llama/Llama/Sampler/Temp","kind":"class","full_name":"Llama::Sampler::Temp","name":"Temp"},{"html_id":"llama/Llama/Sampler/TempExt","kind":"class","full_name":"Llama::Sampler::TempExt","name":"TempExt"},{"html_id":"llama/Llama/Sampler/TopK","kind":"class","full_name":"Llama::Sampler::TopK","name":"TopK"},{"html_id":"llama/Llama/Sampler/TopNSigma","kind":"class","full_name":"Llama::Sampler::TopNSigma","name":"TopNSigma"},{"html_id":"llama/Llama/Sampler/TopP","kind":"class","full_name":"Llama::Sampler::TopP","name":"TopP"},{"html_id":"llama/Llama/Sampler/Typical","kind":"class","full_name":"Llama::Sampler::Typical","name":"Typical"},{"html_id":"llama/Llama/Sampler/Xtc","kind":"class","full_name":"Llama::Sampler::Xtc","name":"Xtc"}],"namespace":{"html_id":"llama/Llama/Sampler","kind":"module","full_name":"Llama::Sampler","name":"Sampler"},"doc":"Base class for all sampling methods.\nSampling is the process of selecting the next token during text generation.\nThis is an abstract class.","summary":"<p>Base class for all sampling methods.</p>","constructors":[{"html_id":"new(handle:Pointer(LibLlama::LlamaSampler))-class-method","name":"new","doc":"Creates a new Sampler instance from a raw pointer.\n\nNote: This constructor is intended for internal use.","summary":"<p>Creates a new Sampler instance from a raw pointer.</p>","abstract":false,"args":[{"name":"handle","external_name":"handle","restriction":"::Pointer(LibLlama::LlamaSampler)"}],"args_string":"(handle : Pointer(LibLlama::LlamaSampler))","args_html":"(handle : Pointer(LibLlama::LlamaSampler))","location":{"filename":"src/llama/sampler/base.cr","line_number":10,"url":"https://github.com/kojix2/llama.cr/blob/038f4ed10509841d5e14fe7055fbc1a21dac6a15/src/llama/sampler/base.cr#L10"},"def":{"name":"new","args":[{"name":"handle","external_name":"handle","restriction":"::Pointer(LibLlama::LlamaSampler)"}],"visibility":"Public","body":"_ = allocate\n_.initialize(handle)\nif _.responds_to?(:finalize)\n  ::GC.add_finalizer(_)\nend\n_\n"},"external_var":false}],"instance_methods":[{"html_id":"finalize-instance-method","name":"finalize","doc":"Frees the resources associated with this sampler.","summary":"<p>Frees the resources associated with this sampler.</p>","abstract":false,"location":{"filename":"src/llama/sampler/base.cr","line_number":19,"url":"https://github.com/kojix2/llama.cr/blob/038f4ed10509841d5e14fe7055fbc1a21dac6a15/src/llama/sampler/base.cr#L19"},"def":{"name":"finalize","visibility":"Public","body":"if @handle && !@handle.null?\n  LibLlama.llama_sampler_free(@handle)\nend"},"external_var":false},{"html_id":"to_unsafe:Pointer(Llama::LibLlama::LlamaSampler)-instance-method","name":"to_unsafe","doc":"Returns the raw pointer to the underlying llama_sampler structure.","summary":"<p>Returns the raw pointer to the underlying llama_sampler structure.</p>","abstract":false,"location":{"filename":"src/llama/sampler/base.cr","line_number":14,"url":"https://github.com/kojix2/llama.cr/blob/038f4ed10509841d5e14fe7055fbc1a21dac6a15/src/llama/sampler/base.cr#L14"},"def":{"name":"to_unsafe","visibility":"Public","body":"@handle"},"external_var":false}]},{"html_id":"llama/Llama/Sampler/Chain","path":"Llama/Sampler/Chain.html","kind":"class","full_name":"Llama::Sampler::Chain","name":"Chain","abstract":false,"superclass":{"html_id":"llama/Llama/Sampler/Base","kind":"class","full_name":"Llama::Sampler::Base","name":"Base"},"ancestors":[{"html_id":"llama/Llama/Sampler/Base","kind":"class","full_name":"Llama::Sampler::Base","name":"Base"},{"html_id":"llama/Reference","kind":"class","full_name":"Reference","name":"Reference"},{"html_id":"llama/Object","kind":"class","full_name":"Object","name":"Object"}],"locations":[{"filename":"src/llama/sampler/chain.cr","line_number":20,"url":"https://github.com/kojix2/llama.cr/blob/038f4ed10509841d5e14fe7055fbc1a21dac6a15/src/llama/sampler/chain.cr#L20"}],"repository_name":"llama","program":false,"enum":false,"alias":false,"const":false,"namespace":{"html_id":"llama/Llama/Sampler","kind":"module","full_name":"Llama::Sampler","name":"Sampler"},"doc":"Wrapper for a chain of samplers\n\nThe SamplerChain class allows you to combine multiple sampling methods\nin a sequence. Each sampler in the chain filters the token probabilities\nbefore passing them to the next sampler.\n\nExample:\n```\nchain = Llama::Sampler::Chain.new\nchain.add(Llama::Sampler::TopK.new(40))      # First filter with top-k\nchain.add(Llama::Sampler::TopP.new(0.95, 1)) # Then apply top-p\nchain.add(Llama::Sampler::Temp.new(0.8))     # Apply temperature\nchain.add(Llama::Sampler::Dist.new(42))      # Final distribution sampling\n\n# Generate text with the custom sampler chain\nresult = context.generate_with_sampler(\"Your prompt\", chain, 150)\n```","summary":"<p>Wrapper for a chain of samplers</p>","constructors":[{"html_id":"new(no_perf:Bool=false)-class-method","name":"new","doc":"Creates a new SamplerChain with optional parameters\n\nParameters:\n- no_perf: Whether to disable performance counters (default: false)\n\nRaises:\n- Llama::Error if the sampler chain cannot be created","summary":"<p>Creates a new SamplerChain with optional parameters</p>","abstract":false,"args":[{"name":"no_perf","default_value":"false","external_name":"no_perf","restriction":"Bool"}],"args_string":"(no_perf : Bool = false)","args_html":"(no_perf : Bool = <span class=\"n\">false</span>)","location":{"filename":"src/llama/sampler/chain.cr","line_number":28,"url":"https://github.com/kojix2/llama.cr/blob/038f4ed10509841d5e14fe7055fbc1a21dac6a15/src/llama/sampler/chain.cr#L28"},"def":{"name":"new","args":[{"name":"no_perf","default_value":"false","external_name":"no_perf","restriction":"Bool"}],"visibility":"Public","body":"_ = allocate\n_.initialize(no_perf)\nif _.responds_to?(:finalize)\n  ::GC.add_finalizer(_)\nend\n_\n"},"external_var":false}],"instance_methods":[{"html_id":"accept(token:Int32)-instance-method","name":"accept","doc":"Accepts a token, updating the internal state of the samplers\n\nParameters:\n- token: The token to accept","summary":"<p>Accepts a token, updating the internal state of the samplers</p>","abstract":false,"args":[{"name":"token","external_name":"token","restriction":"Int32"}],"args_string":"(token : Int32)","args_html":"(token : Int32)","location":{"filename":"src/llama/sampler/chain.cr","line_number":61,"url":"https://github.com/kojix2/llama.cr/blob/038f4ed10509841d5e14fe7055fbc1a21dac6a15/src/llama/sampler/chain.cr#L61"},"def":{"name":"accept","args":[{"name":"token","external_name":"token","restriction":"Int32"}],"visibility":"Public","body":"LibLlama.llama_sampler_accept(@handle, token)"},"external_var":false},{"html_id":"add(sampler:Base)-instance-method","name":"add","doc":"Adds a sampler to the chain\n\nParameters:\n- sampler: The sampler to add to the chain","summary":"<p>Adds a sampler to the chain</p>","abstract":false,"args":[{"name":"sampler","external_name":"sampler","restriction":"Base"}],"args_string":"(sampler : Base)","args_html":"(sampler : <a href=\"../../Llama/Sampler/Base.html\">Base</a>)","location":{"filename":"src/llama/sampler/chain.cr","line_number":40,"url":"https://github.com/kojix2/llama.cr/blob/038f4ed10509841d5e14fe7055fbc1a21dac6a15/src/llama/sampler/chain.cr#L40"},"def":{"name":"add","args":[{"name":"sampler","external_name":"sampler","restriction":"Base"}],"visibility":"Public","body":"LibLlama.llama_sampler_chain_add(@handle, sampler.to_unsafe)\n@samplers << sampler\n"},"external_var":false},{"html_id":"finalize-instance-method","name":"finalize","doc":"Frees the resources associated with this sampler chain\nOverrides the parent class's finalize method to also clean up the samplers array","summary":"<p>Frees the resources associated with this sampler chain Overrides the parent class's finalize method to also clean up the samplers array</p>","abstract":false,"location":{"filename":"src/llama/sampler/chain.cr","line_number":67,"url":"https://github.com/kojix2/llama.cr/blob/038f4ed10509841d5e14fe7055fbc1a21dac6a15/src/llama/sampler/chain.cr#L67"},"def":{"name":"finalize","visibility":"Public","body":"if @samplers\n  @samplers.clear\nend\nsuper()\n"},"external_var":false},{"html_id":"print_perf-instance-method","name":"print_perf","doc":"Print performance information for this sampler chain\n\nThis method prints performance statistics about the sampler chain to STDERR.\nIt's useful for debugging and performance analysis.","summary":"<p>Print performance information for this sampler chain</p>","abstract":false,"location":{"filename":"src/llama/sampler/chain.cr","line_number":77,"url":"https://github.com/kojix2/llama.cr/blob/038f4ed10509841d5e14fe7055fbc1a21dac6a15/src/llama/sampler/chain.cr#L77"},"def":{"name":"print_perf","visibility":"Public","body":"LibLlama.llama_perf_sampler_print(@handle)"},"external_var":false},{"html_id":"reset_perf-instance-method","name":"reset_perf","doc":"Reset performance counters for this sampler chain\n\nThis method resets all performance counters for the sampler chain.","summary":"<p>Reset performance counters for this sampler chain</p>","abstract":false,"location":{"filename":"src/llama/sampler/chain.cr","line_number":84,"url":"https://github.com/kojix2/llama.cr/blob/038f4ed10509841d5e14fe7055fbc1a21dac6a15/src/llama/sampler/chain.cr#L84"},"def":{"name":"reset_perf","visibility":"Public","body":"LibLlama.llama_perf_sampler_reset(@handle)"},"external_var":false},{"html_id":"sample(ctx:Context,idx:Int32=-1):Int32-instance-method","name":"sample","doc":"Samples a token using the sampler chain\n\nParameters:\n- ctx: The context to sample from\n- idx: The index of the logits to sample from (-1 for the last token)\n\nReturns:\n- The sampled token","summary":"<p>Samples a token using the sampler chain</p>","abstract":false,"args":[{"name":"ctx","external_name":"ctx","restriction":"Context"},{"name":"idx","default_value":"-1","external_name":"idx","restriction":"Int32"}],"args_string":"(ctx : Context, idx : Int32 = -1) : Int32","args_html":"(ctx : <a href=\"../../Llama/Context.html\">Context</a>, idx : Int32 = <span class=\"n\">-1</span>) : Int32","location":{"filename":"src/llama/sampler/chain.cr","line_number":53,"url":"https://github.com/kojix2/llama.cr/blob/038f4ed10509841d5e14fe7055fbc1a21dac6a15/src/llama/sampler/chain.cr#L53"},"def":{"name":"sample","args":[{"name":"ctx","external_name":"ctx","restriction":"Context"},{"name":"idx","default_value":"-1","external_name":"idx","restriction":"Int32"}],"return_type":"Int32","visibility":"Public","body":"LibLlama.llama_sampler_sample(@handle, ctx.to_unsafe, idx)"},"external_var":false}]},{"html_id":"llama/Llama/Sampler/Dist","path":"Llama/Sampler/Dist.html","kind":"class","full_name":"Llama::Sampler::Dist","name":"Dist","abstract":false,"superclass":{"html_id":"llama/Llama/Sampler/Base","kind":"class","full_name":"Llama::Sampler::Base","name":"Base"},"ancestors":[{"html_id":"llama/Llama/Sampler/Base","kind":"class","full_name":"Llama::Sampler::Base","name":"Base"},{"html_id":"llama/Reference","kind":"class","full_name":"Reference","name":"Reference"},{"html_id":"llama/Object","kind":"class","full_name":"Object","name":"Object"}],"locations":[{"filename":"src/llama/sampler/dist.cr","line_number":14,"url":"https://github.com/kojix2/llama.cr/blob/038f4ed10509841d5e14fe7055fbc1a21dac6a15/src/llama/sampler/dist.cr#L14"}],"repository_name":"llama","program":false,"enum":false,"alias":false,"const":false,"namespace":{"html_id":"llama/Llama/Sampler","kind":"module","full_name":"Llama::Sampler","name":"Sampler"},"doc":"Distribution sampler (final sampler in a chain)\n\nThe Distribution sampler is typically the final sampler in a chain.\nIt samples a token from the probability distribution after all other\nsamplers have filtered the logits. This sampler is required to actually\nselect a token from the distribution.\n\nExample:\n```\nsampler = Llama::Sampler::Dist.new(42) # Use seed 42 for reproducibility\n```","summary":"<p>Distribution sampler (final sampler in a chain)</p>","constructors":[{"html_id":"new(seed:UInt32=LibLlama::LLAMA_DEFAULT_SEED)-class-method","name":"new","doc":"Creates a new distribution sampler\n\nParameters:\n- seed: Random seed for sampling (default: LLAMA_DEFAULT_SEED)\n\nRaises:\n- Llama::Error if the sampler cannot be created","summary":"<p>Creates a new distribution sampler</p>","abstract":false,"args":[{"name":"seed","default_value":"LibLlama::LLAMA_DEFAULT_SEED","external_name":"seed","restriction":"UInt32"}],"args_string":"(seed : UInt32 = LibLlama::LLAMA_DEFAULT_SEED)","args_html":"(seed : UInt32 = <span class=\"t\">LibLlama</span><span class=\"t\">::</span><span class=\"t\">LLAMA_DEFAULT_SEED</span>)","location":{"filename":"src/llama/sampler/dist.cr","line_number":22,"url":"https://github.com/kojix2/llama.cr/blob/038f4ed10509841d5e14fe7055fbc1a21dac6a15/src/llama/sampler/dist.cr#L22"},"def":{"name":"new","args":[{"name":"seed","default_value":"LibLlama::LLAMA_DEFAULT_SEED","external_name":"seed","restriction":"UInt32"}],"visibility":"Public","body":"_ = allocate\n_.initialize(seed)\nif _.responds_to?(:finalize)\n  ::GC.add_finalizer(_)\nend\n_\n"},"external_var":false}]},{"html_id":"llama/Llama/Sampler/Error","path":"Llama/Sampler/Error.html","kind":"class","full_name":"Llama::Sampler::Error","name":"Error","abstract":false,"superclass":{"html_id":"llama/Llama/Error","kind":"class","full_name":"Llama::Error","name":"Error"},"ancestors":[{"html_id":"llama/Llama/Error","kind":"class","full_name":"Llama::Error","name":"Error"},{"html_id":"llama/Exception","kind":"class","full_name":"Exception","name":"Exception"},{"html_id":"llama/Reference","kind":"class","full_name":"Reference","name":"Reference"},{"html_id":"llama/Object","kind":"class","full_name":"Object","name":"Object"}],"locations":[{"filename":"src/llama/sampler/error.cr","line_number":5,"url":"https://github.com/kojix2/llama.cr/blob/038f4ed10509841d5e14fe7055fbc1a21dac6a15/src/llama/sampler/error.cr#L5"}],"repository_name":"llama","program":false,"enum":false,"alias":false,"const":false,"subclasses":[{"html_id":"llama/Llama/Sampler/TokenizationError","kind":"class","full_name":"Llama::Sampler::TokenizationError","name":"TokenizationError"}],"namespace":{"html_id":"llama/Llama/Sampler","kind":"module","full_name":"Llama::Sampler","name":"Sampler"}},{"html_id":"llama/Llama/Sampler/Grammar","path":"Llama/Sampler/Grammar.html","kind":"class","full_name":"Llama::Sampler::Grammar","name":"Grammar","abstract":false,"superclass":{"html_id":"llama/Llama/Sampler/Base","kind":"class","full_name":"Llama::Sampler::Base","name":"Base"},"ancestors":[{"html_id":"llama/Llama/Sampler/Base","kind":"class","full_name":"Llama::Sampler::Base","name":"Base"},{"html_id":"llama/Reference","kind":"class","full_name":"Reference","name":"Reference"},{"html_id":"llama/Object","kind":"class","full_name":"Object","name":"Object"}],"locations":[{"filename":"src/llama/sampler/grammar.cr","line_number":22,"url":"https://github.com/kojix2/llama.cr/blob/038f4ed10509841d5e14fe7055fbc1a21dac6a15/src/llama/sampler/grammar.cr#L22"}],"repository_name":"llama","program":false,"enum":false,"alias":false,"const":false,"namespace":{"html_id":"llama/Llama/Sampler","kind":"module","full_name":"Llama::Sampler","name":"Sampler"},"doc":"Grammar sampler\n\nThe Grammar sampler constrains token generation to follow a formal grammar\ndefined in GBNF format. This is useful for generating structured text like\nJSON, XML, or code that must follow specific syntax rules.\n\nExample:\n```\n# Define a simple grammar for a numbered list\ngrammar = %q{\n  root ::= list\n  list ::= item+\n  item ::= number \" \" text \"\\n\"\n  number ::= \"1\" | \"2\" | \"3\" | \"4\" | \"5\"\n  text ::= [a-zA-Z ]+\n}\n\nsampler = Llama::Sampler::Grammar.new(model.vocab, grammar, \"root\")\n```","summary":"<p>Grammar sampler</p>","constructors":[{"html_id":"new(vocab:Vocab,grammar_str:String,grammar_root:String)-class-method","name":"new","doc":"Creates a new Grammar sampler\n\nParameters:\n- vocab: The vocabulary to use\n- grammar_str: The grammar definition string in GBNF format\n- grammar_root: The root symbol of the grammar\n\nRaises:\n- Llama::Error if the sampler cannot be created","summary":"<p>Creates a new Grammar sampler</p>","abstract":false,"args":[{"name":"vocab","external_name":"vocab","restriction":"Vocab"},{"name":"grammar_str","external_name":"grammar_str","restriction":"String"},{"name":"grammar_root","external_name":"grammar_root","restriction":"String"}],"args_string":"(vocab : Vocab, grammar_str : String, grammar_root : String)","args_html":"(vocab : <a href=\"../../Llama/Vocab.html\">Vocab</a>, grammar_str : String, grammar_root : String)","location":{"filename":"src/llama/sampler/grammar.cr","line_number":32,"url":"https://github.com/kojix2/llama.cr/blob/038f4ed10509841d5e14fe7055fbc1a21dac6a15/src/llama/sampler/grammar.cr#L32"},"def":{"name":"new","args":[{"name":"vocab","external_name":"vocab","restriction":"Vocab"},{"name":"grammar_str","external_name":"grammar_str","restriction":"String"},{"name":"grammar_root","external_name":"grammar_root","restriction":"String"}],"visibility":"Public","body":"_ = allocate\n_.initialize(vocab, grammar_str, grammar_root)\nif _.responds_to?(:finalize)\n  ::GC.add_finalizer(_)\nend\n_\n"},"external_var":false}],"instance_methods":[{"html_id":"finalize-instance-method","name":"finalize","doc":"Overrides the parent class's finalize method to ensure proper cleanup","summary":"<p>Overrides the parent class's finalize method to ensure proper cleanup</p>","abstract":false,"location":{"filename":"src/llama/sampler/grammar.cr","line_number":47,"url":"https://github.com/kojix2/llama.cr/blob/038f4ed10509841d5e14fe7055fbc1a21dac6a15/src/llama/sampler/grammar.cr#L47"},"def":{"name":"finalize","visibility":"Public","body":"@vocab = nil\n@grammar_str = nil\n@grammar_root = nil\n\n\nsuper()\n"},"external_var":false}]},{"html_id":"llama/Llama/Sampler/GrammarLazyPatterns","path":"Llama/Sampler/GrammarLazyPatterns.html","kind":"class","full_name":"Llama::Sampler::GrammarLazyPatterns","name":"GrammarLazyPatterns","abstract":false,"superclass":{"html_id":"llama/Llama/Sampler/Base","kind":"class","full_name":"Llama::Sampler::Base","name":"Base"},"ancestors":[{"html_id":"llama/Llama/Sampler/Base","kind":"class","full_name":"Llama::Sampler::Base","name":"Base"},{"html_id":"llama/Reference","kind":"class","full_name":"Reference","name":"Reference"},{"html_id":"llama/Object","kind":"class","full_name":"Object","name":"Object"}],"locations":[{"filename":"src/llama/sampler/grammar_lazy_patterns.cr","line_number":28,"url":"https://github.com/kojix2/llama.cr/blob/038f4ed10509841d5e14fe7055fbc1a21dac6a15/src/llama/sampler/grammar_lazy_patterns.cr#L28"}],"repository_name":"llama","program":false,"enum":false,"alias":false,"const":false,"namespace":{"html_id":"llama/Llama/Sampler","kind":"module","full_name":"Llama::Sampler","name":"Sampler"},"doc":"Grammar Lazy Patterns sampler\n\nThe Grammar Lazy Patterns sampler is an extension of the Grammar sampler\nthat only applies grammar constraints when triggered by specific patterns\nor tokens. This is useful for mixed-format generation where grammar\nconstraints should only apply to certain parts of the output.\n\nExample:\n```\n# Define a JSON grammar that only activates when the text contains \"JSON:\"\ngrammar = %q{\n  root ::= object\n  object ::= \"{\" ws (string \":\" ws value (\",\" ws string \":\" ws value)*)? \"}\" ws\n  array ::= \"[\" ws (value (\",\" ws value)*)? \"]\" ws\n  value ::= object | array | string | number | \"true\" | \"false\" | \"null\"\n  string ::= \"\\\"\" ([^\"\\\\] | \"\\\\\" .)* \"\\\"\"\n  number ::= \"-\"? (\"0\" | [1-9] [0-9]*) (\".\" [0-9]+)? ([eE] [-+]? [0-9]+)?\n  ws ::= [ \\t\\n]*\n}\n\ntrigger_patterns = [\"JSON:\"]\nsampler = Llama::Sampler::GrammarLazyPatterns.new(\n  model.vocab, grammar, \"root\", trigger_patterns\n)\n```","summary":"<p>Grammar Lazy Patterns sampler</p>","constructors":[{"html_id":"new(vocab:Vocab,grammar_str:String,grammar_root:String,trigger_patterns:Array(String)=[]ofString,trigger_tokens:Array(Int32)=[]ofInt32)-class-method","name":"new","doc":"Creates a new Grammar Lazy Patterns sampler\n\nParameters:\n- vocab: The vocabulary to use\n- grammar_str: The grammar definition string in GBNF format\n- grammar_root: The root symbol of the grammar\n- trigger_patterns: Array of string patterns that will trigger the grammar\n- trigger_tokens: Array of token IDs that will trigger the grammar\n\nRaises:\n- Llama::Error if the sampler cannot be created","summary":"<p>Creates a new Grammar Lazy Patterns sampler</p>","abstract":false,"args":[{"name":"vocab","external_name":"vocab","restriction":"Vocab"},{"name":"grammar_str","external_name":"grammar_str","restriction":"String"},{"name":"grammar_root","external_name":"grammar_root","restriction":"String"},{"name":"trigger_patterns","default_value":"[] of String","external_name":"trigger_patterns","restriction":"Array(String)"},{"name":"trigger_tokens","default_value":"[] of Int32","external_name":"trigger_tokens","restriction":"Array(Int32)"}],"args_string":"(vocab : Vocab, grammar_str : String, grammar_root : String, trigger_patterns : Array(String) = [] of String, trigger_tokens : Array(Int32) = [] of Int32)","args_html":"(vocab : <a href=\"../../Llama/Vocab.html\">Vocab</a>, grammar_str : String, grammar_root : String, trigger_patterns : Array(String) = <span class=\"o\">[]</span> <span class=\"k\">of</span> <span class=\"t\">String</span>, trigger_tokens : Array(Int32) = <span class=\"o\">[]</span> <span class=\"k\">of</span> <span class=\"t\">Int32</span>)","location":{"filename":"src/llama/sampler/grammar_lazy_patterns.cr","line_number":40,"url":"https://github.com/kojix2/llama.cr/blob/038f4ed10509841d5e14fe7055fbc1a21dac6a15/src/llama/sampler/grammar_lazy_patterns.cr#L40"},"def":{"name":"new","args":[{"name":"vocab","external_name":"vocab","restriction":"Vocab"},{"name":"grammar_str","external_name":"grammar_str","restriction":"String"},{"name":"grammar_root","external_name":"grammar_root","restriction":"String"},{"name":"trigger_patterns","default_value":"[] of String","external_name":"trigger_patterns","restriction":"Array(String)"},{"name":"trigger_tokens","default_value":"[] of Int32","external_name":"trigger_tokens","restriction":"Array(Int32)"}],"visibility":"Public","body":"_ = allocate\n_.initialize(vocab, grammar_str, grammar_root, trigger_patterns, trigger_tokens)\nif _.responds_to?(:finalize)\n  ::GC.add_finalizer(_)\nend\n_\n"},"external_var":false}],"instance_methods":[{"html_id":"finalize-instance-method","name":"finalize","doc":"Overrides the parent class's finalize method to ensure proper cleanup","summary":"<p>Overrides the parent class's finalize method to ensure proper cleanup</p>","abstract":false,"location":{"filename":"src/llama/sampler/grammar_lazy_patterns.cr","line_number":83,"url":"https://github.com/kojix2/llama.cr/blob/038f4ed10509841d5e14fe7055fbc1a21dac6a15/src/llama/sampler/grammar_lazy_patterns.cr#L83"},"def":{"name":"finalize","visibility":"Public","body":"@vocab = nil\n@grammar_str = nil\n@grammar_root = nil\n@trigger_patterns = nil\n@trigger_tokens = nil\n\n\nsuper()\n"},"external_var":false}]},{"html_id":"llama/Llama/Sampler/Greedy","path":"Llama/Sampler/Greedy.html","kind":"class","full_name":"Llama::Sampler::Greedy","name":"Greedy","abstract":false,"superclass":{"html_id":"llama/Llama/Sampler/Base","kind":"class","full_name":"Llama::Sampler::Base","name":"Base"},"ancestors":[{"html_id":"llama/Llama/Sampler/Base","kind":"class","full_name":"Llama::Sampler::Base","name":"Base"},{"html_id":"llama/Reference","kind":"class","full_name":"Reference","name":"Reference"},{"html_id":"llama/Object","kind":"class","full_name":"Object","name":"Object"}],"locations":[{"filename":"src/llama/sampler/greedy.cr","line_number":10,"url":"https://github.com/kojix2/llama.cr/blob/038f4ed10509841d5e14fe7055fbc1a21dac6a15/src/llama/sampler/greedy.cr#L10"}],"repository_name":"llama","program":false,"enum":false,"alias":false,"const":false,"namespace":{"html_id":"llama/Llama/Sampler","kind":"module","full_name":"Llama::Sampler","name":"Sampler"},"doc":"The Greedy sampler always selects the token with the highest probability.\nThis is the simplest sampling method and produces deterministic output.\n\nExample:\n```\nsampler = Llama::Sampler::Greedy.new\n```","summary":"<p>The Greedy sampler always selects the token with the highest probability.</p>","constructors":[{"html_id":"new-class-method","name":"new","doc":"Creates a new greedy sampler\n\nRaises:\n- Llama::Error if the sampler cannot be created","summary":"<p>Creates a new greedy sampler</p>","abstract":false,"location":{"filename":"src/llama/sampler/greedy.cr","line_number":15,"url":"https://github.com/kojix2/llama.cr/blob/038f4ed10509841d5e14fe7055fbc1a21dac6a15/src/llama/sampler/greedy.cr#L15"},"def":{"name":"new","visibility":"Public","body":"_ = allocate\n_.initialize\nif _.responds_to?(:finalize)\n  ::GC.add_finalizer(_)\nend\n_\n"},"external_var":false}]},{"html_id":"llama/Llama/Sampler/Infill","path":"Llama/Sampler/Infill.html","kind":"class","full_name":"Llama::Sampler::Infill","name":"Infill","abstract":false,"superclass":{"html_id":"llama/Llama/Sampler/Base","kind":"class","full_name":"Llama::Sampler::Base","name":"Base"},"ancestors":[{"html_id":"llama/Llama/Sampler/Base","kind":"class","full_name":"Llama::Sampler::Base","name":"Base"},{"html_id":"llama/Reference","kind":"class","full_name":"Reference","name":"Reference"},{"html_id":"llama/Object","kind":"class","full_name":"Object","name":"Object"}],"locations":[{"filename":"src/llama/sampler/infill.cr","line_number":12,"url":"https://github.com/kojix2/llama.cr/blob/038f4ed10509841d5e14fe7055fbc1a21dac6a15/src/llama/sampler/infill.cr#L12"}],"repository_name":"llama","program":false,"enum":false,"alias":false,"const":false,"namespace":{"html_id":"llama/Llama/Sampler","kind":"module","full_name":"Llama::Sampler","name":"Sampler"},"doc":"Infill sampler\n\nThe Infill sampler is designed for fill-in-the-middle (FIM) tasks.\nIt helps to generate text that fits naturally between existing content.\n\nExample:\n```\nsampler = Llama::Sampler::Infill.new(model.vocab)\n```","summary":"<p>Infill sampler</p>","constructors":[{"html_id":"new(vocab:Vocab)-class-method","name":"new","doc":"Creates a new Infill sampler\n\nParameters:\n- vocab: The vocabulary to use\n\nRaises:\n- Llama::Error if the sampler cannot be created","summary":"<p>Creates a new Infill sampler</p>","abstract":false,"args":[{"name":"vocab","external_name":"vocab","restriction":"Vocab"}],"args_string":"(vocab : Vocab)","args_html":"(vocab : <a href=\"../../Llama/Vocab.html\">Vocab</a>)","location":{"filename":"src/llama/sampler/infill.cr","line_number":20,"url":"https://github.com/kojix2/llama.cr/blob/038f4ed10509841d5e14fe7055fbc1a21dac6a15/src/llama/sampler/infill.cr#L20"},"def":{"name":"new","args":[{"name":"vocab","external_name":"vocab","restriction":"Vocab"}],"visibility":"Public","body":"_ = allocate\n_.initialize(vocab)\nif _.responds_to?(:finalize)\n  ::GC.add_finalizer(_)\nend\n_\n"},"external_var":false}],"instance_methods":[{"html_id":"finalize-instance-method","name":"finalize","doc":"Overrides the parent class's finalize method to ensure proper cleanup","summary":"<p>Overrides the parent class's finalize method to ensure proper cleanup</p>","abstract":false,"location":{"filename":"src/llama/sampler/infill.cr","line_number":29,"url":"https://github.com/kojix2/llama.cr/blob/038f4ed10509841d5e14fe7055fbc1a21dac6a15/src/llama/sampler/infill.cr#L29"},"def":{"name":"finalize","visibility":"Public","body":"@vocab = nil\n\n\nsuper()\n"},"external_var":false}]},{"html_id":"llama/Llama/Sampler/MinP","path":"Llama/Sampler/MinP.html","kind":"class","full_name":"Llama::Sampler::MinP","name":"MinP","abstract":false,"superclass":{"html_id":"llama/Llama/Sampler/Base","kind":"class","full_name":"Llama::Sampler::Base","name":"Base"},"ancestors":[{"html_id":"llama/Llama/Sampler/Base","kind":"class","full_name":"Llama::Sampler::Base","name":"Base"},{"html_id":"llama/Reference","kind":"class","full_name":"Reference","name":"Reference"},{"html_id":"llama/Object","kind":"class","full_name":"Object","name":"Object"}],"locations":[{"filename":"src/llama/sampler/min_p.cr","line_number":13,"url":"https://github.com/kojix2/llama.cr/blob/038f4ed10509841d5e14fe7055fbc1a21dac6a15/src/llama/sampler/min_p.cr#L13"}],"repository_name":"llama","program":false,"enum":false,"alias":false,"const":false,"namespace":{"html_id":"llama/Llama/Sampler","kind":"module","full_name":"Llama::Sampler","name":"Sampler"},"doc":"Min-P sampler\n\nThe Min-P sampler keeps tokens with probability >= P * max_probability.\nThis is similar to Top-P but uses a minimum probability threshold relative\nto the most likely token, rather than a cumulative probability threshold.\n\nExample:\n```\nsampler = Llama::Sampler::MinP.new(0.05, 1) # Keep tokens with prob >= 5% of max prob\n```","summary":"<p>Min-P sampler</p>","constructors":[{"html_id":"new(p:Float32,min_keep:Int32=1)-class-method","name":"new","doc":"Creates a new Min-P sampler\n\nParameters:\n- p: The minimum probability threshold (0.0 to 1.0)\n- min_keep: Minimum number of tokens to keep\n\nRaises:\n- Llama::Error if the sampler cannot be created","summary":"<p>Creates a new Min-P sampler</p>","abstract":false,"args":[{"name":"p","external_name":"p","restriction":"Float32"},{"name":"min_keep","default_value":"1","external_name":"min_keep","restriction":"Int32"}],"args_string":"(p : Float32, min_keep : Int32 = 1)","args_html":"(p : Float32, min_keep : Int32 = <span class=\"n\">1</span>)","location":{"filename":"src/llama/sampler/min_p.cr","line_number":22,"url":"https://github.com/kojix2/llama.cr/blob/038f4ed10509841d5e14fe7055fbc1a21dac6a15/src/llama/sampler/min_p.cr#L22"},"def":{"name":"new","args":[{"name":"p","external_name":"p","restriction":"Float32"},{"name":"min_keep","default_value":"1","external_name":"min_keep","restriction":"Int32"}],"visibility":"Public","body":"_ = allocate\n_.initialize(p, min_keep)\nif _.responds_to?(:finalize)\n  ::GC.add_finalizer(_)\nend\n_\n"},"external_var":false}]},{"html_id":"llama/Llama/Sampler/Mirostat","path":"Llama/Sampler/Mirostat.html","kind":"class","full_name":"Llama::Sampler::Mirostat","name":"Mirostat","abstract":false,"superclass":{"html_id":"llama/Llama/Sampler/Base","kind":"class","full_name":"Llama::Sampler::Base","name":"Base"},"ancestors":[{"html_id":"llama/Llama/Sampler/Base","kind":"class","full_name":"Llama::Sampler::Base","name":"Base"},{"html_id":"llama/Reference","kind":"class","full_name":"Reference","name":"Reference"},{"html_id":"llama/Object","kind":"class","full_name":"Object","name":"Object"}],"locations":[{"filename":"src/llama/sampler/mirostat.cr","line_number":15,"url":"https://github.com/kojix2/llama.cr/blob/038f4ed10509841d5e14fe7055fbc1a21dac6a15/src/llama/sampler/mirostat.cr#L15"}],"repository_name":"llama","program":false,"enum":false,"alias":false,"const":false,"namespace":{"html_id":"llama/Llama/Sampler","kind":"module","full_name":"Llama::Sampler","name":"Sampler"},"doc":"Mirostat sampler (version 1)\n\nThe Mirostat sampler dynamically adjusts the temperature to maintain\na target entropy level in the generated text. This helps to produce\nconsistent quality output regardless of the context.\n\nBased on the paper: https://arxiv.org/abs/2007.14966\n\nExample:\n```\nsampler = Llama::Sampler::Mirostat.new(32000, 42, 5.0, 0.1, 100)\n```","summary":"<p>Mirostat sampler (version 1)</p>","constructors":[{"html_id":"new(n_vocab:Int32,seed:UInt32,tau:Float32,eta:Float32,m:Int32)-class-method","name":"new","doc":"Creates a new Mirostat sampler\n\nParameters:\n- n_vocab: Vocabulary size\n- seed: Random seed\n- tau: Target entropy (5.0 - 8.0 is a good range)\n- eta: Learning rate (0.1 is a good default)\n- m: Number of tokens for estimating entropy (100 is a good default)\n\nRaises:\n- Llama::Error if the sampler cannot be created","summary":"<p>Creates a new Mirostat sampler</p>","abstract":false,"args":[{"name":"n_vocab","external_name":"n_vocab","restriction":"Int32"},{"name":"seed","external_name":"seed","restriction":"UInt32"},{"name":"tau","external_name":"tau","restriction":"Float32"},{"name":"eta","external_name":"eta","restriction":"Float32"},{"name":"m","external_name":"m","restriction":"Int32"}],"args_string":"(n_vocab : Int32, seed : UInt32, tau : Float32, eta : Float32, m : Int32)","args_html":"(n_vocab : Int32, seed : UInt32, tau : Float32, eta : Float32, m : Int32)","location":{"filename":"src/llama/sampler/mirostat.cr","line_number":27,"url":"https://github.com/kojix2/llama.cr/blob/038f4ed10509841d5e14fe7055fbc1a21dac6a15/src/llama/sampler/mirostat.cr#L27"},"def":{"name":"new","args":[{"name":"n_vocab","external_name":"n_vocab","restriction":"Int32"},{"name":"seed","external_name":"seed","restriction":"UInt32"},{"name":"tau","external_name":"tau","restriction":"Float32"},{"name":"eta","external_name":"eta","restriction":"Float32"},{"name":"m","external_name":"m","restriction":"Int32"}],"visibility":"Public","body":"_ = allocate\n_.initialize(n_vocab, seed, tau, eta, m)\nif _.responds_to?(:finalize)\n  ::GC.add_finalizer(_)\nend\n_\n"},"external_var":false}]},{"html_id":"llama/Llama/Sampler/MirostatV2","path":"Llama/Sampler/MirostatV2.html","kind":"class","full_name":"Llama::Sampler::MirostatV2","name":"MirostatV2","abstract":false,"superclass":{"html_id":"llama/Llama/Sampler/Base","kind":"class","full_name":"Llama::Sampler::Base","name":"Base"},"ancestors":[{"html_id":"llama/Llama/Sampler/Base","kind":"class","full_name":"Llama::Sampler::Base","name":"Base"},{"html_id":"llama/Reference","kind":"class","full_name":"Reference","name":"Reference"},{"html_id":"llama/Object","kind":"class","full_name":"Object","name":"Object"}],"locations":[{"filename":"src/llama/sampler/mirostat_v2.cr","line_number":15,"url":"https://github.com/kojix2/llama.cr/blob/038f4ed10509841d5e14fe7055fbc1a21dac6a15/src/llama/sampler/mirostat_v2.cr#L15"}],"repository_name":"llama","program":false,"enum":false,"alias":false,"const":false,"namespace":{"html_id":"llama/Llama/Sampler","kind":"module","full_name":"Llama::Sampler","name":"Sampler"},"doc":"Mirostat sampler (version 2)\n\nThe Mirostat V2 sampler is an improved version of the Mirostat algorithm\nthat requires fewer parameters and is more efficient. It dynamically\nadjusts sampling to maintain a target entropy level.\n\nBased on the paper: https://arxiv.org/abs/2007.14966\n\nExample:\n```\nsampler = Llama::Sampler::MirostatV2.new(42, 5.0, 0.1)\n```","summary":"<p>Mirostat sampler (version 2)</p>","constructors":[{"html_id":"new(seed:UInt32,tau:Float32,eta:Float32)-class-method","name":"new","doc":"Creates a new Mirostat V2 sampler\n\nParameters:\n- seed: Random seed\n- tau: Target entropy (5.0 is a good default)\n- eta: Learning rate (0.1 is a good default)\n\nRaises:\n- Llama::Error if the sampler cannot be created","summary":"<p>Creates a new Mirostat V2 sampler</p>","abstract":false,"args":[{"name":"seed","external_name":"seed","restriction":"UInt32"},{"name":"tau","external_name":"tau","restriction":"Float32"},{"name":"eta","external_name":"eta","restriction":"Float32"}],"args_string":"(seed : UInt32, tau : Float32, eta : Float32)","args_html":"(seed : UInt32, tau : Float32, eta : Float32)","location":{"filename":"src/llama/sampler/mirostat_v2.cr","line_number":25,"url":"https://github.com/kojix2/llama.cr/blob/038f4ed10509841d5e14fe7055fbc1a21dac6a15/src/llama/sampler/mirostat_v2.cr#L25"},"def":{"name":"new","args":[{"name":"seed","external_name":"seed","restriction":"UInt32"},{"name":"tau","external_name":"tau","restriction":"Float32"},{"name":"eta","external_name":"eta","restriction":"Float32"}],"visibility":"Public","body":"_ = allocate\n_.initialize(seed, tau, eta)\nif _.responds_to?(:finalize)\n  ::GC.add_finalizer(_)\nend\n_\n"},"external_var":false}]},{"html_id":"llama/Llama/Sampler/Penalties","path":"Llama/Sampler/Penalties.html","kind":"class","full_name":"Llama::Sampler::Penalties","name":"Penalties","abstract":false,"superclass":{"html_id":"llama/Llama/Sampler/Base","kind":"class","full_name":"Llama::Sampler::Base","name":"Base"},"ancestors":[{"html_id":"llama/Llama/Sampler/Base","kind":"class","full_name":"Llama::Sampler::Base","name":"Base"},{"html_id":"llama/Reference","kind":"class","full_name":"Reference","name":"Reference"},{"html_id":"llama/Object","kind":"class","full_name":"Object","name":"Object"}],"locations":[{"filename":"src/llama/sampler/penalties.cr","line_number":14,"url":"https://github.com/kojix2/llama.cr/blob/038f4ed10509841d5e14fe7055fbc1a21dac6a15/src/llama/sampler/penalties.cr#L14"}],"repository_name":"llama","program":false,"enum":false,"alias":false,"const":false,"namespace":{"html_id":"llama/Llama/Sampler","kind":"module","full_name":"Llama::Sampler","name":"Sampler"},"doc":"Penalties sampler\n\nThe Penalties sampler applies various penalties to token probabilities\nto reduce repetition and improve diversity in the generated text.\nIt can penalize recently used tokens, frequent tokens, and more.\n\nExample:\n```\n# Apply penalties to the last 64 tokens with a repetition penalty of 1.1\nsampler = Llama::Sampler::Penalties.new(64, 1.1, 0.0, 0.0)\n```","summary":"<p>Penalties sampler</p>","constructors":[{"html_id":"new(penalty_last_n:Int32,penalty_repeat:Float32,penalty_freq:Float32,penalty_present:Float32)-class-method","name":"new","doc":"Creates a new Penalties sampler\n\nParameters:\n- penalty_last_n: Last n tokens to penalize (0 = disable, -1 = context size)\n- penalty_repeat: Repetition penalty (1.0 = disabled)\n- penalty_freq: Frequency penalty (0.0 = disabled)\n- penalty_present: Presence penalty (0.0 = disabled)\n\nRaises:\n- Llama::Error if the sampler cannot be created","summary":"<p>Creates a new Penalties sampler</p>","abstract":false,"args":[{"name":"penalty_last_n","external_name":"penalty_last_n","restriction":"Int32"},{"name":"penalty_repeat","external_name":"penalty_repeat","restriction":"Float32"},{"name":"penalty_freq","external_name":"penalty_freq","restriction":"Float32"},{"name":"penalty_present","external_name":"penalty_present","restriction":"Float32"}],"args_string":"(penalty_last_n : Int32, penalty_repeat : Float32, penalty_freq : Float32, penalty_present : Float32)","args_html":"(penalty_last_n : Int32, penalty_repeat : Float32, penalty_freq : Float32, penalty_present : Float32)","location":{"filename":"src/llama/sampler/penalties.cr","line_number":25,"url":"https://github.com/kojix2/llama.cr/blob/038f4ed10509841d5e14fe7055fbc1a21dac6a15/src/llama/sampler/penalties.cr#L25"},"def":{"name":"new","args":[{"name":"penalty_last_n","external_name":"penalty_last_n","restriction":"Int32"},{"name":"penalty_repeat","external_name":"penalty_repeat","restriction":"Float32"},{"name":"penalty_freq","external_name":"penalty_freq","restriction":"Float32"},{"name":"penalty_present","external_name":"penalty_present","restriction":"Float32"}],"visibility":"Public","body":"_ = allocate\n_.initialize(penalty_last_n, penalty_repeat, penalty_freq, penalty_present)\nif _.responds_to?(:finalize)\n  ::GC.add_finalizer(_)\nend\n_\n"},"external_var":false}]},{"html_id":"llama/Llama/Sampler/Temp","path":"Llama/Sampler/Temp.html","kind":"class","full_name":"Llama::Sampler::Temp","name":"Temp","abstract":false,"superclass":{"html_id":"llama/Llama/Sampler/Base","kind":"class","full_name":"Llama::Sampler::Base","name":"Base"},"ancestors":[{"html_id":"llama/Llama/Sampler/Base","kind":"class","full_name":"Llama::Sampler::Base","name":"Base"},{"html_id":"llama/Reference","kind":"class","full_name":"Reference","name":"Reference"},{"html_id":"llama/Object","kind":"class","full_name":"Object","name":"Object"}],"locations":[{"filename":"src/llama/sampler/temp.cr","line_number":14,"url":"https://github.com/kojix2/llama.cr/blob/038f4ed10509841d5e14fe7055fbc1a21dac6a15/src/llama/sampler/temp.cr#L14"}],"repository_name":"llama","program":false,"enum":false,"alias":false,"const":false,"namespace":{"html_id":"llama/Llama/Sampler","kind":"module","full_name":"Llama::Sampler","name":"Sampler"},"doc":"Temperature sampler\n\nThe Temperature sampler adjusts the logits by dividing them by the temperature\nvalue. Higher temperatures (>1.0) make the distribution more uniform, leading\nto more random outputs. Lower temperatures (<1.0) make the distribution more\npeaked, leading to more deterministic outputs.\n\nExample:\n```\nsampler = Llama::Sampler::Temp.new(0.8) # Slightly more deterministic than default\n```","summary":"<p>Temperature sampler</p>","constructors":[{"html_id":"new(temp:Float32)-class-method","name":"new","doc":"Creates a new temperature sampler\n\nParameters:\n- temp: The temperature value (0.0 = greedy, 1.0 = normal, >1.0 = more random)\n\nRaises:\n- Llama::Error if the sampler cannot be created","summary":"<p>Creates a new temperature sampler</p>","abstract":false,"args":[{"name":"temp","external_name":"temp","restriction":"Float32"}],"args_string":"(temp : Float32)","args_html":"(temp : Float32)","location":{"filename":"src/llama/sampler/temp.cr","line_number":22,"url":"https://github.com/kojix2/llama.cr/blob/038f4ed10509841d5e14fe7055fbc1a21dac6a15/src/llama/sampler/temp.cr#L22"},"def":{"name":"new","args":[{"name":"temp","external_name":"temp","restriction":"Float32"}],"visibility":"Public","body":"_ = allocate\n_.initialize(temp)\nif _.responds_to?(:finalize)\n  ::GC.add_finalizer(_)\nend\n_\n"},"external_var":false}]},{"html_id":"llama/Llama/Sampler/TempExt","path":"Llama/Sampler/TempExt.html","kind":"class","full_name":"Llama::Sampler::TempExt","name":"TempExt","abstract":false,"superclass":{"html_id":"llama/Llama/Sampler/Base","kind":"class","full_name":"Llama::Sampler::Base","name":"Base"},"ancestors":[{"html_id":"llama/Llama/Sampler/Base","kind":"class","full_name":"Llama::Sampler::Base","name":"Base"},{"html_id":"llama/Reference","kind":"class","full_name":"Reference","name":"Reference"},{"html_id":"llama/Object","kind":"class","full_name":"Object","name":"Object"}],"locations":[{"filename":"src/llama/sampler/temp_ext.cr","line_number":14,"url":"https://github.com/kojix2/llama.cr/blob/038f4ed10509841d5e14fe7055fbc1a21dac6a15/src/llama/sampler/temp_ext.cr#L14"}],"repository_name":"llama","program":false,"enum":false,"alias":false,"const":false,"namespace":{"html_id":"llama/Llama/Sampler","kind":"module","full_name":"Llama::Sampler","name":"Sampler"},"doc":"Extended Temperature sampler\n\nThe Extended Temperature sampler provides more control over temperature\nsampling with additional parameters for dynamic temperature adjustment.\nThis is based on the paper \"Dynamic Temperature for Language Models\" (https://arxiv.org/abs/2309.02772).\n\nExample:\n```\n# Create a temperature sampler with base temp 0.8, delta 0.5, and exponent 1.0\nsampler = Llama::Sampler::TempExt.new(0.8, 0.5, 1.0)\n```","summary":"<p>Extended Temperature sampler</p>","constructors":[{"html_id":"new(t:Float32,delta:Float32,exponent:Float32)-class-method","name":"new","doc":"Creates a new Extended Temperature sampler\n\nParameters:\n- t: Base temperature value\n- delta: Temperature delta for dynamic adjustment\n- exponent: Exponent for the temperature formula\n\nRaises:\n- Llama::Error if the sampler cannot be created","summary":"<p>Creates a new Extended Temperature sampler</p>","abstract":false,"args":[{"name":"t","external_name":"t","restriction":"Float32"},{"name":"delta","external_name":"delta","restriction":"Float32"},{"name":"exponent","external_name":"exponent","restriction":"Float32"}],"args_string":"(t : Float32, delta : Float32, exponent : Float32)","args_html":"(t : Float32, delta : Float32, exponent : Float32)","location":{"filename":"src/llama/sampler/temp_ext.cr","line_number":24,"url":"https://github.com/kojix2/llama.cr/blob/038f4ed10509841d5e14fe7055fbc1a21dac6a15/src/llama/sampler/temp_ext.cr#L24"},"def":{"name":"new","args":[{"name":"t","external_name":"t","restriction":"Float32"},{"name":"delta","external_name":"delta","restriction":"Float32"},{"name":"exponent","external_name":"exponent","restriction":"Float32"}],"visibility":"Public","body":"_ = allocate\n_.initialize(t, delta, exponent)\nif _.responds_to?(:finalize)\n  ::GC.add_finalizer(_)\nend\n_\n"},"external_var":false}]},{"html_id":"llama/Llama/Sampler/TokenizationError","path":"Llama/Sampler/TokenizationError.html","kind":"class","full_name":"Llama::Sampler::TokenizationError","name":"TokenizationError","abstract":false,"superclass":{"html_id":"llama/Llama/Sampler/Error","kind":"class","full_name":"Llama::Sampler::Error","name":"Error"},"ancestors":[{"html_id":"llama/Llama/Sampler/Error","kind":"class","full_name":"Llama::Sampler::Error","name":"Error"},{"html_id":"llama/Llama/Error","kind":"class","full_name":"Llama::Error","name":"Error"},{"html_id":"llama/Exception","kind":"class","full_name":"Exception","name":"Exception"},{"html_id":"llama/Reference","kind":"class","full_name":"Reference","name":"Reference"},{"html_id":"llama/Object","kind":"class","full_name":"Object","name":"Object"}],"locations":[{"filename":"src/llama/sampler/error.cr","line_number":7,"url":"https://github.com/kojix2/llama.cr/blob/038f4ed10509841d5e14fe7055fbc1a21dac6a15/src/llama/sampler/error.cr#L7"}],"repository_name":"llama","program":false,"enum":false,"alias":false,"const":false,"namespace":{"html_id":"llama/Llama/Sampler","kind":"module","full_name":"Llama::Sampler","name":"Sampler"}},{"html_id":"llama/Llama/Sampler/TopK","path":"Llama/Sampler/TopK.html","kind":"class","full_name":"Llama::Sampler::TopK","name":"TopK","abstract":false,"superclass":{"html_id":"llama/Llama/Sampler/Base","kind":"class","full_name":"Llama::Sampler::Base","name":"Base"},"ancestors":[{"html_id":"llama/Llama/Sampler/Base","kind":"class","full_name":"Llama::Sampler::Base","name":"Base"},{"html_id":"llama/Reference","kind":"class","full_name":"Reference","name":"Reference"},{"html_id":"llama/Object","kind":"class","full_name":"Object","name":"Object"}],"locations":[{"filename":"src/llama/sampler/top_k.cr","line_number":12,"url":"https://github.com/kojix2/llama.cr/blob/038f4ed10509841d5e14fe7055fbc1a21dac6a15/src/llama/sampler/top_k.cr#L12"}],"repository_name":"llama","program":false,"enum":false,"alias":false,"const":false,"namespace":{"html_id":"llama/Llama/Sampler","kind":"module","full_name":"Llama::Sampler","name":"Sampler"},"doc":"Top-K sampler\n\nThe Top-K sampler keeps only the K most likely tokens and zeros out the rest.\nThis is a simple but effective way to filter out unlikely tokens.\n\nExample:\n```\nsampler = Llama::Sampler::TopK.new(40) # Keep only the top 40 tokens\n```","summary":"<p>Top-K sampler</p>","constructors":[{"html_id":"new(k:Int32)-class-method","name":"new","doc":"Creates a new Top-K sampler\n\nParameters:\n- k: The number of top tokens to consider\n\nRaises:\n- Llama::Error if the sampler cannot be created","summary":"<p>Creates a new Top-K sampler</p>","abstract":false,"args":[{"name":"k","external_name":"k","restriction":"Int32"}],"args_string":"(k : Int32)","args_html":"(k : Int32)","location":{"filename":"src/llama/sampler/top_k.cr","line_number":20,"url":"https://github.com/kojix2/llama.cr/blob/038f4ed10509841d5e14fe7055fbc1a21dac6a15/src/llama/sampler/top_k.cr#L20"},"def":{"name":"new","args":[{"name":"k","external_name":"k","restriction":"Int32"}],"visibility":"Public","body":"_ = allocate\n_.initialize(k)\nif _.responds_to?(:finalize)\n  ::GC.add_finalizer(_)\nend\n_\n"},"external_var":false}]},{"html_id":"llama/Llama/Sampler/TopNSigma","path":"Llama/Sampler/TopNSigma.html","kind":"class","full_name":"Llama::Sampler::TopNSigma","name":"TopNSigma","abstract":false,"superclass":{"html_id":"llama/Llama/Sampler/Base","kind":"class","full_name":"Llama::Sampler::Base","name":"Base"},"ancestors":[{"html_id":"llama/Llama/Sampler/Base","kind":"class","full_name":"Llama::Sampler::Base","name":"Base"},{"html_id":"llama/Reference","kind":"class","full_name":"Reference","name":"Reference"},{"html_id":"llama/Object","kind":"class","full_name":"Object","name":"Object"}],"locations":[{"filename":"src/llama/sampler/top_n_sigma.cr","line_number":13,"url":"https://github.com/kojix2/llama.cr/blob/038f4ed10509841d5e14fe7055fbc1a21dac6a15/src/llama/sampler/top_n_sigma.cr#L13"}],"repository_name":"llama","program":false,"enum":false,"alias":false,"const":false,"namespace":{"html_id":"llama/Llama/Sampler","kind":"module","full_name":"Llama::Sampler","name":"Sampler"},"doc":"Top-N Sigma sampler\n\nThe Top-N Sigma sampler selects tokens based on their distance from the mean\nin terms of standard deviations. This is based on the paper \"Top-n: Not All Logits Are You Need\"\n(https://arxiv.org/pdf/2411.07641).\n\nExample:\n```\nsampler = Llama::Sampler::TopNSigma.new(2.0) # Keep tokens within 2 standard deviations\n```","summary":"<p>Top-N Sigma sampler</p>","constructors":[{"html_id":"new(n:Float32)-class-method","name":"new","doc":"Creates a new Top-N Sigma sampler\n\nParameters:\n- n: Number of standard deviations to keep\n\nRaises:\n- Llama::Error if the sampler cannot be created","summary":"<p>Creates a new Top-N Sigma sampler</p>","abstract":false,"args":[{"name":"n","external_name":"n","restriction":"Float32"}],"args_string":"(n : Float32)","args_html":"(n : Float32)","location":{"filename":"src/llama/sampler/top_n_sigma.cr","line_number":21,"url":"https://github.com/kojix2/llama.cr/blob/038f4ed10509841d5e14fe7055fbc1a21dac6a15/src/llama/sampler/top_n_sigma.cr#L21"},"def":{"name":"new","args":[{"name":"n","external_name":"n","restriction":"Float32"}],"visibility":"Public","body":"_ = allocate\n_.initialize(n)\nif _.responds_to?(:finalize)\n  ::GC.add_finalizer(_)\nend\n_\n"},"external_var":false}]},{"html_id":"llama/Llama/Sampler/TopP","path":"Llama/Sampler/TopP.html","kind":"class","full_name":"Llama::Sampler::TopP","name":"TopP","abstract":false,"superclass":{"html_id":"llama/Llama/Sampler/Base","kind":"class","full_name":"Llama::Sampler::Base","name":"Base"},"ancestors":[{"html_id":"llama/Llama/Sampler/Base","kind":"class","full_name":"Llama::Sampler::Base","name":"Base"},{"html_id":"llama/Reference","kind":"class","full_name":"Reference","name":"Reference"},{"html_id":"llama/Object","kind":"class","full_name":"Object","name":"Object"}],"locations":[{"filename":"src/llama/sampler/top_p.cr","line_number":13,"url":"https://github.com/kojix2/llama.cr/blob/038f4ed10509841d5e14fe7055fbc1a21dac6a15/src/llama/sampler/top_p.cr#L13"}],"repository_name":"llama","program":false,"enum":false,"alias":false,"const":false,"namespace":{"html_id":"llama/Llama/Sampler","kind":"module","full_name":"Llama::Sampler","name":"Sampler"},"doc":"Top-P (nucleus) sampler\n\nThe Top-P sampler (also known as nucleus sampling) keeps tokens whose\ncumulative probability exceeds the threshold P. This helps to filter out\nthe long tail of low-probability tokens.\n\nExample:\n```\nsampler = Llama::Sampler::TopP.new(0.95, 1) # Keep tokens until 95% probability is covered\n```","summary":"<p>Top-P (nucleus) sampler</p>","constructors":[{"html_id":"new(p:Float32,min_keep:Int32=1)-class-method","name":"new","doc":"Creates a new Top-P sampler\n\nParameters:\n- p: The cumulative probability threshold (0.0 to 1.0)\n- min_keep: Minimum number of tokens to keep\n\nRaises:\n- Llama::Error if the sampler cannot be created","summary":"<p>Creates a new Top-P sampler</p>","abstract":false,"args":[{"name":"p","external_name":"p","restriction":"Float32"},{"name":"min_keep","default_value":"1","external_name":"min_keep","restriction":"Int32"}],"args_string":"(p : Float32, min_keep : Int32 = 1)","args_html":"(p : Float32, min_keep : Int32 = <span class=\"n\">1</span>)","location":{"filename":"src/llama/sampler/top_p.cr","line_number":22,"url":"https://github.com/kojix2/llama.cr/blob/038f4ed10509841d5e14fe7055fbc1a21dac6a15/src/llama/sampler/top_p.cr#L22"},"def":{"name":"new","args":[{"name":"p","external_name":"p","restriction":"Float32"},{"name":"min_keep","default_value":"1","external_name":"min_keep","restriction":"Int32"}],"visibility":"Public","body":"_ = allocate\n_.initialize(p, min_keep)\nif _.responds_to?(:finalize)\n  ::GC.add_finalizer(_)\nend\n_\n"},"external_var":false}]},{"html_id":"llama/Llama/Sampler/Typical","path":"Llama/Sampler/Typical.html","kind":"class","full_name":"Llama::Sampler::Typical","name":"Typical","abstract":false,"superclass":{"html_id":"llama/Llama/Sampler/Base","kind":"class","full_name":"Llama::Sampler::Base","name":"Base"},"ancestors":[{"html_id":"llama/Llama/Sampler/Base","kind":"class","full_name":"Llama::Sampler::Base","name":"Base"},{"html_id":"llama/Reference","kind":"class","full_name":"Reference","name":"Reference"},{"html_id":"llama/Object","kind":"class","full_name":"Object","name":"Object"}],"locations":[{"filename":"src/llama/sampler/typical.cr","line_number":15,"url":"https://github.com/kojix2/llama.cr/blob/038f4ed10509841d5e14fe7055fbc1a21dac6a15/src/llama/sampler/typical.cr#L15"}],"repository_name":"llama","program":false,"enum":false,"alias":false,"const":false,"namespace":{"html_id":"llama/Llama/Sampler","kind":"module","full_name":"Llama::Sampler","name":"Sampler"},"doc":"Typical sampler\n\nThe Typical sampler selects tokens based on their \"typicality\" (entropy).\nIt filters out tokens that are either too predictable or too surprising,\nleading to more natural and diverse text generation.\n\nBased on the paper: https://arxiv.org/abs/2202.00666\n\nExample:\n```\nsampler = Llama::Sampler::Typical.new(0.95, 1) # Keep tokens with typicality >= 0.95\n```","summary":"<p>Typical sampler</p>","constructors":[{"html_id":"new(p:Float32,min_keep:Int32=1)-class-method","name":"new","doc":"Creates a new Typical sampler\n\nParameters:\n- p: The typicality threshold (0.0 to 1.0)\n- min_keep: Minimum number of tokens to keep\n\nRaises:\n- Llama::Error if the sampler cannot be created","summary":"<p>Creates a new Typical sampler</p>","abstract":false,"args":[{"name":"p","external_name":"p","restriction":"Float32"},{"name":"min_keep","default_value":"1","external_name":"min_keep","restriction":"Int32"}],"args_string":"(p : Float32, min_keep : Int32 = 1)","args_html":"(p : Float32, min_keep : Int32 = <span class=\"n\">1</span>)","location":{"filename":"src/llama/sampler/typical.cr","line_number":24,"url":"https://github.com/kojix2/llama.cr/blob/038f4ed10509841d5e14fe7055fbc1a21dac6a15/src/llama/sampler/typical.cr#L24"},"def":{"name":"new","args":[{"name":"p","external_name":"p","restriction":"Float32"},{"name":"min_keep","default_value":"1","external_name":"min_keep","restriction":"Int32"}],"visibility":"Public","body":"_ = allocate\n_.initialize(p, min_keep)\nif _.responds_to?(:finalize)\n  ::GC.add_finalizer(_)\nend\n_\n"},"external_var":false}]},{"html_id":"llama/Llama/Sampler/Xtc","path":"Llama/Sampler/Xtc.html","kind":"class","full_name":"Llama::Sampler::Xtc","name":"Xtc","abstract":false,"superclass":{"html_id":"llama/Llama/Sampler/Base","kind":"class","full_name":"Llama::Sampler::Base","name":"Base"},"ancestors":[{"html_id":"llama/Llama/Sampler/Base","kind":"class","full_name":"Llama::Sampler::Base","name":"Base"},{"html_id":"llama/Reference","kind":"class","full_name":"Reference","name":"Reference"},{"html_id":"llama/Object","kind":"class","full_name":"Object","name":"Object"}],"locations":[{"filename":"src/llama/sampler/xtc.cr","line_number":12,"url":"https://github.com/kojix2/llama.cr/blob/038f4ed10509841d5e14fe7055fbc1a21dac6a15/src/llama/sampler/xtc.cr#L12"}],"repository_name":"llama","program":false,"enum":false,"alias":false,"const":false,"namespace":{"html_id":"llama/Llama/Sampler","kind":"module","full_name":"Llama::Sampler","name":"Sampler"},"doc":"XTC sampler\n\nThe XTC sampler combines aspects of several sampling methods for improved\ntext generation quality. It was introduced in the Text Generation WebUI project.\n\nExample:\n```\nsampler = Llama::Sampler::Xtc.new(0.3, 0.8, 1, 42)\n```","summary":"<p>XTC sampler</p>","constructors":[{"html_id":"new(p:Float32,t:Float32,min_keep:Int32,seed:UInt32=LibLlama::LLAMA_DEFAULT_SEED)-class-method","name":"new","doc":"Creates a new XTC sampler\n\nParameters:\n- p: Probability threshold\n- t: Temperature value\n- min_keep: Minimum number of tokens to keep\n- seed: Random seed for sampling\n\nRaises:\n- Llama::Error if the sampler cannot be created","summary":"<p>Creates a new XTC sampler</p>","abstract":false,"args":[{"name":"p","external_name":"p","restriction":"Float32"},{"name":"t","external_name":"t","restriction":"Float32"},{"name":"min_keep","external_name":"min_keep","restriction":"Int32"},{"name":"seed","default_value":"LibLlama::LLAMA_DEFAULT_SEED","external_name":"seed","restriction":"UInt32"}],"args_string":"(p : Float32, t : Float32, min_keep : Int32, seed : UInt32 = LibLlama::LLAMA_DEFAULT_SEED)","args_html":"(p : Float32, t : Float32, min_keep : Int32, seed : UInt32 = <span class=\"t\">LibLlama</span><span class=\"t\">::</span><span class=\"t\">LLAMA_DEFAULT_SEED</span>)","location":{"filename":"src/llama/sampler/xtc.cr","line_number":23,"url":"https://github.com/kojix2/llama.cr/blob/038f4ed10509841d5e14fe7055fbc1a21dac6a15/src/llama/sampler/xtc.cr#L23"},"def":{"name":"new","args":[{"name":"p","external_name":"p","restriction":"Float32"},{"name":"t","external_name":"t","restriction":"Float32"},{"name":"min_keep","external_name":"min_keep","restriction":"Int32"},{"name":"seed","default_value":"LibLlama::LLAMA_DEFAULT_SEED","external_name":"seed","restriction":"UInt32"}],"visibility":"Public","body":"_ = allocate\n_.initialize(p, t, min_keep, seed)\nif _.responds_to?(:finalize)\n  ::GC.add_finalizer(_)\nend\n_\n"},"external_var":false}]}]},{"html_id":"llama/Llama/State","path":"Llama/State.html","kind":"class","full_name":"Llama::State","name":"State","abstract":false,"superclass":{"html_id":"llama/Reference","kind":"class","full_name":"Reference","name":"Reference"},"ancestors":[{"html_id":"llama/Reference","kind":"class","full_name":"Reference","name":"Reference"},{"html_id":"llama/Object","kind":"class","full_name":"Object","name":"Object"}],"locations":[{"filename":"src/llama/state.cr","line_number":6,"url":"https://github.com/kojix2/llama.cr/blob/038f4ed10509841d5e14fe7055fbc1a21dac6a15/src/llama/state.cr#L6"},{"filename":"src/llama/state/error.cr","line_number":4,"url":"https://github.com/kojix2/llama.cr/blob/038f4ed10509841d5e14fe7055fbc1a21dac6a15/src/llama/state/error.cr#L4"}],"repository_name":"llama","program":false,"enum":false,"alias":false,"const":false,"namespace":{"html_id":"llama/Llama","kind":"module","full_name":"Llama","name":"Llama"},"doc":"Wrapper for state management functions in llama.cpp\nProvides methods for saving and loading model state","summary":"<p>Wrapper for state management functions in llama.cpp Provides methods for saving and loading model state</p>","constructors":[{"html_id":"new(ctx:Context)-class-method","name":"new","doc":"Creates a new State instance\n\nParameters:\n- ctx: The Context to manage state for\n\nTo avoid circular references, we store the context pointer rather than the context object\n\nRaises:\n- Llama::State::Error if the context pointer is null","summary":"<p>Creates a new State instance</p>","abstract":false,"args":[{"name":"ctx","external_name":"ctx","restriction":"Context"}],"args_string":"(ctx : Context)","args_html":"(ctx : <a href=\"../Llama/Context.html\">Context</a>)","location":{"filename":"src/llama/state.cr","line_number":16,"url":"https://github.com/kojix2/llama.cr/blob/038f4ed10509841d5e14fe7055fbc1a21dac6a15/src/llama/state.cr#L16"},"def":{"name":"new","args":[{"name":"ctx","external_name":"ctx","restriction":"Context"}],"visibility":"Public","body":"_ = allocate\n_.initialize(ctx)\nif _.responds_to?(:finalize)\n  ::GC.add_finalizer(_)\nend\n_\n"},"external_var":false}],"instance_methods":[{"html_id":"finalize-instance-method","name":"finalize","doc":"Frees the resources associated with this state","summary":"<p>Frees the resources associated with this state</p>","abstract":false,"location":{"filename":"src/llama/state.cr","line_number":525,"url":"https://github.com/kojix2/llama.cr/blob/038f4ed10509841d5e14fe7055fbc1a21dac6a15/src/llama/state.cr#L525"},"def":{"name":"finalize","visibility":"Public","body":"@ctx_ptr = Pointer(LibLlama::LlamaContext).null"},"external_var":false},{"html_id":"get_data:Bytes-instance-method","name":"get_data","doc":"Gets the current state data\n\nReturns:\n- A Bytes object containing the state data\n\nRaises:\n- Llama::State::Error if the operation fails","summary":"<p>Gets the current state data</p>","abstract":false,"location":{"filename":"src/llama/state.cr","line_number":83,"url":"https://github.com/kojix2/llama.cr/blob/038f4ed10509841d5e14fe7055fbc1a21dac6a15/src/llama/state.cr#L83"},"def":{"name":"get_data","return_type":"Bytes","visibility":"Public","body":"begin\n  state_size = size\n\n\n  buffer = Bytes.new(state_size)\n\n\n  bytes_copied = LibLlama.llama_state_get_data(ctx_ptr, buffer.to_unsafe, state_size)\n\n  if bytes_copied == 0\n    error_msg = Llama.format_error(\"Failed to get state data\", -8, \"no bytes were copied\")\n    raise(State::Error.new(error_msg))\n  end\n\n\n  buffer[0, bytes_copied]\nrescue ex : State::Error\n  raise(ex)\nrescue ex\n  error_msg = Llama.format_error(\"Failed to get state data\", -8, ex.message)\n  raise(State::Error.new(error_msg))\nend"},"external_var":false},{"html_id":"load_file(path:String,max_tokens:Int32=1024):Array(Int32)-instance-method","name":"load_file","doc":"Loads state from a file\n\nParameters:\n- path: Path to the session file\n- max_tokens: Maximum number of tokens to load (default: 1024)\n\nReturns:\n- An array of tokens loaded from the file\n\nRaises:\n- ArgumentError if path is empty or max_tokens is not positive\n- Llama::State::Error if the file cannot be loaded","summary":"<p>Loads state from a file</p>","abstract":false,"args":[{"name":"path","external_name":"path","restriction":"String"},{"name":"max_tokens","default_value":"1024","external_name":"max_tokens","restriction":"Int32"}],"args_string":"(path : String, max_tokens : Int32 = 1024) : Array(Int32)","args_html":"(path : String, max_tokens : Int32 = <span class=\"n\">1024</span>) : Array(Int32)","location":{"filename":"src/llama/state.cr","line_number":170,"url":"https://github.com/kojix2/llama.cr/blob/038f4ed10509841d5e14fe7055fbc1a21dac6a15/src/llama/state.cr#L170"},"def":{"name":"load_file","args":[{"name":"path","external_name":"path","restriction":"String"},{"name":"max_tokens","default_value":"1024","external_name":"max_tokens","restriction":"Int32"}],"return_type":"Array(Int32)","visibility":"Public","body":"if path.empty?\n  raise(ArgumentError.new(\"Path cannot be empty\"))\nend\n\nif max_tokens <= 0\n  raise(ArgumentError.new(\"max_tokens must be positive\"))\nend\n\nbegin\n  tokens = Pointer(LibLlama::LlamaToken).malloc(max_tokens)\n  token_count = Pointer(LibC::SizeT).malloc(1)\n  token_count.value = 0\n\n\n  success = LibLlama.llama_state_load_file(ctx_ptr, path, tokens, max_tokens, token_count)\n\n  if success\n  else\n    error_msg = Llama.format_error(\"Failed to load state from file\", -8, \"path: #{path}\")\n    raise(State::Error.new(error_msg))\n  end\n\n\n  result = Array(Int32).new(token_count.value)\n  token_count.value.times do |i|\n    result << tokens[i]\n  end\n\n  result\nrescue ex : State::Error | ArgumentError\n  raise(ex)\nrescue ex\n  error_msg = Llama.format_error(\"Failed to load state from file\", -8, \"path: #{path}, error: #{ex.message}\")\n  raise(State::Error.new(error_msg))\nend\n"},"external_var":false},{"html_id":"save_file(path:String,tokens:Array(Int32)):Bool-instance-method","name":"save_file","doc":"Saves state to a file\n\nParameters:\n- path: Path to save the session file\n- tokens: Array of tokens to save with the state\n\nReturns:\n- true if successful, false otherwise\n\nRaises:\n- ArgumentError if path is empty\n- Llama::State::Error if the operation fails","summary":"<p>Saves state to a file</p>","abstract":false,"args":[{"name":"path","external_name":"path","restriction":"String"},{"name":"tokens","external_name":"tokens","restriction":"Array(Int32)"}],"args_string":"(path : String, tokens : Array(Int32)) : Bool","args_html":"(path : String, tokens : Array(Int32)) : Bool","location":{"filename":"src/llama/state.cr","line_number":234,"url":"https://github.com/kojix2/llama.cr/blob/038f4ed10509841d5e14fe7055fbc1a21dac6a15/src/llama/state.cr#L234"},"def":{"name":"save_file","args":[{"name":"path","external_name":"path","restriction":"String"},{"name":"tokens","external_name":"tokens","restriction":"Array(Int32)"}],"return_type":"Bool","visibility":"Public","body":"if path.empty?\n  raise(ArgumentError.new(\"Path cannot be empty\"))\nend\n\nbegin\n  tokens_ptr = tokens.to_unsafe\n\n\n  result = LibLlama.llama_state_save_file(ctx_ptr, path, tokens_ptr, tokens.size)\n\n  if result\n  else\n    error_msg = Llama.format_error(\"Failed to save state to file\", -8, \"path: #{path}\")\n    raise(State::Error.new(error_msg))\n  end\n\n  result\nrescue ex : State::Error | ArgumentError\n  raise(ex)\nrescue ex\n  error_msg = Llama.format_error(\"Failed to save state to file\", -8, \"path: #{path}, error: #{ex.message}\")\n  raise(State::Error.new(error_msg))\nend\n"},"external_var":false},{"html_id":"seq_get_data(seq_id:Int32):Bytes-instance-method","name":"seq_get_data","doc":"Gets the state data for a specific sequence\n\nParameters:\n- seq_id: The sequence ID\n\nReturns:\n- A Bytes object containing the sequence state data\n\nRaises:\n- Llama::State::Error if the operation fails","summary":"<p>Gets the state data for a specific sequence</p>","abstract":false,"args":[{"name":"seq_id","external_name":"seq_id","restriction":"Int32"}],"args_string":"(seq_id : Int32) : Bytes","args_html":"(seq_id : Int32) : Bytes","location":{"filename":"src/llama/state.cr","line_number":319,"url":"https://github.com/kojix2/llama.cr/blob/038f4ed10509841d5e14fe7055fbc1a21dac6a15/src/llama/state.cr#L319"},"def":{"name":"seq_get_data","args":[{"name":"seq_id","external_name":"seq_id","restriction":"Int32"}],"return_type":"Bytes","visibility":"Public","body":"begin\n  state_size = seq_size(seq_id)\n\n\n  buffer = Bytes.new(state_size)\n\n\n  bytes_copied = LibLlama.llama_state_seq_get_data(ctx_ptr, buffer.to_unsafe, state_size, seq_id)\n\n  if bytes_copied == 0\n    error_msg = Llama.format_error(\"Failed to get sequence state data\", -8, \"seq_id: #{seq_id}, no bytes were copied\")\n    raise(State::Error.new(error_msg))\n  end\n\n\n  buffer[0, bytes_copied]\nrescue ex : State::Error\n  raise(ex)\nrescue ex\n  error_msg = Llama.format_error(\"Failed to get sequence state data\", -8, \"seq_id: #{seq_id}, error: #{ex.message}\")\n  raise(State::Error.new(error_msg))\nend"},"external_var":false},{"html_id":"seq_load_file(path:String,dest_seq_id:Int32,max_tokens:Int32=1024):Array(Int32)-instance-method","name":"seq_load_file","doc":"Loads a sequence's state from a file\n\nParameters:\n- path: Path to the sequence file\n- dest_seq_id: The destination sequence ID\n- max_tokens: Maximum number of tokens to load (default: 1024)\n\nReturns:\n- An array of tokens loaded from the file\n\nRaises:\n- ArgumentError if path is empty or max_tokens is not positive\n- Llama::State::Error if the file cannot be loaded","summary":"<p>Loads a sequence's state from a file</p>","abstract":false,"args":[{"name":"path","external_name":"path","restriction":"String"},{"name":"dest_seq_id","external_name":"dest_seq_id","restriction":"Int32"},{"name":"max_tokens","default_value":"1024","external_name":"max_tokens","restriction":"Int32"}],"args_string":"(path : String, dest_seq_id : Int32, max_tokens : Int32 = 1024) : Array(Int32)","args_html":"(path : String, dest_seq_id : Int32, max_tokens : Int32 = <span class=\"n\">1024</span>) : Array(Int32)","location":{"filename":"src/llama/state.cr","line_number":471,"url":"https://github.com/kojix2/llama.cr/blob/038f4ed10509841d5e14fe7055fbc1a21dac6a15/src/llama/state.cr#L471"},"def":{"name":"seq_load_file","args":[{"name":"path","external_name":"path","restriction":"String"},{"name":"dest_seq_id","external_name":"dest_seq_id","restriction":"Int32"},{"name":"max_tokens","default_value":"1024","external_name":"max_tokens","restriction":"Int32"}],"return_type":"Array(Int32)","visibility":"Public","body":"if path.empty?\n  raise(ArgumentError.new(\"Path cannot be empty\"))\nend\n\nif max_tokens <= 0\n  raise(ArgumentError.new(\"max_tokens must be positive\"))\nend\n\nbegin\n  tokens = Pointer(LibLlama::LlamaToken).malloc(max_tokens)\n  token_count = Pointer(LibC::SizeT).malloc(1)\n  token_count.value = 0\n\n\n  bytes_read = LibLlama.llama_state_seq_load_file(ctx_ptr, path, dest_seq_id, tokens, max_tokens, token_count)\n\n  if bytes_read == 0\n    error_msg = Llama.format_error(\"Failed to load sequence state from file\", -8, \"path: #{path}, dest_seq_id: #{dest_seq_id}\")\n    raise(State::Error.new(error_msg))\n  end\n\n\n  result = Array(Int32).new(token_count.value)\n  token_count.value.times do |i|\n    result << tokens[i]\n  end\n\n  result\nrescue ex : State::Error | ArgumentError\n  raise(ex)\nrescue ex\n  error_msg = Llama.format_error(\"Failed to load sequence state from file\", -8, \"path: #{path}, dest_seq_id: #{dest_seq_id}, error: #{ex.message}\")\n  raise(State::Error.new(error_msg))\nend\n"},"external_var":false},{"html_id":"seq_save_file(path:String,seq_id:Int32,tokens:Array(Int32)):LibC::SizeT-instance-method","name":"seq_save_file","doc":"Saves a sequence's state to a file\n\nParameters:\n- path: Path to save the sequence file\n- seq_id: The sequence ID to save\n- tokens: Array of tokens to save with the state\n\nReturns:\n- The number of bytes written, or 0 if failed\n\nRaises:\n- ArgumentError if path is empty\n- Llama::State::Error if the operation fails","summary":"<p>Saves a sequence's state to a file</p>","abstract":false,"args":[{"name":"path","external_name":"path","restriction":"String"},{"name":"seq_id","external_name":"seq_id","restriction":"Int32"},{"name":"tokens","external_name":"tokens","restriction":"Array(Int32)"}],"args_string":"(path : String, seq_id : Int32, tokens : Array(Int32)) : LibC::SizeT","args_html":"(path : String, seq_id : Int32, tokens : Array(Int32)) : LibC::SizeT","location":{"filename":"src/llama/state.cr","line_number":418,"url":"https://github.com/kojix2/llama.cr/blob/038f4ed10509841d5e14fe7055fbc1a21dac6a15/src/llama/state.cr#L418"},"def":{"name":"seq_save_file","args":[{"name":"path","external_name":"path","restriction":"String"},{"name":"seq_id","external_name":"seq_id","restriction":"Int32"},{"name":"tokens","external_name":"tokens","restriction":"Array(Int32)"}],"return_type":"LibC::SizeT","visibility":"Public","body":"if path.empty?\n  raise(ArgumentError.new(\"Path cannot be empty\"))\nend\n\nbegin\n  tokens_ptr = tokens.to_unsafe\n\n\n  result = LibLlama.llama_state_seq_save_file(ctx_ptr, path, seq_id, tokens_ptr, tokens.size)\n\n  if result == 0\n    error_msg = Llama.format_error(\"Failed to save sequence state to file\", -8, \"path: #{path}, seq_id: #{seq_id}\")\n    raise(State::Error.new(error_msg))\n  end\n\n  result\nrescue ex : State::Error | ArgumentError\n  raise(ex)\nrescue ex\n  error_msg = Llama.format_error(\"Failed to save sequence state to file\", -8, \"path: #{path}, seq_id: #{seq_id}, error: #{ex.message}\")\n  raise(State::Error.new(error_msg))\nend\n"},"external_var":false},{"html_id":"seq_set_data(data:Bytes,dest_seq_id:Int32):LibC::SizeT-instance-method","name":"seq_set_data","doc":"Sets the state for a specific sequence from data\n\nParameters:\n- data: The state data to set\n- dest_seq_id: The destination sequence ID\n\nReturns:\n- The number of bytes read, or 0 if failed\n\nRaises:\n- ArgumentError if data is empty\n- Llama::State::Error if the operation fails","summary":"<p>Sets the state for a specific sequence from data</p>","abstract":false,"args":[{"name":"data","external_name":"data","restriction":"Bytes"},{"name":"dest_seq_id","external_name":"dest_seq_id","restriction":"Int32"}],"args_string":"(data : Bytes, dest_seq_id : Int32) : LibC::SizeT","args_html":"(data : Bytes, dest_seq_id : Int32) : LibC::SizeT","location":{"filename":"src/llama/state.cr","line_number":370,"url":"https://github.com/kojix2/llama.cr/blob/038f4ed10509841d5e14fe7055fbc1a21dac6a15/src/llama/state.cr#L370"},"def":{"name":"seq_set_data","args":[{"name":"data","external_name":"data","restriction":"Bytes"},{"name":"dest_seq_id","external_name":"dest_seq_id","restriction":"Int32"}],"return_type":"LibC::SizeT","visibility":"Public","body":"if data.empty?\n  raise(ArgumentError.new(\"State data cannot be empty\"))\nend\n\nbegin\n  result = LibLlama.llama_state_seq_set_data(ctx_ptr, data.to_unsafe, data.size, dest_seq_id)\n\n  if result == 0\n    error_msg = Llama.format_error(\"Failed to set sequence state data\", -8, \"dest_seq_id: #{dest_seq_id}, no bytes were read\")\n    raise(State::Error.new(error_msg))\n  end\n\n  result\nrescue ex : State::Error | ArgumentError\n  raise(ex)\nrescue ex\n  error_msg = Llama.format_error(\"Failed to set sequence state data\", -8, \"dest_seq_id: #{dest_seq_id}, error: #{ex.message}\")\n  raise(State::Error.new(error_msg))\nend\n"},"external_var":false},{"html_id":"seq_size(seq_id:Int32):LibC::SizeT-instance-method","name":"seq_size","doc":"Gets the size needed to store a specific sequence's state\n\nParameters:\n- seq_id: The sequence ID\n\nReturns:\n- The size in bytes\n\nRaises:\n- Llama::State::Error if the operation fails","summary":"<p>Gets the size needed to store a specific sequence's state</p>","abstract":false,"args":[{"name":"seq_id","external_name":"seq_id","restriction":"Int32"}],"args_string":"(seq_id : Int32) : LibC::SizeT","args_html":"(seq_id : Int32) : LibC::SizeT","location":{"filename":"src/llama/state.cr","line_number":283,"url":"https://github.com/kojix2/llama.cr/blob/038f4ed10509841d5e14fe7055fbc1a21dac6a15/src/llama/state.cr#L283"},"def":{"name":"seq_size","args":[{"name":"seq_id","external_name":"seq_id","restriction":"Int32"}],"return_type":"LibC::SizeT","visibility":"Public","body":"begin\n  result = LibLlama.llama_state_seq_get_size(ctx_ptr, seq_id)\n\n  if result == 0\n    error_msg = Llama.format_error(\"Failed to get sequence state size\", -8, \"seq_id: #{seq_id}, returned size is 0\")\n    raise(State::Error.new(error_msg))\n  end\n\n  result\nrescue ex : State::Error\n  raise(ex)\nrescue ex\n  error_msg = Llama.format_error(\"Failed to get sequence state size\", -8, \"seq_id: #{seq_id}, error: #{ex.message}\")\n  raise(State::Error.new(error_msg))\nend"},"external_var":false},{"html_id":"set_data(data:Bytes):LibC::SizeT-instance-method","name":"set_data","doc":"Sets the state from data\n\nParameters:\n- data: The state data to set\n\nReturns:\n- The number of bytes read\n\nRaises:\n- ArgumentError if data is empty\n- Llama::State::Error if the operation fails","summary":"<p>Sets the state from data</p>","abstract":false,"args":[{"name":"data","external_name":"data","restriction":"Bytes"}],"args_string":"(data : Bytes) : LibC::SizeT","args_html":"(data : Bytes) : LibC::SizeT","location":{"filename":"src/llama/state.cr","line_number":128,"url":"https://github.com/kojix2/llama.cr/blob/038f4ed10509841d5e14fe7055fbc1a21dac6a15/src/llama/state.cr#L128"},"def":{"name":"set_data","args":[{"name":"data","external_name":"data","restriction":"Bytes"}],"return_type":"LibC::SizeT","visibility":"Public","body":"if data.empty?\n  raise(ArgumentError.new(\"State data cannot be empty\"))\nend\n\nbegin\n  result = LibLlama.llama_state_set_data(ctx_ptr, data.to_unsafe, data.size)\n\n  if result == 0\n    error_msg = Llama.format_error(\"Failed to set state data\", -8, \"no bytes were read\")\n    raise(State::Error.new(error_msg))\n  end\n\n  result\nrescue ex : State::Error | ArgumentError\n  raise(ex)\nrescue ex\n  error_msg = Llama.format_error(\"Failed to set state data\", -8, ex.message)\n  raise(State::Error.new(error_msg))\nend\n"},"external_var":false},{"html_id":"size:LibC::SizeT-instance-method","name":"size","doc":"Returns the size in bytes needed to store the current state\n\nReturns:\n- The size in bytes\n\nRaises:\n- Llama::State::Error if the operation fails","summary":"<p>Returns the size in bytes needed to store the current state</p>","abstract":false,"location":{"filename":"src/llama/state.cr","line_number":50,"url":"https://github.com/kojix2/llama.cr/blob/038f4ed10509841d5e14fe7055fbc1a21dac6a15/src/llama/state.cr#L50"},"def":{"name":"size","return_type":"LibC::SizeT","visibility":"Public","body":"begin\n  result = LibLlama.llama_state_get_size(ctx_ptr)\n\n  if result == 0\n    error_msg = Llama.format_error(\"Failed to get state size\", -8, \"returned size is 0\")\n    raise(State::Error.new(error_msg))\n  end\n\n  result\nrescue ex : State::Error\n  raise(ex)\nrescue ex\n  error_msg = Llama.format_error(\"Failed to get state size\", -8, ex.message)\n  raise(State::Error.new(error_msg))\nend"},"external_var":false}],"types":[{"html_id":"llama/Llama/State/Error","path":"Llama/State/Error.html","kind":"class","full_name":"Llama::State::Error","name":"Error","abstract":false,"superclass":{"html_id":"llama/Llama/Error","kind":"class","full_name":"Llama::Error","name":"Error"},"ancestors":[{"html_id":"llama/Llama/Error","kind":"class","full_name":"Llama::Error","name":"Error"},{"html_id":"llama/Exception","kind":"class","full_name":"Exception","name":"Exception"},{"html_id":"llama/Reference","kind":"class","full_name":"Reference","name":"Reference"},{"html_id":"llama/Object","kind":"class","full_name":"Object","name":"Object"}],"locations":[{"filename":"src/llama/state/error.cr","line_number":5,"url":"https://github.com/kojix2/llama.cr/blob/038f4ed10509841d5e14fe7055fbc1a21dac6a15/src/llama/state/error.cr#L5"}],"repository_name":"llama","program":false,"enum":false,"alias":false,"const":false,"namespace":{"html_id":"llama/Llama/State","kind":"class","full_name":"Llama::State","name":"State"}}]},{"html_id":"llama/Llama/Vocab","path":"Llama/Vocab.html","kind":"class","full_name":"Llama::Vocab","name":"Vocab","abstract":false,"superclass":{"html_id":"llama/Reference","kind":"class","full_name":"Reference","name":"Reference"},"ancestors":[{"html_id":"llama/Reference","kind":"class","full_name":"Reference","name":"Reference"},{"html_id":"llama/Object","kind":"class","full_name":"Object","name":"Object"}],"locations":[{"filename":"src/llama/vocab.cr","line_number":3,"url":"https://github.com/kojix2/llama.cr/blob/038f4ed10509841d5e14fe7055fbc1a21dac6a15/src/llama/vocab.cr#L3"}],"repository_name":"llama","program":false,"enum":false,"alias":false,"const":false,"namespace":{"html_id":"llama/Llama","kind":"module","full_name":"Llama","name":"Llama"},"doc":"Wrapper for the llama_vocab structure","summary":"<p>Wrapper for the llama_vocab structure</p>","constructors":[{"html_id":"new(handle:Pointer(LibLlama::LlamaVocab))-class-method","name":"new","doc":"Creates a new Vocab instance from a raw pointer\n\nNote: This constructor is intended for internal use.\nUsers should obtain Vocab instances through Model#vocab.","summary":"<p>Creates a new Vocab instance from a raw pointer</p>","abstract":false,"args":[{"name":"handle","external_name":"handle","restriction":"::Pointer(LibLlama::LlamaVocab)"}],"args_string":"(handle : Pointer(LibLlama::LlamaVocab))","args_html":"(handle : Pointer(LibLlama::LlamaVocab))","location":{"filename":"src/llama/vocab.cr","line_number":8,"url":"https://github.com/kojix2/llama.cr/blob/038f4ed10509841d5e14fe7055fbc1a21dac6a15/src/llama/vocab.cr#L8"},"def":{"name":"new","args":[{"name":"handle","external_name":"handle","restriction":"::Pointer(LibLlama::LlamaVocab)"}],"visibility":"Public","body":"_ = allocate\n_.initialize(handle)\nif _.responds_to?(:finalize)\n  ::GC.add_finalizer(_)\nend\n_\n"},"external_var":false}],"instance_methods":[{"html_id":"add_bos?:Bool-instance-method","name":"add_bos?","doc":"Returns whether the model adds BOS token by default","summary":"<p>Returns whether the model adds BOS token by default</p>","abstract":false,"location":{"filename":"src/llama/vocab.cr","line_number":109,"url":"https://github.com/kojix2/llama.cr/blob/038f4ed10509841d5e14fe7055fbc1a21dac6a15/src/llama/vocab.cr#L109"},"def":{"name":"add_bos?","return_type":"Bool","visibility":"Public","body":"LibLlama.llama_vocab_get_add_bos(@handle)"},"external_var":false},{"html_id":"add_eos?:Bool-instance-method","name":"add_eos?","doc":"Returns whether the model adds EOS token by default","summary":"<p>Returns whether the model adds EOS token by default</p>","abstract":false,"location":{"filename":"src/llama/vocab.cr","line_number":114,"url":"https://github.com/kojix2/llama.cr/blob/038f4ed10509841d5e14fe7055fbc1a21dac6a15/src/llama/vocab.cr#L114"},"def":{"name":"add_eos?","return_type":"Bool","visibility":"Public","body":"LibLlama.llama_vocab_get_add_eos(@handle)"},"external_var":false},{"html_id":"bos:Int32-instance-method","name":"bos","doc":"Returns the beginning-of-sentence token ID","summary":"<p>Returns the beginning-of-sentence token ID</p>","abstract":false,"location":{"filename":"src/llama/vocab.cr","line_number":121,"url":"https://github.com/kojix2/llama.cr/blob/038f4ed10509841d5e14fe7055fbc1a21dac6a15/src/llama/vocab.cr#L121"},"def":{"name":"bos","return_type":"Int32","visibility":"Public","body":"LibLlama.llama_vocab_bos(@handle)"},"external_var":false},{"html_id":"eos:Int32-instance-method","name":"eos","doc":"Returns the end-of-sentence token ID","summary":"<p>Returns the end-of-sentence token ID</p>","abstract":false,"location":{"filename":"src/llama/vocab.cr","line_number":126,"url":"https://github.com/kojix2/llama.cr/blob/038f4ed10509841d5e14fe7055fbc1a21dac6a15/src/llama/vocab.cr#L126"},"def":{"name":"eos","return_type":"Int32","visibility":"Public","body":"LibLlama.llama_vocab_eos(@handle)"},"external_var":false},{"html_id":"eot:Int32-instance-method","name":"eot","doc":"Returns the end-of-turn token ID","summary":"<p>Returns the end-of-turn token ID</p>","abstract":false,"location":{"filename":"src/llama/vocab.cr","line_number":131,"url":"https://github.com/kojix2/llama.cr/blob/038f4ed10509841d5e14fe7055fbc1a21dac6a15/src/llama/vocab.cr#L131"},"def":{"name":"eot","return_type":"Int32","visibility":"Public","body":"LibLlama.llama_vocab_eot(@handle)"},"external_var":false},{"html_id":"format_token(token:Int32,show_id:Bool=true,show_text:Bool=true):String-instance-method","name":"format_token","doc":"Format a token for display\n\nParameters:\n- token: The token to format\n- show_id: Whether to show the token ID\n- show_text: Whether to show the token text\n\nReturns:\n- A formatted string representation of the token","summary":"<p>Format a token for display</p>","abstract":false,"args":[{"name":"token","external_name":"token","restriction":"Int32"},{"name":"show_id","default_value":"true","external_name":"show_id","restriction":"Bool"},{"name":"show_text","default_value":"true","external_name":"show_text","restriction":"Bool"}],"args_string":"(token : Int32, show_id : Bool = true, show_text : Bool = true) : String","args_html":"(token : Int32, show_id : Bool = <span class=\"n\">true</span>, show_text : Bool = <span class=\"n\">true</span>) : String","location":{"filename":"src/llama/vocab.cr","line_number":54,"url":"https://github.com/kojix2/llama.cr/blob/038f4ed10509841d5e14fe7055fbc1a21dac6a15/src/llama/vocab.cr#L54"},"def":{"name":"format_token","args":[{"name":"token","external_name":"token","restriction":"Int32"},{"name":"show_id","default_value":"true","external_name":"show_id","restriction":"Bool"},{"name":"show_text","default_value":"true","external_name":"show_text","restriction":"Bool"}],"return_type":"String","visibility":"Public","body":"if show_id && show_text\n  piece = token_to_piece(token, 0, true)\n  \"%6d -> '#{piece}'\" % token\nelse\n  if show_id\n    token.to_s\n  else\n    if show_text\n      token_to_piece(token, 0, true)\n    else\n      \"\"\n    end\n  end\nend"},"external_var":false},{"html_id":"is_control(token:Int32):Bool-instance-method","name":"is_control","doc":"Checks if a token is a control token","summary":"<p>Checks if a token is a control token</p>","abstract":false,"args":[{"name":"token","external_name":"token","restriction":"Int32"}],"args_string":"(token : Int32) : Bool","args_html":"(token : Int32) : Bool","location":{"filename":"src/llama/vocab.cr","line_number":151,"url":"https://github.com/kojix2/llama.cr/blob/038f4ed10509841d5e14fe7055fbc1a21dac6a15/src/llama/vocab.cr#L151"},"def":{"name":"is_control","args":[{"name":"token","external_name":"token","restriction":"Int32"}],"return_type":"Bool","visibility":"Public","body":"LibLlama.llama_vocab_is_control(@handle, token)"},"external_var":false},{"html_id":"is_eog(token:Int32):Bool-instance-method","name":"is_eog","doc":"Checks if a token is an end-of-generation token","summary":"<p>Checks if a token is an end-of-generation token</p>","abstract":false,"args":[{"name":"token","external_name":"token","restriction":"Int32"}],"args_string":"(token : Int32) : Bool","args_html":"(token : Int32) : Bool","location":{"filename":"src/llama/vocab.cr","line_number":146,"url":"https://github.com/kojix2/llama.cr/blob/038f4ed10509841d5e14fe7055fbc1a21dac6a15/src/llama/vocab.cr#L146"},"def":{"name":"is_eog","args":[{"name":"token","external_name":"token","restriction":"Int32"}],"return_type":"Bool","visibility":"Public","body":"LibLlama.llama_vocab_is_eog(@handle, token)"},"external_var":false},{"html_id":"n_tokens:Int32-instance-method","name":"n_tokens","doc":"Returns the number of tokens in the vocabulary","summary":"<p>Returns the number of tokens in the vocabulary</p>","abstract":false,"location":{"filename":"src/llama/vocab.cr","line_number":12,"url":"https://github.com/kojix2/llama.cr/blob/038f4ed10509841d5e14fe7055fbc1a21dac6a15/src/llama/vocab.cr#L12"},"def":{"name":"n_tokens","return_type":"Int32","visibility":"Public","body":"LibLlama.llama_vocab_n_tokens(@handle)"},"external_var":false},{"html_id":"nl:Int32-instance-method","name":"nl","doc":"Returns the newline token ID","summary":"<p>Returns the newline token ID</p>","abstract":false,"location":{"filename":"src/llama/vocab.cr","line_number":136,"url":"https://github.com/kojix2/llama.cr/blob/038f4ed10509841d5e14fe7055fbc1a21dac6a15/src/llama/vocab.cr#L136"},"def":{"name":"nl","return_type":"Int32","visibility":"Public","body":"LibLlama.llama_vocab_nl(@handle)"},"external_var":false},{"html_id":"pad:Int32-instance-method","name":"pad","doc":"Returns the padding token ID","summary":"<p>Returns the padding token ID</p>","abstract":false,"location":{"filename":"src/llama/vocab.cr","line_number":141,"url":"https://github.com/kojix2/llama.cr/blob/038f4ed10509841d5e14fe7055fbc1a21dac6a15/src/llama/vocab.cr#L141"},"def":{"name":"pad","return_type":"Int32","visibility":"Public","body":"LibLlama.llama_vocab_pad(@handle)"},"external_var":false},{"html_id":"to_unsafe:Pointer(Llama::LibLlama::LlamaVocab)-instance-method","name":"to_unsafe","doc":"Returns the raw pointer to the underlying llama_vocab structure","summary":"<p>Returns the raw pointer to the underlying llama_vocab structure</p>","abstract":false,"location":{"filename":"src/llama/vocab.cr","line_number":156,"url":"https://github.com/kojix2/llama.cr/blob/038f4ed10509841d5e14fe7055fbc1a21dac6a15/src/llama/vocab.cr#L156"},"def":{"name":"to_unsafe","visibility":"Public","body":"@handle"},"external_var":false},{"html_id":"token_to_piece(token:Int32,lstrip:Int32=0,special:Bool=false):String-instance-method","name":"token_to_piece","doc":"Converts a token to a piece of text\nThis is similar to token_to_text but provides more control over the output format\n\nParameters:\n- token: The token to convert\n- lstrip: Whether to strip leading spaces (0 = no, 1 = yes)\n- special: Whether to render special tokens\n\nReturns:\n- The text representation of the token","summary":"<p>Converts a token to a piece of text This is similar to token_to_text but provides more control over the output format</p>","abstract":false,"args":[{"name":"token","external_name":"token","restriction":"Int32"},{"name":"lstrip","default_value":"0","external_name":"lstrip","restriction":"Int32"},{"name":"special","default_value":"false","external_name":"special","restriction":"Bool"}],"args_string":"(token : Int32, lstrip : Int32 = 0, special : Bool = false) : String","args_html":"(token : Int32, lstrip : Int32 = <span class=\"n\">0</span>, special : Bool = <span class=\"n\">false</span>) : String","location":{"filename":"src/llama/vocab.cr","line_number":32,"url":"https://github.com/kojix2/llama.cr/blob/038f4ed10509841d5e14fe7055fbc1a21dac6a15/src/llama/vocab.cr#L32"},"def":{"name":"token_to_piece","args":[{"name":"token","external_name":"token","restriction":"Int32"},{"name":"lstrip","default_value":"0","external_name":"lstrip","restriction":"Int32"},{"name":"special","default_value":"false","external_name":"special","restriction":"Bool"}],"return_type":"String","visibility":"Public","body":"buf_size = 128\nbuf = Pointer(LibC::Char).malloc(buf_size)\n\nn = LibLlama.llama_token_to_piece(@handle, token, buf, buf_size, lstrip, special)\n\nif n < 0\n  raise(Error.new(\"Failed to convert token to piece\"))\nend\n\nString.new(buf, n)\n"},"external_var":false},{"html_id":"token_to_text(token:Int32):String-instance-method","name":"token_to_text","doc":"Returns the text representation of a token","summary":"<p>Returns the text representation of a token</p>","abstract":false,"args":[{"name":"token","external_name":"token","restriction":"Int32"}],"args_string":"(token : Int32) : String","args_html":"(token : Int32) : String","location":{"filename":"src/llama/vocab.cr","line_number":17,"url":"https://github.com/kojix2/llama.cr/blob/038f4ed10509841d5e14fe7055fbc1a21dac6a15/src/llama/vocab.cr#L17"},"def":{"name":"token_to_text","args":[{"name":"token","external_name":"token","restriction":"Int32"}],"return_type":"String","visibility":"Public","body":"ptr = LibLlama.llama_vocab_get_text(@handle, token)\nString.new(ptr)\n"},"external_var":false},{"html_id":"tokenize(text:String,add_special:Bool=true,parse_special:Bool=true):Array(Int32)-instance-method","name":"tokenize","doc":"Tokenizes a string into an array of token IDs","summary":"<p>Tokenizes a string into an array of token IDs</p>","abstract":false,"args":[{"name":"text","external_name":"text","restriction":"String"},{"name":"add_special","default_value":"true","external_name":"add_special","restriction":"Bool"},{"name":"parse_special","default_value":"true","external_name":"parse_special","restriction":"Bool"}],"args_string":"(text : String, add_special : Bool = true, parse_special : Bool = true) : Array(Int32)","args_html":"(text : String, add_special : Bool = <span class=\"n\">true</span>, parse_special : Bool = <span class=\"n\">true</span>) : Array(Int32)","location":{"filename":"src/llama/vocab.cr","line_number":68,"url":"https://github.com/kojix2/llama.cr/blob/038f4ed10509841d5e14fe7055fbc1a21dac6a15/src/llama/vocab.cr#L68"},"def":{"name":"tokenize","args":[{"name":"text","external_name":"text","restriction":"String"},{"name":"add_special","default_value":"true","external_name":"add_special","restriction":"Bool"},{"name":"parse_special","default_value":"true","external_name":"parse_special","restriction":"Bool"}],"return_type":"Array(Int32)","visibility":"Public","body":"max_tokens = text.size * 2\ntokens = Pointer(LibLlama::LlamaToken).malloc(max_tokens)\n\nn_tokens = LibLlama.llama_tokenize(@handle, text, text.bytesize, tokens, max_tokens, add_special, parse_special)\n\nif n_tokens < 0\n  max_tokens = -n_tokens\n  tokens = Pointer(LibLlama::LlamaToken).malloc(max_tokens)\n\n  n_tokens = LibLlama.llama_tokenize(@handle, text, text.bytesize, tokens, max_tokens, add_special, parse_special)\nend\n\nif n_tokens < 0\n  raise(Error.new(\"Failed to tokenize text\"))\nend\n\nresult = Array(Int32).new(n_tokens)\nn_tokens.times do |i|\n  result << tokens[i]\nend\n\nresult\n"},"external_var":false}]}]}]}})