crystal_doc_search_index_callback({"repository_name":"llama","body":"# llama.cr\n\n[![test](https://github.com/kojix2/llama.cr/actions/workflows/test.yml/badge.svg)](https://github.com/kojix2/llama.cr/actions/workflows/test.yml)\n[![docs](https://img.shields.io/badge/docs-latest-blue.svg)](https://kojix2.github.io/llama.cr)\n\nCrystal bindings for [llama.cpp](https://github.com/ggml-org/llama.cpp), a C/C++ implementation of LLaMA, Falcon, GPT-2, and other large language models.\n\n## Features\n\n- Low-level bindings to the llama.cpp C API\n- High-level Crystal wrapper classes for easy usage\n- Memory management for C resources\n- Simple text generation interface\n\n## Installation\n\n### Prerequisites\n\nBefore using this shard, you need to have llama.cpp compiled and installed on your system:\n\n1. Clone and build llama.cpp using CMake:\n```bash\ngit clone https://github.com/ggml-org/llama.cpp.git\ncd llama.cpp\nmkdir build && cd build\ncmake ..\ncmake --build . --config Release\n```\n\n2. Install the library to your system:\n```bash\n# On Linux\nsudo cmake --install .\nsudo ldconfig\n\n# On macOS\nsudo cmake --install .\n```\n\n3. Alternatively, you can specify the library location without installing:\n```bash\n# Set the library path at compile time\ncrystal build examples/text_generation.cr --link-flags=\"-L/path/to/llama.cpp/build/bin\"\n\n# Or set the library path at runtime\nLD_LIBRARY_PATH=/path/to/llama.cpp/build/bin ./text_generation /path/to/model.gguf \"Your prompt here\"\n```\n\n### Obtaining GGUF Model Files\n\nYou'll need a model file in GGUF format to use with this library. Here are some options:\n\n1. **Download pre-converted models from Hugging Face**:\n\n   - [TheBloke's GGUF models](https://huggingface.co/TheBloke) - Large collection of models converted to GGUF format\n   - Popular models include:\n     - [Llama 3 8B Instruct](https://huggingface.co/TheBloke/Llama-3-8B-Instruct-GGUF)\n     - [Mistral 7B Instruct v0.2](https://huggingface.co/TheBloke/Mistral-7B-Instruct-v0.2-GGUF)\n     - [TinyLlama 1.1B](https://huggingface.co/TheBloke/TinyLlama-1.1B-Chat-v1.0-GGUF) (good for testing)\n\n2. **Convert models yourself using llama.cpp**:\n\n```bash\n# Example: Converting a Hugging Face model to GGUF\npython3 llama.cpp/convert.py /path/to/model --outfile model.gguf\n```\n\nFor testing, smaller quantized models (1-3B parameters) with Q4_K_M quantization are recommended.\n\n### Adding to Your Project\n\n1. Add the dependency to your `shard.yml`:\n\n```yaml\ndependencies:\n  llama:\n    github: kojix2/llama.cr\n```\n\n2. Run `shards install`\n\n## Usage\n\n```crystal\nrequire \"llama\"\n\n# Load a model\nmodel = Llama::Model.new(\"/path/to/model.gguf\")\n\n# Create a context\ncontext = model.context\n\n# Generate text\nresponse = context.generate(\"Once upon a time\", max_tokens: 100, temperature: 0.8)\nputs response\n\n# Or use the convenience method\nresponse = Llama.generate(\"/path/to/model.gguf\", \"Once upon a time\")\nputs response\n```\n\n## Examples\n\nThe `examples` directory contains sample code demonstrating how to use the library:\n\n### Text Generation\n\nA simple example showing how to generate text from a prompt:\n\n```bash\ncrystal build examples/text_generation.cr --link-flags=\"-L/path/to/llama.cpp/build/bin\"\nLD_LIBRARY_PATH=/path/to/llama.cpp/build/bin ./text_generation /path/to/model.gguf \"Once upon a time\"\n```\n\nOptional parameters:\n\n- Third argument: Maximum number of tokens to generate (default: 128)\n- Fourth argument: Temperature (default: 0.8)\n\n### Tokenization\n\nAn example demonstrating tokenization and vocabulary features:\n\n```bash\ncrystal build examples/tokenization.cr --link-flags=\"-L/path/to/llama.cpp/build/bin\"\nLD_LIBRARY_PATH=/path/to/llama.cpp/build/bin ./tokenization /path/to/model.gguf \"Hello, world!\"\n```\n\nThis example shows:\n\n- How to tokenize text into token IDs\n- How to convert token IDs back to text\n- How to access special tokens in the vocabulary\n\n## API Documentation\n\n### Llama::Model\n\nThe `Model` class represents a loaded LLaMA model.\n\n```crystal\n# Load a model\nmodel = Llama::Model.new(\"/path/to/model.gguf\")\n\n# Get model information\nputs model.n_params  # Number of parameters\nputs model.n_embd    # Embedding size\nputs model.n_layer   # Number of layers\nputs model.n_head    # Number of attention heads\n```\n\n### Llama::Context\n\nThe `Context` class handles the inference state for a model.\n\n```crystal\n# Create a context\ncontext = model.context\n\n# Generate text\nresponse = context.generate(\"Hello, I am a\", max_tokens: 50, temperature: 0.7)\n```\n\n### Llama::Vocab\n\nThe `Vocab` class provides access to the model's vocabulary.\n\n```crystal\n# Get the vocabulary\nvocab = model.vocab\n\n# Tokenize text\ntokens = vocab.tokenize(\"Hello, world!\")\n\n# Convert tokens back to text\ntext = tokens.map { |token| vocab.token_to_text(token) }.join\n```\n\n## Development\n\nSee [DEVELOPMENT.md](DEVELOPMENT.md) for development guidelines.\n\n## Contributing\n\n1. Fork it (<https://github.com/kojix2/llama.cr/fork>)\n2. Create your feature branch (`git checkout -b my-new-feature`)\n3. Commit your changes (`git commit -am 'Add some feature'`)\n4. Push to the branch (`git push origin my-new-feature`)\n5. Create a new Pull Request\n\n## License\n\nThis project is available under the MIT License. See the LICENSE file for more info.\n","program":{"html_id":"llama/toplevel","path":"toplevel.html","kind":"module","full_name":"Top Level Namespace","name":"Top Level Namespace","abstract":false,"locations":[],"repository_name":"llama","program":true,"enum":false,"alias":false,"const":false,"types":[{"html_id":"llama/Llama","path":"Llama.html","kind":"module","full_name":"Llama","name":"Llama","abstract":false,"locations":[{"filename":"src/llama.cr","line_number":15,"url":"https://github.com/kojix2/llama.cr/blob/114c9694ea5c084fb272373b545d065f36513c0c/src/llama.cr#L15"},{"filename":"src/llama/batch.cr","line_number":1,"url":"https://github.com/kojix2/llama.cr/blob/114c9694ea5c084fb272373b545d065f36513c0c/src/llama/batch.cr#L1"},{"filename":"src/llama/chat.cr","line_number":1,"url":"https://github.com/kojix2/llama.cr/blob/114c9694ea5c084fb272373b545d065f36513c0c/src/llama/chat.cr#L1"},{"filename":"src/llama/context.cr","line_number":1,"url":"https://github.com/kojix2/llama.cr/blob/114c9694ea5c084fb272373b545d065f36513c0c/src/llama/context.cr#L1"},{"filename":"src/llama/kv_cache.cr","line_number":1,"url":"https://github.com/kojix2/llama.cr/blob/114c9694ea5c084fb272373b545d065f36513c0c/src/llama/kv_cache.cr#L1"},{"filename":"src/llama/lib_llama.cr","line_number":1,"url":"https://github.com/kojix2/llama.cr/blob/114c9694ea5c084fb272373b545d065f36513c0c/src/llama/lib_llama.cr#L1"},{"filename":"src/llama/model.cr","line_number":1,"url":"https://github.com/kojix2/llama.cr/blob/114c9694ea5c084fb272373b545d065f36513c0c/src/llama/model.cr#L1"},{"filename":"src/llama/sampler.cr","line_number":1,"url":"https://github.com/kojix2/llama.cr/blob/114c9694ea5c084fb272373b545d065f36513c0c/src/llama/sampler.cr#L1"},{"filename":"src/llama/state.cr","line_number":1,"url":"https://github.com/kojix2/llama.cr/blob/114c9694ea5c084fb272373b545d065f36513c0c/src/llama/state.cr#L1"},{"filename":"src/llama/vocab.cr","line_number":1,"url":"https://github.com/kojix2/llama.cr/blob/114c9694ea5c084fb272373b545d065f36513c0c/src/llama/vocab.cr#L1"}],"repository_name":"llama","program":false,"enum":false,"alias":false,"const":false,"constants":[{"id":"VERSION","name":"VERSION","value":"\"0.1.0\""}],"class_methods":[{"html_id":"apply_chat_template(template:String|Nil,messages:Array(ChatMessage),add_assistant:Bool=true):String-class-method","name":"apply_chat_template","doc":"Applies a chat template to a list of messages\n\nParameters:\n- template: The template string (nil to use model's default)\n- messages: Array of chat messages\n- add_assistant: Whether to end with an assistant message prefix\n\nReturns:\n- The formatted prompt string\n\nRaises:\n- Llama::Error if template application fails","summary":"<p>Applies a chat template to a list of messages</p>","abstract":false,"args":[{"name":"template","external_name":"template","restriction":"String | ::Nil"},{"name":"messages","external_name":"messages","restriction":"Array(ChatMessage)"},{"name":"add_assistant","default_value":"true","external_name":"add_assistant","restriction":"Bool"}],"args_string":"(template : String | Nil, messages : Array(ChatMessage), add_assistant : Bool = true) : String","args_html":"(template : String | Nil, messages : Array(<a href=\"Llama/ChatMessage.html\">ChatMessage</a>), add_assistant : Bool = <span class=\"n\">true</span>) : String","location":{"filename":"src/llama/chat.cr","line_number":39,"url":"https://github.com/kojix2/llama.cr/blob/114c9694ea5c084fb272373b545d065f36513c0c/src/llama/chat.cr#L39"},"def":{"name":"apply_chat_template","args":[{"name":"template","external_name":"template","restriction":"String | ::Nil"},{"name":"messages","external_name":"messages","restriction":"Array(ChatMessage)"},{"name":"add_assistant","default_value":"true","external_name":"add_assistant","restriction":"Bool"}],"return_type":"String","visibility":"Public","body":"c_messages = messages.map(&.to_unsafe)\nestimated_size = messages.sum do |msg|\n  msg.content.size + msg.role.size\nend * 2\nbuffer = Pointer(LibC::Char).malloc(estimated_size)\nresult = LibLlama.llama_chat_apply_template((template || \"\").to_unsafe, c_messages.to_unsafe, messages.size, add_assistant, buffer, estimated_size)\nif result > estimated_size\n  buffer = Pointer(LibC::Char).malloc(result)\n  result = LibLlama.llama_chat_apply_template((template || \"\").to_unsafe, c_messages.to_unsafe, messages.size, add_assistant, buffer, result)\nend\nif result < 0\n  raise(Error.new(\"Failed to apply chat template\"))\nend\nString.new(buffer, result)\n"}},{"html_id":"builtin_chat_templates:Array(String)-class-method","name":"builtin_chat_templates","doc":"Gets the list of built-in chat templates\n\nReturns:\n- Array of template names","summary":"<p>Gets the list of built-in chat templates</p>","abstract":false,"location":{"filename":"src/llama/chat.cr","line_number":85,"url":"https://github.com/kojix2/llama.cr/blob/114c9694ea5c084fb272373b545d065f36513c0c/src/llama/chat.cr#L85"},"def":{"name":"builtin_chat_templates","return_type":"Array(String)","visibility":"Public","body":"output = Pointer(::Pointer(LibC::Char)).malloc(100)\ncount = LibLlama.llama_chat_builtin_templates(output, 100)\nresult = [] of String\ncount.times do |i|\n  result << (String.new(output[i]))\nend\nresult\n"}},{"html_id":"generate(model_path:String,prompt:String,max_tokens:Int32=128,temperature:Float32=0.8):String-class-method","name":"generate","doc":"A simple example of text generation\n\nParameters:\n- model_path: Path to the model file (.gguf format)\n- prompt: The input prompt\n- max_tokens: Maximum number of tokens to generate (must be positive)\n- temperature: Sampling temperature (0.0 = greedy, 1.0 = more random)\n\nReturns:\n- The generated text\n\nRaises:\n- ArgumentError if parameters are invalid\n- Llama::Error if model loading or text generation fails","summary":"<p>A simple example of text generation</p>","abstract":false,"args":[{"name":"model_path","external_name":"model_path","restriction":"String"},{"name":"prompt","external_name":"prompt","restriction":"String"},{"name":"max_tokens","default_value":"128","external_name":"max_tokens","restriction":"Int32"},{"name":"temperature","default_value":"0.8","external_name":"temperature","restriction":"Float32"}],"args_string":"(model_path : String, prompt : String, max_tokens : Int32 = 128, temperature : Float32 = 0.8) : String","args_html":"(model_path : String, prompt : String, max_tokens : Int32 = <span class=\"n\">128</span>, temperature : Float32 = <span class=\"n\">0.8</span>) : String","location":{"filename":"src/llama.cr","line_number":37,"url":"https://github.com/kojix2/llama.cr/blob/114c9694ea5c084fb272373b545d065f36513c0c/src/llama.cr#L37"},"def":{"name":"generate","args":[{"name":"model_path","external_name":"model_path","restriction":"String"},{"name":"prompt","external_name":"prompt","restriction":"String"},{"name":"max_tokens","default_value":"128","external_name":"max_tokens","restriction":"Int32"},{"name":"temperature","default_value":"0.8","external_name":"temperature","restriction":"Float32"}],"return_type":"String","visibility":"Public","body":"if max_tokens <= 0\n  raise(ArgumentError.new(\"max_tokens must be positive\"))\nend\nif temperature < 0\n  raise(ArgumentError.new(\"temperature must be non-negative\"))\nend\nmodel = Model.new(model_path)\ncontext = model.context\ncontext.generate(prompt, max_tokens, temperature)\n"}},{"html_id":"system_info:String-class-method","name":"system_info","doc":"Returns the llama.cpp system information","summary":"<p>Returns the llama.cpp system information</p>","abstract":false,"location":{"filename":"src/llama.cr","line_number":19,"url":"https://github.com/kojix2/llama.cr/blob/114c9694ea5c084fb272373b545d065f36513c0c/src/llama.cr#L19"},"def":{"name":"system_info","return_type":"String","visibility":"Public","body":"String.new(LibLlama.llama_print_system_info)"}}],"types":[{"html_id":"llama/Llama/Batch","path":"Llama/Batch.html","kind":"class","full_name":"Llama::Batch","name":"Batch","abstract":false,"superclass":{"html_id":"llama/Reference","kind":"class","full_name":"Reference","name":"Reference"},"ancestors":[{"html_id":"llama/Reference","kind":"class","full_name":"Reference","name":"Reference"},{"html_id":"llama/Object","kind":"class","full_name":"Object","name":"Object"}],"locations":[{"filename":"src/llama/batch.cr","line_number":4,"url":"https://github.com/kojix2/llama.cr/blob/114c9694ea5c084fb272373b545d065f36513c0c/src/llama/batch.cr#L4"}],"repository_name":"llama","program":false,"enum":false,"alias":false,"const":false,"namespace":{"html_id":"llama/Llama","kind":"module","full_name":"Llama","name":"Llama"},"doc":"Wrapper for the llama_batch structure\nProvides methods for managing batches of tokens for efficient processing","summary":"<p>Wrapper for the llama_batch structure Provides methods for managing batches of tokens for efficient processing</p>","constructors":[{"html_id":"get_one(tokens:Array(Int32)):Batch-class-method","name":"get_one","doc":"Creates a new Batch for a single sequence of tokens\n\nParameters:\n- tokens: Array of token IDs\n\nReturns:\n- A new Batch instance\n\nRaises:\n- Llama::Error if the batch cannot be created","summary":"<p>Creates a new Batch for a single sequence of tokens</p>","abstract":false,"args":[{"name":"tokens","external_name":"tokens","restriction":"Array(Int32)"}],"args_string":"(tokens : Array(Int32)) : Batch","args_html":"(tokens : Array(Int32)) : <a href=\"../Llama/Batch.html\">Batch</a>","location":{"filename":"src/llama/batch.cr","line_number":61,"url":"https://github.com/kojix2/llama.cr/blob/114c9694ea5c084fb272373b545d065f36513c0c/src/llama/batch.cr#L61"},"def":{"name":"get_one","args":[{"name":"tokens","external_name":"tokens","restriction":"Array(Int32)"}],"return_type":"Batch","visibility":"Public","body":"tokens_ptr = tokens.to_unsafe\nhandle = LibLlama.llama_batch_get_one(tokens_ptr, tokens.size)\nBatch.new(handle)\n"}},{"html_id":"new(n_tokens:Int32,embd:Int32=0,n_seq_max:Int32=1)-class-method","name":"new","doc":"Creates a new Batch instance with the specified parameters\n\nParameters:\n- n_tokens: Maximum number of tokens this batch can hold\n- embd: Embedding dimension (0 for token-based batch, >0 for embedding-based batch)\n- n_seq_max: Maximum number of sequences per token\n\nRaises:\n- Llama::Error if the batch cannot be created","summary":"<p>Creates a new Batch instance with the specified parameters</p>","abstract":false,"args":[{"name":"n_tokens","external_name":"n_tokens","restriction":"Int32"},{"name":"embd","default_value":"0","external_name":"embd","restriction":"Int32"},{"name":"n_seq_max","default_value":"1","external_name":"n_seq_max","restriction":"Int32"}],"args_string":"(n_tokens : Int32, embd : Int32 = 0, n_seq_max : Int32 = 1)","args_html":"(n_tokens : Int32, embd : Int32 = <span class=\"n\">0</span>, n_seq_max : Int32 = <span class=\"n\">1</span>)","location":{"filename":"src/llama/batch.cr","line_number":14,"url":"https://github.com/kojix2/llama.cr/blob/114c9694ea5c084fb272373b545d065f36513c0c/src/llama/batch.cr#L14"},"def":{"name":"new","args":[{"name":"n_tokens","external_name":"n_tokens","restriction":"Int32"},{"name":"embd","default_value":"0","external_name":"embd","restriction":"Int32"},{"name":"n_seq_max","default_value":"1","external_name":"n_seq_max","restriction":"Int32"}],"visibility":"Public","body":"_ = allocate\n_.initialize(n_tokens, embd, n_seq_max)\nif _.responds_to?(:finalize)\n  ::GC.add_finalizer(_)\nend\n_\n"}},{"html_id":"new(handle:LibLlama::LlamaBatch,owned:Bool=false)-class-method","name":"new","doc":"Creates a new Batch instance from a raw llama_batch structure\n\nNote: This constructor is intended for internal use.\nThe batch created this way is not owned by this wrapper and will not be freed.","summary":"<p>Creates a new Batch instance from a raw llama_batch structure</p>","abstract":false,"args":[{"name":"handle","external_name":"handle","restriction":"LibLlama::LlamaBatch"},{"name":"owned","default_value":"false","external_name":"owned","restriction":"::Bool"}],"args_string":"(handle : LibLlama::LlamaBatch, owned : Bool = false)","args_html":"(handle : LibLlama::LlamaBatch, owned : Bool = <span class=\"n\">false</span>)","location":{"filename":"src/llama/batch.cr","line_number":48,"url":"https://github.com/kojix2/llama.cr/blob/114c9694ea5c084fb272373b545d065f36513c0c/src/llama/batch.cr#L48"},"def":{"name":"new","args":[{"name":"handle","external_name":"handle","restriction":"LibLlama::LlamaBatch"},{"name":"owned","default_value":"false","external_name":"owned","restriction":"::Bool"}],"visibility":"Public","body":"_ = allocate\n_.initialize(handle, owned)\nif _.responds_to?(:finalize)\n  ::GC.add_finalizer(_)\nend\n_\n"}}],"instance_methods":[{"html_id":"finalize-instance-method","name":"finalize","doc":"Frees the resources associated with this batch","summary":"<p>Frees the resources associated with this batch</p>","abstract":false,"location":{"filename":"src/llama/batch.cr","line_number":259,"url":"https://github.com/kojix2/llama.cr/blob/114c9694ea5c084fb272373b545d065f36513c0c/src/llama/batch.cr#L259"},"def":{"name":"finalize","visibility":"Public","body":"if @owned && !@handle.n_tokens.zero?\n  LibLlama.llama_batch_free(@handle)\nend"}},{"html_id":"n_tokens:Int32-instance-method","name":"n_tokens","doc":"Returns the number of tokens in this batch","summary":"<p>Returns the number of tokens in this batch</p>","abstract":false,"location":{"filename":"src/llama/batch.cr","line_number":68,"url":"https://github.com/kojix2/llama.cr/blob/114c9694ea5c084fb272373b545d065f36513c0c/src/llama/batch.cr#L68"},"def":{"name":"n_tokens","return_type":"Int32","visibility":"Public","body":"@handle.n_tokens"}},{"html_id":"set_embedding(i:Int32,embedding:Array(Float32),pos:Int32|Nil=nil,seq_id:Int32|Nil=nil,logits:Bool|Nil=nil)-instance-method","name":"set_embedding","doc":"Sets an embedding at the specified index\n\nParameters:\n- i: Index in the batch\n- embedding: Array of embedding values\n- pos: Position of the embedding in the sequence (nil for auto-position)\n- seq_id: Sequence ID (nil for default sequence 0)\n- logits: Whether to compute logits for this embedding (nil for default)\n\nRaises:\n- IndexError if the index is out of bounds\n- ArgumentError if the batch is not embedding-based","summary":"<p>Sets an embedding at the specified index</p>","abstract":false,"args":[{"name":"i","external_name":"i","restriction":"Int32"},{"name":"embedding","external_name":"embedding","restriction":"Array(Float32)"},{"name":"pos","default_value":"nil","external_name":"pos","restriction":"Int32 | ::Nil"},{"name":"seq_id","default_value":"nil","external_name":"seq_id","restriction":"Int32 | ::Nil"},{"name":"logits","default_value":"nil","external_name":"logits","restriction":"Bool | ::Nil"}],"args_string":"(i : Int32, embedding : Array(Float32), pos : Int32 | Nil = nil, seq_id : Int32 | Nil = nil, logits : Bool | Nil = nil)","args_html":"(i : Int32, embedding : Array(Float32), pos : Int32 | Nil = <span class=\"n\">nil</span>, seq_id : Int32 | Nil = <span class=\"n\">nil</span>, logits : Bool | Nil = <span class=\"n\">nil</span>)","location":{"filename":"src/llama/batch.cr","line_number":171,"url":"https://github.com/kojix2/llama.cr/blob/114c9694ea5c084fb272373b545d065f36513c0c/src/llama/batch.cr#L171"},"def":{"name":"set_embedding","args":[{"name":"i","external_name":"i","restriction":"Int32"},{"name":"embedding","external_name":"embedding","restriction":"Array(Float32)"},{"name":"pos","default_value":"nil","external_name":"pos","restriction":"Int32 | ::Nil"},{"name":"seq_id","default_value":"nil","external_name":"seq_id","restriction":"Int32 | ::Nil"},{"name":"logits","default_value":"nil","external_name":"logits","restriction":"Bool | ::Nil"}],"visibility":"Public","body":"if i < 0 || i >= @handle.n_tokens\n  raise(IndexError.new(\"Index out of bounds\"))\nend\nif @handle.embd.null?\n  embd_size = embedding.size\n  @handle.embd = Pointer(Float32).malloc(@handle.n_tokens * embd_size)\nelse\n  embd_size = embedding.size\nend\nembd_size.times do |j|\n  @handle.embd[(i * embd_size) + j] = embedding[j]\nend\nif @handle.pos.null?\n  @handle.pos = Pointer(LibLlama::LlamaPos).malloc(@handle.n_tokens)\nend\nif pos\n  @handle.pos[i] = pos\nelse\n  @handle.pos[i] = i\nend\nif @handle.n_seq_id.null?\n  @handle.n_seq_id = Pointer(Int32).malloc(@handle.n_tokens)\n  @handle.n_tokens.times do |j|\n    @handle.n_seq_id[j] = 0\n  end\nend\nif @handle.seq_id.null?\n  @handle.seq_id = Pointer(Pointer(Int32)).malloc(@handle.n_tokens)\n  @handle.n_tokens.times do |j|\n    @handle.seq_id[j] = Pointer(Int32).null\n  end\nend\nif seq_id\n  if @handle.seq_id[i].null?\n    @handle.seq_id[i] = Pointer(Int32).malloc(1)\n  end\n  @handle.n_seq_id[i] = 1\n  @handle.seq_id[i][0] = seq_id\nelse\n  if @handle.seq_id[i].null?\n    @handle.seq_id[i] = Pointer(Int32).malloc(1)\n  end\n  @handle.n_seq_id[i] = 1\n  @handle.seq_id[i][0] = 0\nend\nif logits\n  if @handle.logits.null?\n    @handle.logits = Pointer(Int8).malloc(@handle.n_tokens)\n    @handle.n_tokens.times do |j|\n      @handle.logits[j] = 0_i8\n    end\n  end\n  @handle.logits[i] = logits ? 1_i8 : 0_i8\nend\n"}},{"html_id":"set_token(i:Int32,token:Int32,pos:Int32|Nil=nil,seq_id:Int32|Nil=nil,logits:Bool|Nil=nil)-instance-method","name":"set_token","doc":"Sets a token at the specified index\n\nParameters:\n- i: Index in the batch\n- token: Token ID to set\n- pos: Position of the token in the sequence (nil for auto-position)\n- seq_id: Sequence ID (nil for default sequence 0)\n- logits: Whether to compute logits for this token (nil for default)\n\nRaises:\n- IndexError if the index is out of bounds","summary":"<p>Sets a token at the specified index</p>","abstract":false,"args":[{"name":"i","external_name":"i","restriction":"Int32"},{"name":"token","external_name":"token","restriction":"Int32"},{"name":"pos","default_value":"nil","external_name":"pos","restriction":"Int32 | ::Nil"},{"name":"seq_id","default_value":"nil","external_name":"seq_id","restriction":"Int32 | ::Nil"},{"name":"logits","default_value":"nil","external_name":"logits","restriction":"Bool | ::Nil"}],"args_string":"(i : Int32, token : Int32, pos : Int32 | Nil = nil, seq_id : Int32 | Nil = nil, logits : Bool | Nil = nil)","args_html":"(i : Int32, token : Int32, pos : Int32 | Nil = <span class=\"n\">nil</span>, seq_id : Int32 | Nil = <span class=\"n\">nil</span>, logits : Bool | Nil = <span class=\"n\">nil</span>)","location":{"filename":"src/llama/batch.cr","line_number":83,"url":"https://github.com/kojix2/llama.cr/blob/114c9694ea5c084fb272373b545d065f36513c0c/src/llama/batch.cr#L83"},"def":{"name":"set_token","args":[{"name":"i","external_name":"i","restriction":"Int32"},{"name":"token","external_name":"token","restriction":"Int32"},{"name":"pos","default_value":"nil","external_name":"pos","restriction":"Int32 | ::Nil"},{"name":"seq_id","default_value":"nil","external_name":"seq_id","restriction":"Int32 | ::Nil"},{"name":"logits","default_value":"nil","external_name":"logits","restriction":"Bool | ::Nil"}],"visibility":"Public","body":"if i < 0 || i >= @handle.n_tokens\n  raise(IndexError.new(\"Index out of bounds\"))\nend\nif @handle.token.null?\n  @handle.token = Pointer(LibLlama::LlamaToken).malloc(@handle.n_tokens)\nend\n@handle.token[i] = token\nif @handle.pos.null?\n  @handle.pos = Pointer(LibLlama::LlamaPos).malloc(@handle.n_tokens)\nend\nif pos\n  @handle.pos[i] = pos\nelse\n  @handle.pos[i] = i\nend\nif @handle.n_seq_id.null?\n  @handle.n_seq_id = Pointer(Int32).malloc(@handle.n_tokens)\n  @handle.n_tokens.times do |j|\n    @handle.n_seq_id[j] = 0\n  end\nend\nif @handle.seq_id.null?\n  @handle.seq_id = Pointer(Pointer(Int32)).malloc(@handle.n_tokens)\n  @handle.n_tokens.times do |j|\n    @handle.seq_id[j] = Pointer(Int32).null\n  end\nend\nif seq_id\n  if @handle.seq_id[i].null?\n    @handle.seq_id[i] = Pointer(Int32).malloc(1)\n  end\n  @handle.n_seq_id[i] = 1\n  @handle.seq_id[i][0] = seq_id\nelse\n  if @handle.seq_id[i].null?\n    @handle.seq_id[i] = Pointer(Int32).malloc(1)\n  end\n  @handle.n_seq_id[i] = 1\n  @handle.seq_id[i][0] = 0\nend\nif logits\n  if @handle.logits.null?\n    @handle.logits = Pointer(Int8).malloc(@handle.n_tokens)\n    @handle.n_tokens.times do |j|\n      @handle.logits[j] = 0_i8\n    end\n  end\n  @handle.logits[i] = logits ? 1_i8 : 0_i8\nend\n"}},{"html_id":"to_unsafe:Llama::LibLlama::LlamaBatch-instance-method","name":"to_unsafe","doc":"Returns the raw pointer to the underlying llama_batch structure","summary":"<p>Returns the raw pointer to the underlying llama_batch structure</p>","abstract":false,"location":{"filename":"src/llama/batch.cr","line_number":254,"url":"https://github.com/kojix2/llama.cr/blob/114c9694ea5c084fb272373b545d065f36513c0c/src/llama/batch.cr#L254"},"def":{"name":"to_unsafe","visibility":"Public","body":"@handle"}}]},{"html_id":"llama/Llama/ChatMessage","path":"Llama/ChatMessage.html","kind":"class","full_name":"Llama::ChatMessage","name":"ChatMessage","abstract":false,"superclass":{"html_id":"llama/Reference","kind":"class","full_name":"Reference","name":"Reference"},"ancestors":[{"html_id":"llama/Reference","kind":"class","full_name":"Reference","name":"Reference"},{"html_id":"llama/Object","kind":"class","full_name":"Object","name":"Object"}],"locations":[{"filename":"src/llama/chat.cr","line_number":3,"url":"https://github.com/kojix2/llama.cr/blob/114c9694ea5c084fb272373b545d065f36513c0c/src/llama/chat.cr#L3"}],"repository_name":"llama","program":false,"enum":false,"alias":false,"const":false,"namespace":{"html_id":"llama/Llama","kind":"module","full_name":"Llama","name":"Llama"},"doc":"Represents a message in a chat conversation","summary":"<p>Represents a message in a chat conversation</p>","constructors":[{"html_id":"new(role:String,content:String)-class-method","name":"new","doc":"Creates a new ChatMessage\n\nParameters:\n- role: The role of the message sender\n- content: The content of the message","summary":"<p>Creates a new ChatMessage</p>","abstract":false,"args":[{"name":"role","external_name":"role","restriction":"String"},{"name":"content","external_name":"content","restriction":"String"}],"args_string":"(role : String, content : String)","args_html":"(role : String, content : String)","location":{"filename":"src/llama/chat.cr","line_number":15,"url":"https://github.com/kojix2/llama.cr/blob/114c9694ea5c084fb272373b545d065f36513c0c/src/llama/chat.cr#L15"},"def":{"name":"new","args":[{"name":"role","external_name":"role","restriction":"String"},{"name":"content","external_name":"content","restriction":"String"}],"visibility":"Public","body":"_ = allocate\n_.initialize(role, content)\nif _.responds_to?(:finalize)\n  ::GC.add_finalizer(_)\nend\n_\n"}}],"instance_methods":[{"html_id":"content:String-instance-method","name":"content","doc":"The content of the message","summary":"<p>The content of the message</p>","abstract":false,"location":{"filename":"src/llama/chat.cr","line_number":8,"url":"https://github.com/kojix2/llama.cr/blob/114c9694ea5c084fb272373b545d065f36513c0c/src/llama/chat.cr#L8"},"def":{"name":"content","return_type":"String","visibility":"Public","body":"@content"}},{"html_id":"content=(content:String)-instance-method","name":"content=","doc":"The content of the message","summary":"<p>The content of the message</p>","abstract":false,"args":[{"name":"content","external_name":"content","restriction":"String"}],"args_string":"(content : String)","args_html":"(content : String)","location":{"filename":"src/llama/chat.cr","line_number":8,"url":"https://github.com/kojix2/llama.cr/blob/114c9694ea5c084fb272373b545d065f36513c0c/src/llama/chat.cr#L8"},"def":{"name":"content=","args":[{"name":"content","external_name":"content","restriction":"String"}],"visibility":"Public","body":"@content = content"}},{"html_id":"role:String-instance-method","name":"role","doc":"The role of the message sender (e.g., \"system\", \"user\", \"assistant\")","summary":"<p>The role of the message sender (e.g., &quot;system&quot;, &quot;user&quot;, &quot;assistant&quot;)</p>","abstract":false,"location":{"filename":"src/llama/chat.cr","line_number":5,"url":"https://github.com/kojix2/llama.cr/blob/114c9694ea5c084fb272373b545d065f36513c0c/src/llama/chat.cr#L5"},"def":{"name":"role","return_type":"String","visibility":"Public","body":"@role"}},{"html_id":"role=(role:String)-instance-method","name":"role=","doc":"The role of the message sender (e.g., \"system\", \"user\", \"assistant\")","summary":"<p>The role of the message sender (e.g., &quot;system&quot;, &quot;user&quot;, &quot;assistant&quot;)</p>","abstract":false,"args":[{"name":"role","external_name":"role","restriction":"String"}],"args_string":"(role : String)","args_html":"(role : String)","location":{"filename":"src/llama/chat.cr","line_number":5,"url":"https://github.com/kojix2/llama.cr/blob/114c9694ea5c084fb272373b545d065f36513c0c/src/llama/chat.cr#L5"},"def":{"name":"role=","args":[{"name":"role","external_name":"role","restriction":"String"}],"visibility":"Public","body":"@role = role"}},{"html_id":"to_unsafe:LibLlama::LlamaChatMessage-instance-method","name":"to_unsafe","doc":"Converts to the C structure","summary":"<p>Converts to the C structure</p>","abstract":false,"location":{"filename":"src/llama/chat.cr","line_number":19,"url":"https://github.com/kojix2/llama.cr/blob/114c9694ea5c084fb272373b545d065f36513c0c/src/llama/chat.cr#L19"},"def":{"name":"to_unsafe","return_type":"LibLlama::LlamaChatMessage","visibility":"Public","body":"msg = LibLlama::LlamaChatMessage.new\nmsg.role = @role.to_unsafe\nmsg.content = @content.to_unsafe\nmsg\n"}}]},{"html_id":"llama/Llama/Context","path":"Llama/Context.html","kind":"class","full_name":"Llama::Context","name":"Context","abstract":false,"superclass":{"html_id":"llama/Reference","kind":"class","full_name":"Reference","name":"Reference"},"ancestors":[{"html_id":"llama/Reference","kind":"class","full_name":"Reference","name":"Reference"},{"html_id":"llama/Object","kind":"class","full_name":"Object","name":"Object"}],"locations":[{"filename":"src/llama/context.cr","line_number":3,"url":"https://github.com/kojix2/llama.cr/blob/114c9694ea5c084fb272373b545d065f36513c0c/src/llama/context.cr#L3"}],"repository_name":"llama","program":false,"enum":false,"alias":false,"const":false,"namespace":{"html_id":"llama/Llama","kind":"module","full_name":"Llama","name":"Llama"},"doc":"Wrapper for the llama_context structure","summary":"<p>Wrapper for the llama_context structure</p>","constructors":[{"html_id":"new(model:Model,params=nil)-class-method","name":"new","doc":"Creates a new Context instance for a model\n\nParameters:\n- model: The Model to create a context for\n- params: Optional context parameters\n\nRaises:\n- Llama::Error if the context cannot be created","summary":"<p>Creates a new Context instance for a model</p>","abstract":false,"args":[{"name":"model","external_name":"model","restriction":"Model"},{"name":"params","default_value":"nil","external_name":"params","restriction":""}],"args_string":"(model : Model, params = nil)","args_html":"(model : <a href=\"../Llama/Model.html\">Model</a>, params = <span class=\"n\">nil</span>)","location":{"filename":"src/llama/context.cr","line_number":12,"url":"https://github.com/kojix2/llama.cr/blob/114c9694ea5c084fb272373b545d065f36513c0c/src/llama/context.cr#L12"},"def":{"name":"new","args":[{"name":"model","external_name":"model","restriction":"Model"},{"name":"params","default_value":"nil","external_name":"params","restriction":""}],"visibility":"Public","body":"_ = allocate\n_.initialize(model, params)\nif _.responds_to?(:finalize)\n  ::GC.add_finalizer(_)\nend\n_\n"}}],"instance_methods":[{"html_id":"chat(messages:Array(ChatMessage),max_tokens:Int32=128,temperature:Float32=0.8,template:String|Nil=nil):String-instance-method","name":"chat","doc":"Generates a response in a chat conversation\n\nParameters:\n- messages: Array of chat messages\n- max_tokens: Maximum number of tokens to generate\n- temperature: Sampling temperature\n- template: Optional chat template (nil to use model's default)\n\nReturns:\n- The generated response text\n\nRaises:\n- ArgumentError if parameters are invalid\n- Llama::Error if text generation fails","summary":"<p>Generates a response in a chat conversation</p>","abstract":false,"args":[{"name":"messages","external_name":"messages","restriction":"Array(ChatMessage)"},{"name":"max_tokens","default_value":"128","external_name":"max_tokens","restriction":"Int32"},{"name":"temperature","default_value":"0.8","external_name":"temperature","restriction":"Float32"},{"name":"template","default_value":"nil","external_name":"template","restriction":"String | ::Nil"}],"args_string":"(messages : Array(ChatMessage), max_tokens : Int32 = 128, temperature : Float32 = 0.8, template : String | Nil = nil) : String","args_html":"(messages : Array(<a href=\"../Llama/ChatMessage.html\">ChatMessage</a>), max_tokens : Int32 = <span class=\"n\">128</span>, temperature : Float32 = <span class=\"n\">0.8</span>, template : String | Nil = <span class=\"n\">nil</span>) : String","location":{"filename":"src/llama/context.cr","line_number":47,"url":"https://github.com/kojix2/llama.cr/blob/114c9694ea5c084fb272373b545d065f36513c0c/src/llama/context.cr#L47"},"def":{"name":"chat","args":[{"name":"messages","external_name":"messages","restriction":"Array(ChatMessage)"},{"name":"max_tokens","default_value":"128","external_name":"max_tokens","restriction":"Int32"},{"name":"temperature","default_value":"0.8","external_name":"temperature","restriction":"Float32"},{"name":"template","default_value":"nil","external_name":"template","restriction":"String | ::Nil"}],"return_type":"String","visibility":"Public","body":"prompt = Llama.apply_chat_template(template || @model.chat_template, messages, true)\ngenerate(prompt, max_tokens, temperature)\n"}},{"html_id":"decode(batch:LibLlama::LlamaBatch|Batch):Int32-instance-method","name":"decode","doc":"Processes a batch of tokens\n\nParameters:\n- batch: The batch to process (can be a LibLlama::LlamaBatch or a Batch instance)\n\nReturns:\n- 0 on success\n- 1 if no KV slot was found for the batch\n- < 0 on error\n\nRaises:\n- Llama::Error on error","summary":"<p>Processes a batch of tokens</p>","abstract":false,"args":[{"name":"batch","external_name":"batch","restriction":"LibLlama::LlamaBatch | Batch"}],"args_string":"(batch : LibLlama::LlamaBatch | Batch) : Int32","args_html":"(batch : LibLlama::LlamaBatch | <a href=\"../Llama/Batch.html\">Batch</a>) : Int32","location":{"filename":"src/llama/context.cr","line_number":158,"url":"https://github.com/kojix2/llama.cr/blob/114c9694ea5c084fb272373b545d065f36513c0c/src/llama/context.cr#L158"},"def":{"name":"decode","args":[{"name":"batch","external_name":"batch","restriction":"LibLlama::LlamaBatch | Batch"}],"return_type":"Int32","visibility":"Public","body":"batch_ptr = batch.is_a?(Batch) ? batch.to_unsafe : batch\nresult = LibLlama.llama_decode(@handle, batch_ptr)\nif result < 0\n  raise(Error.new(\"Failed to decode batch\"))\nend\nresult\n"}},{"html_id":"finalize-instance-method","name":"finalize","doc":"Frees the resources associated with this context","summary":"<p>Frees the resources associated with this context</p>","abstract":false,"location":{"filename":"src/llama/context.cr","line_number":329,"url":"https://github.com/kojix2/llama.cr/blob/114c9694ea5c084fb272373b545d065f36513c0c/src/llama/context.cr#L329"},"def":{"name":"finalize","visibility":"Public","body":"if @handle.null?\nelse\n  LibLlama.llama_free(@handle)\nend"}},{"html_id":"generate(prompt:String,max_tokens:Int32=128,temperature:Float32=0.8):String-instance-method","name":"generate","doc":"Generates text from a prompt\n\nParameters:\n- prompt: The input prompt\n- max_tokens: Maximum number of tokens to generate (must be positive)\n- temperature: Sampling temperature (0.0 = greedy, 1.0 = more random)\n\nReturns:\n- The generated text\n\nRaises:\n- ArgumentError if parameters are invalid","summary":"<p>Generates text from a prompt</p>","abstract":false,"args":[{"name":"prompt","external_name":"prompt","restriction":"String"},{"name":"max_tokens","default_value":"128","external_name":"max_tokens","restriction":"Int32"},{"name":"temperature","default_value":"0.8","external_name":"temperature","restriction":"Float32"}],"args_string":"(prompt : String, max_tokens : Int32 = 128, temperature : Float32 = 0.8) : String","args_html":"(prompt : String, max_tokens : Int32 = <span class=\"n\">128</span>, temperature : Float32 = <span class=\"n\">0.8</span>) : String","location":{"filename":"src/llama/context.cr","line_number":185,"url":"https://github.com/kojix2/llama.cr/blob/114c9694ea5c084fb272373b545d065f36513c0c/src/llama/context.cr#L185"},"def":{"name":"generate","args":[{"name":"prompt","external_name":"prompt","restriction":"String"},{"name":"max_tokens","default_value":"128","external_name":"max_tokens","restriction":"Int32"},{"name":"temperature","default_value":"0.8","external_name":"temperature","restriction":"Float32"}],"return_type":"String","visibility":"Public","body":"if max_tokens <= 0\n  raise(ArgumentError.new(\"max_tokens must be positive\"))\nend\nif temperature < 0\n  raise(ArgumentError.new(\"temperature must be non-negative\"))\nend\ngenerate_internal(prompt, max_tokens) do |logits|\n  sample_token(logits, temperature)\nend\n"}},{"html_id":"generate_with_sampler(prompt:String,sampler:SamplerChain,max_tokens:Int32=128):String-instance-method","name":"generate_with_sampler","doc":"Generates text using a sampler chain\n\nParameters:\n- prompt: The input prompt\n- sampler: The sampler chain to use\n- max_tokens: Maximum number of tokens to generate (must be positive)\n\nReturns:\n- The generated text\n\nRaises:\n- ArgumentError if parameters are invalid","summary":"<p>Generates text using a sampler chain</p>","abstract":false,"args":[{"name":"prompt","external_name":"prompt","restriction":"String"},{"name":"sampler","external_name":"sampler","restriction":"SamplerChain"},{"name":"max_tokens","default_value":"128","external_name":"max_tokens","restriction":"Int32"}],"args_string":"(prompt : String, sampler : SamplerChain, max_tokens : Int32 = 128) : String","args_html":"(prompt : String, sampler : <a href=\"../Llama/SamplerChain.html\">SamplerChain</a>, max_tokens : Int32 = <span class=\"n\">128</span>) : String","location":{"filename":"src/llama/context.cr","line_number":130,"url":"https://github.com/kojix2/llama.cr/blob/114c9694ea5c084fb272373b545d065f36513c0c/src/llama/context.cr#L130"},"def":{"name":"generate_with_sampler","args":[{"name":"prompt","external_name":"prompt","restriction":"String"},{"name":"sampler","external_name":"sampler","restriction":"SamplerChain"},{"name":"max_tokens","default_value":"128","external_name":"max_tokens","restriction":"Int32"}],"return_type":"String","visibility":"Public","body":"if max_tokens <= 0\n  raise(ArgumentError.new(\"max_tokens must be positive\"))\nend\ngenerate_internal(prompt, max_tokens) do |logits|\n  token = sampler.sample(self)\n  sampler.accept(token)\n  token\nend\n"}},{"html_id":"kv_cache:KvCache-instance-method","name":"kv_cache","doc":"Returns the KV cache for this context","summary":"<p>Returns the KV cache for this context</p>","abstract":false,"location":{"filename":"src/llama/context.cr","line_number":24,"url":"https://github.com/kojix2/llama.cr/blob/114c9694ea5c084fb272373b545d065f36513c0c/src/llama/context.cr#L24"},"def":{"name":"kv_cache","return_type":"KvCache","visibility":"Public","body":"@kv_cache.not_nil!"}},{"html_id":"logits:Pointer(Float32)-instance-method","name":"logits","doc":"Gets the logits for the last token\n\nReturns:\n- A pointer to the logits array","summary":"<p>Gets the logits for the last token</p>","abstract":false,"location":{"filename":"src/llama/context.cr","line_number":169,"url":"https://github.com/kojix2/llama.cr/blob/114c9694ea5c084fb272373b545d065f36513c0c/src/llama/context.cr#L169"},"def":{"name":"logits","return_type":"Pointer(Float32)","visibility":"Public","body":"LibLlama.llama_get_logits(@handle)"}},{"html_id":"state:State-instance-method","name":"state","doc":"Returns the state manager for this context","summary":"<p>Returns the state manager for this context</p>","abstract":false,"location":{"filename":"src/llama/context.cr","line_number":29,"url":"https://github.com/kojix2/llama.cr/blob/114c9694ea5c084fb272373b545d065f36513c0c/src/llama/context.cr#L29"},"def":{"name":"state","return_type":"State","visibility":"Public","body":"@state.not_nil!"}},{"html_id":"to_unsafe:Pointer(Llama::LibLlama::LlamaContext)-instance-method","name":"to_unsafe","doc":"Returns the raw pointer to the underlying llama_context structure","summary":"<p>Returns the raw pointer to the underlying llama_context structure</p>","abstract":false,"location":{"filename":"src/llama/context.cr","line_number":324,"url":"https://github.com/kojix2/llama.cr/blob/114c9694ea5c084fb272373b545d065f36513c0c/src/llama/context.cr#L324"},"def":{"name":"to_unsafe","visibility":"Public","body":"@handle"}}]},{"html_id":"llama/Llama/DistSampler","path":"Llama/DistSampler.html","kind":"class","full_name":"Llama::DistSampler","name":"DistSampler","abstract":false,"superclass":{"html_id":"llama/Llama/Sampler","kind":"class","full_name":"Llama::Sampler","name":"Sampler"},"ancestors":[{"html_id":"llama/Llama/Sampler","kind":"class","full_name":"Llama::Sampler","name":"Sampler"},{"html_id":"llama/Reference","kind":"class","full_name":"Reference","name":"Reference"},{"html_id":"llama/Object","kind":"class","full_name":"Object","name":"Object"}],"locations":[{"filename":"src/llama/sampler.cr","line_number":118,"url":"https://github.com/kojix2/llama.cr/blob/114c9694ea5c084fb272373b545d065f36513c0c/src/llama/sampler.cr#L118"}],"repository_name":"llama","program":false,"enum":false,"alias":false,"const":false,"namespace":{"html_id":"llama/Llama","kind":"module","full_name":"Llama","name":"Llama"},"doc":"Distribution sampler (final sampler in a chain)","summary":"<p>Distribution sampler (final sampler in a chain)</p>","constructors":[{"html_id":"new(seed:UInt32=LibLlama::LLAMA_DEFAULT_SEED)-class-method","name":"new","doc":"Creates a new distribution sampler\n\nParameters:\n- seed: Random seed for sampling (default: LLAMA_DEFAULT_SEED)\n\nRaises:\n- Llama::Error if the sampler cannot be created","summary":"<p>Creates a new distribution sampler</p>","abstract":false,"args":[{"name":"seed","default_value":"LibLlama::LLAMA_DEFAULT_SEED","external_name":"seed","restriction":"UInt32"}],"args_string":"(seed : UInt32 = LibLlama::LLAMA_DEFAULT_SEED)","args_html":"(seed : UInt32 = <span class=\"t\">LibLlama</span><span class=\"t\">::</span><span class=\"t\">LLAMA_DEFAULT_SEED</span>)","location":{"filename":"src/llama/sampler.cr","line_number":126,"url":"https://github.com/kojix2/llama.cr/blob/114c9694ea5c084fb272373b545d065f36513c0c/src/llama/sampler.cr#L126"},"def":{"name":"new","args":[{"name":"seed","default_value":"LibLlama::LLAMA_DEFAULT_SEED","external_name":"seed","restriction":"UInt32"}],"visibility":"Public","body":"_ = allocate\n_.initialize(seed)\nif _.responds_to?(:finalize)\n  ::GC.add_finalizer(_)\nend\n_\n"}}]},{"html_id":"llama/Llama/Error","path":"Llama/Error.html","kind":"class","full_name":"Llama::Error","name":"Error","abstract":false,"superclass":{"html_id":"llama/Exception","kind":"class","full_name":"Exception","name":"Exception"},"ancestors":[{"html_id":"llama/Exception","kind":"class","full_name":"Exception","name":"Exception"},{"html_id":"llama/Reference","kind":"class","full_name":"Reference","name":"Reference"},{"html_id":"llama/Object","kind":"class","full_name":"Object","name":"Object"}],"locations":[{"filename":"src/llama/model.cr","line_number":3,"url":"https://github.com/kojix2/llama.cr/blob/114c9694ea5c084fb272373b545d065f36513c0c/src/llama/model.cr#L3"}],"repository_name":"llama","program":false,"enum":false,"alias":false,"const":false,"namespace":{"html_id":"llama/Llama","kind":"module","full_name":"Llama","name":"Llama"},"doc":"Represents an error that occurred in the Llama library","summary":"<p>Represents an error that occurred in the Llama library</p>"},{"html_id":"llama/Llama/KvCache","path":"Llama/KvCache.html","kind":"class","full_name":"Llama::KvCache","name":"KvCache","abstract":false,"superclass":{"html_id":"llama/Reference","kind":"class","full_name":"Reference","name":"Reference"},"ancestors":[{"html_id":"llama/Reference","kind":"class","full_name":"Reference","name":"Reference"},{"html_id":"llama/Object","kind":"class","full_name":"Object","name":"Object"}],"locations":[{"filename":"src/llama/kv_cache.cr","line_number":4,"url":"https://github.com/kojix2/llama.cr/blob/114c9694ea5c084fb272373b545d065f36513c0c/src/llama/kv_cache.cr#L4"}],"repository_name":"llama","program":false,"enum":false,"alias":false,"const":false,"namespace":{"html_id":"llama/Llama","kind":"module","full_name":"Llama","name":"Llama"},"doc":"Wrapper for the llama_kv_cache structure\nProvides methods for managing the KV (Key-Value) cache in LLaMA models","summary":"<p>Wrapper for the llama_kv_cache structure Provides methods for managing the KV (Key-Value) cache in LLaMA models</p>","constructors":[{"html_id":"new(handle:Pointer(LibLlama::LlamaKvCache),ctx:Context)-class-method","name":"new","doc":"Creates a new KvCache instance from a raw pointer\n\nNote: This constructor is intended for internal use.\nUsers should obtain KvCache instances through Context#kv_cache.","summary":"<p>Creates a new KvCache instance from a raw pointer</p>","abstract":false,"args":[{"name":"handle","external_name":"handle","restriction":"::Pointer(LibLlama::LlamaKvCache)"},{"name":"ctx","external_name":"ctx","restriction":"Context"}],"args_string":"(handle : Pointer(LibLlama::LlamaKvCache), ctx : Context)","args_html":"(handle : Pointer(LibLlama::LlamaKvCache), ctx : <a href=\"../Llama/Context.html\">Context</a>)","location":{"filename":"src/llama/kv_cache.cr","line_number":9,"url":"https://github.com/kojix2/llama.cr/blob/114c9694ea5c084fb272373b545d065f36513c0c/src/llama/kv_cache.cr#L9"},"def":{"name":"new","args":[{"name":"handle","external_name":"handle","restriction":"::Pointer(LibLlama::LlamaKvCache)"},{"name":"ctx","external_name":"ctx","restriction":"Context"}],"visibility":"Public","body":"_ = allocate\n_.initialize(handle, ctx)\nif _.responds_to?(:finalize)\n  ::GC.add_finalizer(_)\nend\n_\n"}}],"instance_methods":[{"html_id":"can_shift?:Bool-instance-method","name":"can_shift?","doc":"Checks if the context supports KV cache shifting\n\nReturns:\n- true if the context supports KV cache shifting, false otherwise","summary":"<p>Checks if the context supports KV cache shifting</p>","abstract":false,"location":{"filename":"src/llama/kv_cache.cr","line_number":107,"url":"https://github.com/kojix2/llama.cr/blob/114c9694ea5c084fb272373b545d065f36513c0c/src/llama/kv_cache.cr#L107"},"def":{"name":"can_shift?","return_type":"Bool","visibility":"Public","body":"LibLlama.llama_kv_self_can_shift(@ctx.to_unsafe)"}},{"html_id":"clear-instance-method","name":"clear","doc":"Clears the KV cache\nThis removes all tokens from the cache and resets its state","summary":"<p>Clears the KV cache This removes all tokens from the cache and resets its state</p>","abstract":false,"location":{"filename":"src/llama/kv_cache.cr","line_number":14,"url":"https://github.com/kojix2/llama.cr/blob/114c9694ea5c084fb272373b545d065f36513c0c/src/llama/kv_cache.cr#L14"},"def":{"name":"clear","visibility":"Public","body":"LibLlama.llama_kv_self_clear(@ctx.to_unsafe)"}},{"html_id":"defrag-instance-method","name":"defrag","doc":"Defragments the KV cache\nThis will be applied lazily on next decode or explicitly with update","summary":"<p>Defragments the KV cache This will be applied lazily on next decode or explicitly with update</p>","abstract":false,"location":{"filename":"src/llama/kv_cache.cr","line_number":99,"url":"https://github.com/kojix2/llama.cr/blob/114c9694ea5c084fb272373b545d065f36513c0c/src/llama/kv_cache.cr#L99"},"def":{"name":"defrag","visibility":"Public","body":"LibLlama.llama_kv_self_defrag(@ctx.to_unsafe)"}},{"html_id":"n_tokens:Int32-instance-method","name":"n_tokens","doc":"Returns the number of tokens in the KV cache\nIf a KV cell has multiple sequences assigned to it, it will be counted multiple times","summary":"<p>Returns the number of tokens in the KV cache If a KV cell has multiple sequences assigned to it, it will be counted multiple times</p>","abstract":false,"location":{"filename":"src/llama/kv_cache.cr","line_number":20,"url":"https://github.com/kojix2/llama.cr/blob/114c9694ea5c084fb272373b545d065f36513c0c/src/llama/kv_cache.cr#L20"},"def":{"name":"n_tokens","return_type":"Int32","visibility":"Public","body":"LibLlama.llama_kv_self_n_tokens(@ctx.to_unsafe)"}},{"html_id":"seq_add(seq_id:Int32,p0:Int32,p1:Int32,delta:Int32)-instance-method","name":"seq_add","doc":"Adds a relative position delta to tokens in a sequence\n\nParameters:\n- seq_id: The sequence ID to modify\n- p0: Start position (p0 < 0 means start from 0)\n- p1: End position (p1 < 0 means end at infinity)\n- delta: The position delta to add","summary":"<p>Adds a relative position delta to tokens in a sequence</p>","abstract":false,"args":[{"name":"seq_id","external_name":"seq_id","restriction":"Int32"},{"name":"p0","external_name":"p0","restriction":"Int32"},{"name":"p1","external_name":"p1","restriction":"Int32"},{"name":"delta","external_name":"delta","restriction":"Int32"}],"args_string":"(seq_id : Int32, p0 : Int32, p1 : Int32, delta : Int32)","args_html":"(seq_id : Int32, p0 : Int32, p1 : Int32, delta : Int32)","location":{"filename":"src/llama/kv_cache.cr","line_number":70,"url":"https://github.com/kojix2/llama.cr/blob/114c9694ea5c084fb272373b545d065f36513c0c/src/llama/kv_cache.cr#L70"},"def":{"name":"seq_add","args":[{"name":"seq_id","external_name":"seq_id","restriction":"Int32"},{"name":"p0","external_name":"p0","restriction":"Int32"},{"name":"p1","external_name":"p1","restriction":"Int32"},{"name":"delta","external_name":"delta","restriction":"Int32"}],"visibility":"Public","body":"LibLlama.llama_kv_self_seq_add(@ctx.to_unsafe, seq_id, p0, p1, delta)"}},{"html_id":"seq_cp(seq_id_src:Int32,seq_id_dst:Int32,p0:Int32,p1:Int32)-instance-method","name":"seq_cp","doc":"Copies tokens from one sequence to another in the KV cache\n\nParameters:\n- seq_id_src: Source sequence ID\n- seq_id_dst: Destination sequence ID\n- p0: Start position (p0 < 0 means start from 0)\n- p1: End position (p1 < 0 means end at infinity)","summary":"<p>Copies tokens from one sequence to another in the KV cache</p>","abstract":false,"args":[{"name":"seq_id_src","external_name":"seq_id_src","restriction":"Int32"},{"name":"seq_id_dst","external_name":"seq_id_dst","restriction":"Int32"},{"name":"p0","external_name":"p0","restriction":"Int32"},{"name":"p1","external_name":"p1","restriction":"Int32"}],"args_string":"(seq_id_src : Int32, seq_id_dst : Int32, p0 : Int32, p1 : Int32)","args_html":"(seq_id_src : Int32, seq_id_dst : Int32, p0 : Int32, p1 : Int32)","location":{"filename":"src/llama/kv_cache.cr","line_number":51,"url":"https://github.com/kojix2/llama.cr/blob/114c9694ea5c084fb272373b545d065f36513c0c/src/llama/kv_cache.cr#L51"},"def":{"name":"seq_cp","args":[{"name":"seq_id_src","external_name":"seq_id_src","restriction":"Int32"},{"name":"seq_id_dst","external_name":"seq_id_dst","restriction":"Int32"},{"name":"p0","external_name":"p0","restriction":"Int32"},{"name":"p1","external_name":"p1","restriction":"Int32"}],"visibility":"Public","body":"LibLlama.llama_kv_self_seq_cp(@ctx.to_unsafe, seq_id_src, seq_id_dst, p0, p1)"}},{"html_id":"seq_div(seq_id:Int32,p0:Int32,p1:Int32,d:Int32)-instance-method","name":"seq_div","doc":"Divides the positions of tokens in a sequence by a factor\n\nParameters:\n- seq_id: The sequence ID to modify\n- p0: Start position (p0 < 0 means start from 0)\n- p1: End position (p1 < 0 means end at infinity)\n- d: The divisor (must be > 1)","summary":"<p>Divides the positions of tokens in a sequence by a factor</p>","abstract":false,"args":[{"name":"seq_id","external_name":"seq_id","restriction":"Int32"},{"name":"p0","external_name":"p0","restriction":"Int32"},{"name":"p1","external_name":"p1","restriction":"Int32"},{"name":"d","external_name":"d","restriction":"Int32"}],"args_string":"(seq_id : Int32, p0 : Int32, p1 : Int32, d : Int32)","args_html":"(seq_id : Int32, p0 : Int32, p1 : Int32, d : Int32)","location":{"filename":"src/llama/kv_cache.cr","line_number":81,"url":"https://github.com/kojix2/llama.cr/blob/114c9694ea5c084fb272373b545d065f36513c0c/src/llama/kv_cache.cr#L81"},"def":{"name":"seq_div","args":[{"name":"seq_id","external_name":"seq_id","restriction":"Int32"},{"name":"p0","external_name":"p0","restriction":"Int32"},{"name":"p1","external_name":"p1","restriction":"Int32"},{"name":"d","external_name":"d","restriction":"Int32"}],"visibility":"Public","body":"if d <= 1\n  raise(ArgumentError.new(\"Divisor must be greater than 1\"))\nend\nLibLlama.llama_kv_self_seq_div(@ctx.to_unsafe, seq_id, p0, p1, d)\n"}},{"html_id":"seq_keep(seq_id:Int32)-instance-method","name":"seq_keep","doc":"Keeps only the specified sequence in the KV cache, removing all others\n\nParameters:\n- seq_id: The sequence ID to keep","summary":"<p>Keeps only the specified sequence in the KV cache, removing all others</p>","abstract":false,"args":[{"name":"seq_id","external_name":"seq_id","restriction":"Int32"}],"args_string":"(seq_id : Int32)","args_html":"(seq_id : Int32)","location":{"filename":"src/llama/kv_cache.cr","line_number":59,"url":"https://github.com/kojix2/llama.cr/blob/114c9694ea5c084fb272373b545d065f36513c0c/src/llama/kv_cache.cr#L59"},"def":{"name":"seq_keep","args":[{"name":"seq_id","external_name":"seq_id","restriction":"Int32"}],"visibility":"Public","body":"LibLlama.llama_kv_self_seq_keep(@ctx.to_unsafe, seq_id)"}},{"html_id":"seq_pos_max(seq_id:Int32):Int32-instance-method","name":"seq_pos_max","doc":"Returns the maximum position in a sequence\n\nParameters:\n- seq_id: The sequence ID to query\n\nReturns:\n- The maximum position in the sequence","summary":"<p>Returns the maximum position in a sequence</p>","abstract":false,"args":[{"name":"seq_id","external_name":"seq_id","restriction":"Int32"}],"args_string":"(seq_id : Int32) : Int32","args_html":"(seq_id : Int32) : Int32","location":{"filename":"src/llama/kv_cache.cr","line_number":93,"url":"https://github.com/kojix2/llama.cr/blob/114c9694ea5c084fb272373b545d065f36513c0c/src/llama/kv_cache.cr#L93"},"def":{"name":"seq_pos_max","args":[{"name":"seq_id","external_name":"seq_id","restriction":"Int32"}],"return_type":"Int32","visibility":"Public","body":"LibLlama.llama_kv_self_seq_pos_max(@ctx.to_unsafe, seq_id)"}},{"html_id":"seq_rm(seq_id:Int32,p0:Int32,p1:Int32):Bool-instance-method","name":"seq_rm","doc":"Removes tokens from a sequence in the KV cache\n\nParameters:\n- seq_id: The sequence ID to remove tokens from (seq_id < 0 matches any sequence)\n- p0: Start position (p0 < 0 means start from 0)\n- p1: End position (p1 < 0 means end at infinity)\n\nReturns:\n- true if successful, false if a partial sequence cannot be removed\n  (removing a whole sequence never fails)","summary":"<p>Removes tokens from a sequence in the KV cache</p>","abstract":false,"args":[{"name":"seq_id","external_name":"seq_id","restriction":"Int32"},{"name":"p0","external_name":"p0","restriction":"Int32"},{"name":"p1","external_name":"p1","restriction":"Int32"}],"args_string":"(seq_id : Int32, p0 : Int32, p1 : Int32) : Bool","args_html":"(seq_id : Int32, p0 : Int32, p1 : Int32) : Bool","location":{"filename":"src/llama/kv_cache.cr","line_number":40,"url":"https://github.com/kojix2/llama.cr/blob/114c9694ea5c084fb272373b545d065f36513c0c/src/llama/kv_cache.cr#L40"},"def":{"name":"seq_rm","args":[{"name":"seq_id","external_name":"seq_id","restriction":"Int32"},{"name":"p0","external_name":"p0","restriction":"Int32"},{"name":"p1","external_name":"p1","restriction":"Int32"}],"return_type":"Bool","visibility":"Public","body":"LibLlama.llama_kv_self_seq_rm(@ctx.to_unsafe, seq_id, p0, p1)"}},{"html_id":"to_unsafe:Pointer(Llama::LibLlama::LlamaKvCache)-instance-method","name":"to_unsafe","doc":"Returns the raw pointer to the underlying llama_kv_cache structure","summary":"<p>Returns the raw pointer to the underlying llama_kv_cache structure</p>","abstract":false,"location":{"filename":"src/llama/kv_cache.cr","line_number":118,"url":"https://github.com/kojix2/llama.cr/blob/114c9694ea5c084fb272373b545d065f36513c0c/src/llama/kv_cache.cr#L118"},"def":{"name":"to_unsafe","visibility":"Public","body":"@handle"}},{"html_id":"update-instance-method","name":"update","doc":"Applies pending KV cache updates\nThis includes K-shifts, defragmentation, etc.","summary":"<p>Applies pending KV cache updates This includes K-shifts, defragmentation, etc.</p>","abstract":false,"location":{"filename":"src/llama/kv_cache.cr","line_number":113,"url":"https://github.com/kojix2/llama.cr/blob/114c9694ea5c084fb272373b545d065f36513c0c/src/llama/kv_cache.cr#L113"},"def":{"name":"update","visibility":"Public","body":"LibLlama.llama_kv_self_update(@ctx.to_unsafe)"}},{"html_id":"used_cells:Int32-instance-method","name":"used_cells","doc":"Returns the number of used KV cells\nA cell is considered used if it has at least one sequence assigned to it","summary":"<p>Returns the number of used KV cells A cell is considered used if it has at least one sequence assigned to it</p>","abstract":false,"location":{"filename":"src/llama/kv_cache.cr","line_number":26,"url":"https://github.com/kojix2/llama.cr/blob/114c9694ea5c084fb272373b545d065f36513c0c/src/llama/kv_cache.cr#L26"},"def":{"name":"used_cells","return_type":"Int32","visibility":"Public","body":"LibLlama.llama_kv_self_used_cells(@ctx.to_unsafe)"}}]},{"html_id":"llama/Llama/Model","path":"Llama/Model.html","kind":"class","full_name":"Llama::Model","name":"Model","abstract":false,"superclass":{"html_id":"llama/Reference","kind":"class","full_name":"Reference","name":"Reference"},"ancestors":[{"html_id":"llama/Reference","kind":"class","full_name":"Reference","name":"Reference"},{"html_id":"llama/Object","kind":"class","full_name":"Object","name":"Object"}],"locations":[{"filename":"src/llama/model.cr","line_number":6,"url":"https://github.com/kojix2/llama.cr/blob/114c9694ea5c084fb272373b545d065f36513c0c/src/llama/model.cr#L6"}],"repository_name":"llama","program":false,"enum":false,"alias":false,"const":false,"namespace":{"html_id":"llama/Llama","kind":"module","full_name":"Llama","name":"Llama"},"doc":"Wrapper for the llama_model structure","summary":"<p>Wrapper for the llama_model structure</p>","constructors":[{"html_id":"new(path:String)-class-method","name":"new","doc":"Creates a new Model instance by loading a model from a file\n\nParameters:\n- path: Path to the model file (.gguf format)\n\nRaises:\n- Llama::Error if the model cannot be loaded","summary":"<p>Creates a new Model instance by loading a model from a file</p>","abstract":false,"args":[{"name":"path","external_name":"path","restriction":"String"}],"args_string":"(path : String)","args_html":"(path : String)","location":{"filename":"src/llama/model.cr","line_number":14,"url":"https://github.com/kojix2/llama.cr/blob/114c9694ea5c084fb272373b545d065f36513c0c/src/llama/model.cr#L14"},"def":{"name":"new","args":[{"name":"path","external_name":"path","restriction":"String"}],"visibility":"Public","body":"_ = allocate\n_.initialize(path)\nif _.responds_to?(:finalize)\n  ::GC.add_finalizer(_)\nend\n_\n"}}],"instance_methods":[{"html_id":"chat_template(name:String|Nil=nil):String|Nil-instance-method","name":"chat_template","doc":"Gets the default chat template for this model\n\nParameters:\n- name: Optional template name (nil for default)\n\nReturns:\n- The chat template string, or nil if not available","summary":"<p>Gets the default chat template for this model</p>","abstract":false,"args":[{"name":"name","default_value":"nil","external_name":"name","restriction":"String | ::Nil"}],"args_string":"(name : String | Nil = nil) : String | Nil","args_html":"(name : String | Nil = <span class=\"n\">nil</span>) : String | Nil","location":{"filename":"src/llama/model.cr","line_number":27,"url":"https://github.com/kojix2/llama.cr/blob/114c9694ea5c084fb272373b545d065f36513c0c/src/llama/model.cr#L27"},"def":{"name":"chat_template","args":[{"name":"name","default_value":"nil","external_name":"name","restriction":"String | ::Nil"}],"return_type":"String | ::Nil","visibility":"Public","body":"ptr = LibLlama.llama_model_chat_template(@handle, name.nil? ? nil : name.to_unsafe)\nif ptr.null?\n  return nil\nend\nString.new(ptr)\n"}},{"html_id":"context(params=nil):Context-instance-method","name":"context","doc":"Creates a new Context for this model","summary":"<p>Creates a new Context for this model</p>","abstract":false,"args":[{"name":"params","default_value":"nil","external_name":"params","restriction":""}],"args_string":"(params = nil) : Context","args_html":"(params = <span class=\"n\">nil</span>) : <a href=\"../Llama/Context.html\">Context</a>","location":{"filename":"src/llama/model.cr","line_number":60,"url":"https://github.com/kojix2/llama.cr/blob/114c9694ea5c084fb272373b545d065f36513c0c/src/llama/model.cr#L60"},"def":{"name":"context","args":[{"name":"params","default_value":"nil","external_name":"params","restriction":""}],"return_type":"Context","visibility":"Public","body":"params || (params = LibLlama.llama_context_default_params)\nContext.new(self, params)\n"}},{"html_id":"finalize-instance-method","name":"finalize","doc":"Frees the resources associated with this model","summary":"<p>Frees the resources associated with this model</p>","abstract":false,"location":{"filename":"src/llama/model.cr","line_number":71,"url":"https://github.com/kojix2/llama.cr/blob/114c9694ea5c084fb272373b545d065f36513c0c/src/llama/model.cr#L71"},"def":{"name":"finalize","visibility":"Public","body":"if @handle.null?\nelse\n  LibLlama.llama_model_free(@handle)\nend"}},{"html_id":"n_embd:Int32-instance-method","name":"n_embd","doc":"Returns the number of embedding dimensions in the model","summary":"<p>Returns the number of embedding dimensions in the model</p>","abstract":false,"location":{"filename":"src/llama/model.cr","line_number":45,"url":"https://github.com/kojix2/llama.cr/blob/114c9694ea5c084fb272373b545d065f36513c0c/src/llama/model.cr#L45"},"def":{"name":"n_embd","return_type":"Int32","visibility":"Public","body":"LibLlama.llama_model_n_embd(@handle)"}},{"html_id":"n_head:Int32-instance-method","name":"n_head","doc":"Returns the number of attention heads in the model","summary":"<p>Returns the number of attention heads in the model</p>","abstract":false,"location":{"filename":"src/llama/model.cr","line_number":55,"url":"https://github.com/kojix2/llama.cr/blob/114c9694ea5c084fb272373b545d065f36513c0c/src/llama/model.cr#L55"},"def":{"name":"n_head","return_type":"Int32","visibility":"Public","body":"LibLlama.llama_model_n_head(@handle)"}},{"html_id":"n_layer:Int32-instance-method","name":"n_layer","doc":"Returns the number of layers in the model","summary":"<p>Returns the number of layers in the model</p>","abstract":false,"location":{"filename":"src/llama/model.cr","line_number":50,"url":"https://github.com/kojix2/llama.cr/blob/114c9694ea5c084fb272373b545d065f36513c0c/src/llama/model.cr#L50"},"def":{"name":"n_layer","return_type":"Int32","visibility":"Public","body":"LibLlama.llama_model_n_layer(@handle)"}},{"html_id":"n_params:UInt64-instance-method","name":"n_params","doc":"Returns the number of parameters in the model","summary":"<p>Returns the number of parameters in the model</p>","abstract":false,"location":{"filename":"src/llama/model.cr","line_number":40,"url":"https://github.com/kojix2/llama.cr/blob/114c9694ea5c084fb272373b545d065f36513c0c/src/llama/model.cr#L40"},"def":{"name":"n_params","return_type":"UInt64","visibility":"Public","body":"LibLlama.llama_model_n_params(@handle)"}},{"html_id":"to_unsafe:Pointer(Llama::LibLlama::LlamaModel)-instance-method","name":"to_unsafe","doc":"Returns the raw pointer to the underlying llama_model structure","summary":"<p>Returns the raw pointer to the underlying llama_model structure</p>","abstract":false,"location":{"filename":"src/llama/model.cr","line_number":66,"url":"https://github.com/kojix2/llama.cr/blob/114c9694ea5c084fb272373b545d065f36513c0c/src/llama/model.cr#L66"},"def":{"name":"to_unsafe","visibility":"Public","body":"@handle"}},{"html_id":"vocab:Vocab-instance-method","name":"vocab","doc":"Returns the vocabulary associated with this model","summary":"<p>Returns the vocabulary associated with this model</p>","abstract":false,"location":{"filename":"src/llama/model.cr","line_number":34,"url":"https://github.com/kojix2/llama.cr/blob/114c9694ea5c084fb272373b545d065f36513c0c/src/llama/model.cr#L34"},"def":{"name":"vocab","return_type":"Vocab","visibility":"Public","body":"vocab_ptr = LibLlama.llama_model_get_vocab(@handle)\nVocab.new(vocab_ptr)\n"}}]},{"html_id":"llama/Llama/Sampler","path":"Llama/Sampler.html","kind":"class","full_name":"Llama::Sampler","name":"Sampler","abstract":false,"superclass":{"html_id":"llama/Reference","kind":"class","full_name":"Reference","name":"Reference"},"ancestors":[{"html_id":"llama/Reference","kind":"class","full_name":"Reference","name":"Reference"},{"html_id":"llama/Object","kind":"class","full_name":"Object","name":"Object"}],"locations":[{"filename":"src/llama/sampler.cr","line_number":3,"url":"https://github.com/kojix2/llama.cr/blob/114c9694ea5c084fb272373b545d065f36513c0c/src/llama/sampler.cr#L3"}],"repository_name":"llama","program":false,"enum":false,"alias":false,"const":false,"subclasses":[{"html_id":"llama/Llama/DistSampler","kind":"class","full_name":"Llama::DistSampler","name":"DistSampler"},{"html_id":"llama/Llama/SamplerChain","kind":"class","full_name":"Llama::SamplerChain","name":"SamplerChain"},{"html_id":"llama/Llama/TempSampler","kind":"class","full_name":"Llama::TempSampler","name":"TempSampler"},{"html_id":"llama/Llama/TopKSampler","kind":"class","full_name":"Llama::TopKSampler","name":"TopKSampler"},{"html_id":"llama/Llama/TopPSampler","kind":"class","full_name":"Llama::TopPSampler","name":"TopPSampler"}],"namespace":{"html_id":"llama/Llama","kind":"module","full_name":"Llama","name":"Llama"},"doc":"Wrapper for the llama_sampler structure","summary":"<p>Wrapper for the llama_sampler structure</p>","constructors":[{"html_id":"new(handle:Pointer(LibLlama::LlamaSampler))-class-method","name":"new","doc":"Creates a new Sampler instance from a raw pointer\n\nNote: This constructor is intended for internal use.","summary":"<p>Creates a new Sampler instance from a raw pointer</p>","abstract":false,"args":[{"name":"handle","external_name":"handle","restriction":"::Pointer(LibLlama::LlamaSampler)"}],"args_string":"(handle : Pointer(LibLlama::LlamaSampler))","args_html":"(handle : Pointer(LibLlama::LlamaSampler))","location":{"filename":"src/llama/sampler.cr","line_number":7,"url":"https://github.com/kojix2/llama.cr/blob/114c9694ea5c084fb272373b545d065f36513c0c/src/llama/sampler.cr#L7"},"def":{"name":"new","args":[{"name":"handle","external_name":"handle","restriction":"::Pointer(LibLlama::LlamaSampler)"}],"visibility":"Public","body":"_ = allocate\n_.initialize(handle)\nif _.responds_to?(:finalize)\n  ::GC.add_finalizer(_)\nend\n_\n"}}],"instance_methods":[{"html_id":"finalize-instance-method","name":"finalize","doc":"Frees the resources associated with this sampler","summary":"<p>Frees the resources associated with this sampler</p>","abstract":false,"location":{"filename":"src/llama/sampler.cr","line_number":16,"url":"https://github.com/kojix2/llama.cr/blob/114c9694ea5c084fb272373b545d065f36513c0c/src/llama/sampler.cr#L16"},"def":{"name":"finalize","visibility":"Public","body":"if @handle.null?\nelse\n  LibLlama.llama_sampler_free(@handle)\nend"}},{"html_id":"to_unsafe:Pointer(Llama::LibLlama::LlamaSampler)-instance-method","name":"to_unsafe","doc":"Returns the raw pointer to the underlying llama_sampler structure","summary":"<p>Returns the raw pointer to the underlying llama_sampler structure</p>","abstract":false,"location":{"filename":"src/llama/sampler.cr","line_number":11,"url":"https://github.com/kojix2/llama.cr/blob/114c9694ea5c084fb272373b545d065f36513c0c/src/llama/sampler.cr#L11"},"def":{"name":"to_unsafe","visibility":"Public","body":"@handle"}}]},{"html_id":"llama/Llama/SamplerChain","path":"Llama/SamplerChain.html","kind":"class","full_name":"Llama::SamplerChain","name":"SamplerChain","abstract":false,"superclass":{"html_id":"llama/Llama/Sampler","kind":"class","full_name":"Llama::Sampler","name":"Sampler"},"ancestors":[{"html_id":"llama/Llama/Sampler","kind":"class","full_name":"Llama::Sampler","name":"Sampler"},{"html_id":"llama/Reference","kind":"class","full_name":"Reference","name":"Reference"},{"html_id":"llama/Object","kind":"class","full_name":"Object","name":"Object"}],"locations":[{"filename":"src/llama/sampler.cr","line_number":24,"url":"https://github.com/kojix2/llama.cr/blob/114c9694ea5c084fb272373b545d065f36513c0c/src/llama/sampler.cr#L24"}],"repository_name":"llama","program":false,"enum":false,"alias":false,"const":false,"namespace":{"html_id":"llama/Llama","kind":"module","full_name":"Llama","name":"Llama"},"doc":"Wrapper for a chain of samplers","summary":"<p>Wrapper for a chain of samplers</p>","constructors":[{"html_id":"new(params=nil)-class-method","name":"new","doc":"Creates a new SamplerChain with optional parameters\n\nParameters:\n- params: Optional sampler chain parameters\n\nRaises:\n- Llama::Error if the sampler chain cannot be created","summary":"<p>Creates a new SamplerChain with optional parameters</p>","abstract":false,"args":[{"name":"params","default_value":"nil","external_name":"params","restriction":""}],"args_string":"(params = nil)","args_html":"(params = <span class=\"n\">nil</span>)","location":{"filename":"src/llama/sampler.cr","line_number":32,"url":"https://github.com/kojix2/llama.cr/blob/114c9694ea5c084fb272373b545d065f36513c0c/src/llama/sampler.cr#L32"},"def":{"name":"new","args":[{"name":"params","default_value":"nil","external_name":"params","restriction":""}],"visibility":"Public","body":"_ = allocate\n_.initialize(params)\nif _.responds_to?(:finalize)\n  ::GC.add_finalizer(_)\nend\n_\n"}}],"instance_methods":[{"html_id":"accept(token:Int32)-instance-method","name":"accept","doc":"Accepts a token, updating the internal state of the samplers\n\nParameters:\n- token: The token to accept","summary":"<p>Accepts a token, updating the internal state of the samplers</p>","abstract":false,"args":[{"name":"token","external_name":"token","restriction":"Int32"}],"args_string":"(token : Int32)","args_html":"(token : Int32)","location":{"filename":"src/llama/sampler.cr","line_number":64,"url":"https://github.com/kojix2/llama.cr/blob/114c9694ea5c084fb272373b545d065f36513c0c/src/llama/sampler.cr#L64"},"def":{"name":"accept","args":[{"name":"token","external_name":"token","restriction":"Int32"}],"visibility":"Public","body":"LibLlama.llama_sampler_accept(@handle, token)"}},{"html_id":"add(sampler:Sampler)-instance-method","name":"add","doc":"Adds a sampler to the chain\n\nParameters:\n- sampler: The sampler to add to the chain","summary":"<p>Adds a sampler to the chain</p>","abstract":false,"args":[{"name":"sampler","external_name":"sampler","restriction":"Sampler"}],"args_string":"(sampler : Sampler)","args_html":"(sampler : <a href=\"../Llama/Sampler.html\">Sampler</a>)","location":{"filename":"src/llama/sampler.cr","line_number":43,"url":"https://github.com/kojix2/llama.cr/blob/114c9694ea5c084fb272373b545d065f36513c0c/src/llama/sampler.cr#L43"},"def":{"name":"add","args":[{"name":"sampler","external_name":"sampler","restriction":"Sampler"}],"visibility":"Public","body":"LibLlama.llama_sampler_chain_add(@handle, sampler.to_unsafe)\n@samplers << sampler\n"}},{"html_id":"sample(ctx:Context,idx:Int32=-1):Int32-instance-method","name":"sample","doc":"Samples a token using the sampler chain\n\nParameters:\n- ctx: The context to sample from\n- idx: The index of the logits to sample from (-1 for the last token)\n\nReturns:\n- The sampled token","summary":"<p>Samples a token using the sampler chain</p>","abstract":false,"args":[{"name":"ctx","external_name":"ctx","restriction":"Context"},{"name":"idx","default_value":"-1","external_name":"idx","restriction":"Int32"}],"args_string":"(ctx : Context, idx : Int32 = -1) : Int32","args_html":"(ctx : <a href=\"../Llama/Context.html\">Context</a>, idx : Int32 = <span class=\"n\">-1</span>) : Int32","location":{"filename":"src/llama/sampler.cr","line_number":56,"url":"https://github.com/kojix2/llama.cr/blob/114c9694ea5c084fb272373b545d065f36513c0c/src/llama/sampler.cr#L56"},"def":{"name":"sample","args":[{"name":"ctx","external_name":"ctx","restriction":"Context"},{"name":"idx","default_value":"-1","external_name":"idx","restriction":"Int32"}],"return_type":"Int32","visibility":"Public","body":"LibLlama.llama_sampler_sample(@handle, ctx.to_unsafe, idx)"}}]},{"html_id":"llama/Llama/State","path":"Llama/State.html","kind":"class","full_name":"Llama::State","name":"State","abstract":false,"superclass":{"html_id":"llama/Reference","kind":"class","full_name":"Reference","name":"Reference"},"ancestors":[{"html_id":"llama/Reference","kind":"class","full_name":"Reference","name":"Reference"},{"html_id":"llama/Object","kind":"class","full_name":"Object","name":"Object"}],"locations":[{"filename":"src/llama/state.cr","line_number":4,"url":"https://github.com/kojix2/llama.cr/blob/114c9694ea5c084fb272373b545d065f36513c0c/src/llama/state.cr#L4"}],"repository_name":"llama","program":false,"enum":false,"alias":false,"const":false,"namespace":{"html_id":"llama/Llama","kind":"module","full_name":"Llama","name":"Llama"},"doc":"Wrapper for state management functions in llama.cpp\nProvides methods for saving and loading model state","summary":"<p>Wrapper for state management functions in llama.cpp Provides methods for saving and loading model state</p>","constructors":[{"html_id":"new(ctx:Context)-class-method","name":"new","doc":"Creates a new State instance\n\nParameters:\n- ctx: The Context to manage state for","summary":"<p>Creates a new State instance</p>","abstract":false,"args":[{"name":"ctx","external_name":"ctx","restriction":"Context"}],"args_string":"(ctx : Context)","args_html":"(ctx : <a href=\"../Llama/Context.html\">Context</a>)","location":{"filename":"src/llama/state.cr","line_number":9,"url":"https://github.com/kojix2/llama.cr/blob/114c9694ea5c084fb272373b545d065f36513c0c/src/llama/state.cr#L9"},"def":{"name":"new","args":[{"name":"ctx","external_name":"ctx","restriction":"Context"}],"visibility":"Public","body":"_ = allocate\n_.initialize(ctx)\nif _.responds_to?(:finalize)\n  ::GC.add_finalizer(_)\nend\n_\n"}}],"instance_methods":[{"html_id":"get_data:Bytes-instance-method","name":"get_data","doc":"Gets the current state data\n\nReturns:\n- A Bytes object containing the state data","summary":"<p>Gets the current state data</p>","abstract":false,"location":{"filename":"src/llama/state.cr","line_number":24,"url":"https://github.com/kojix2/llama.cr/blob/114c9694ea5c084fb272373b545d065f36513c0c/src/llama/state.cr#L24"},"def":{"name":"get_data","return_type":"Bytes","visibility":"Public","body":"state_size = size\nbuffer = Bytes.new(state_size)\nbytes_copied = LibLlama.llama_state_get_data(@ctx.to_unsafe, buffer.to_unsafe, state_size)\nbuffer[0, bytes_copied]\n"}},{"html_id":"load_file(path:String,max_tokens:Int32=1024):Array(Int32)-instance-method","name":"load_file","doc":"Loads state from a file\n\nParameters:\n- path: Path to the session file\n- max_tokens: Maximum number of tokens to load (default: 1024)\n\nReturns:\n- An array of tokens loaded from the file\n\nRaises:\n- Llama::Error if the file cannot be loaded","summary":"<p>Loads state from a file</p>","abstract":false,"args":[{"name":"path","external_name":"path","restriction":"String"},{"name":"max_tokens","default_value":"1024","external_name":"max_tokens","restriction":"Int32"}],"args_string":"(path : String, max_tokens : Int32 = 1024) : Array(Int32)","args_html":"(path : String, max_tokens : Int32 = <span class=\"n\">1024</span>) : Array(Int32)","location":{"filename":"src/llama/state.cr","line_number":60,"url":"https://github.com/kojix2/llama.cr/blob/114c9694ea5c084fb272373b545d065f36513c0c/src/llama/state.cr#L60"},"def":{"name":"load_file","args":[{"name":"path","external_name":"path","restriction":"String"},{"name":"max_tokens","default_value":"1024","external_name":"max_tokens","restriction":"Int32"}],"return_type":"Array(Int32)","visibility":"Public","body":"tokens = Pointer(LibLlama::LlamaToken).malloc(max_tokens)\ntoken_count = Pointer(LibC::SizeT).malloc(1)\ntoken_count.value = 0\nsuccess = LibLlama.llama_state_load_file(@ctx.to_unsafe, path, tokens, max_tokens, token_count)\nif success\nelse\n  raise(Error.new(\"Failed to load state from #{path}\"))\nend\nresult = Array(Int32).new(token_count.value)\ntoken_count.value.times do |i|\n  result << tokens[i]\nend\nresult\n"}},{"html_id":"save_file(path:String,tokens:Array(Int32)):Bool-instance-method","name":"save_file","doc":"Saves state to a file\n\nParameters:\n- path: Path to save the session file\n- tokens: Array of tokens to save with the state\n\nReturns:\n- true if successful, false otherwise","summary":"<p>Saves state to a file</p>","abstract":false,"args":[{"name":"path","external_name":"path","restriction":"String"},{"name":"tokens","external_name":"tokens","restriction":"Array(Int32)"}],"args_string":"(path : String, tokens : Array(Int32)) : Bool","args_html":"(path : String, tokens : Array(Int32)) : Bool","location":{"filename":"src/llama/state.cr","line_number":94,"url":"https://github.com/kojix2/llama.cr/blob/114c9694ea5c084fb272373b545d065f36513c0c/src/llama/state.cr#L94"},"def":{"name":"save_file","args":[{"name":"path","external_name":"path","restriction":"String"},{"name":"tokens","external_name":"tokens","restriction":"Array(Int32)"}],"return_type":"Bool","visibility":"Public","body":"tokens_ptr = tokens.to_unsafe\nLibLlama.llama_state_save_file(@ctx.to_unsafe, path, tokens_ptr, tokens.size)\n"}},{"html_id":"seq_get_data(seq_id:Int32):Bytes-instance-method","name":"seq_get_data","doc":"Gets the state data for a specific sequence\n\nParameters:\n- seq_id: The sequence ID\n\nReturns:\n- A Bytes object containing the sequence state data","summary":"<p>Gets the state data for a specific sequence</p>","abstract":false,"args":[{"name":"seq_id","external_name":"seq_id","restriction":"Int32"}],"args_string":"(seq_id : Int32) : Bytes","args_html":"(seq_id : Int32) : Bytes","location":{"filename":"src/llama/state.cr","line_number":125,"url":"https://github.com/kojix2/llama.cr/blob/114c9694ea5c084fb272373b545d065f36513c0c/src/llama/state.cr#L125"},"def":{"name":"seq_get_data","args":[{"name":"seq_id","external_name":"seq_id","restriction":"Int32"}],"return_type":"Bytes","visibility":"Public","body":"state_size = seq_size(seq_id)\nbuffer = Bytes.new(state_size)\nbytes_copied = LibLlama.llama_state_seq_get_data(@ctx.to_unsafe, buffer.to_unsafe, state_size, seq_id)\nbuffer[0, bytes_copied]\n"}},{"html_id":"seq_load_file(path:String,dest_seq_id:Int32,max_tokens:Int32=1024):Array(Int32)-instance-method","name":"seq_load_file","doc":"Loads a sequence's state from a file\n\nParameters:\n- path: Path to the sequence file\n- dest_seq_id: The destination sequence ID\n- max_tokens: Maximum number of tokens to load (default: 1024)\n\nReturns:\n- An array of tokens loaded from the file\n\nRaises:\n- Llama::Error if the file cannot be loaded","summary":"<p>Loads a sequence's state from a file</p>","abstract":false,"args":[{"name":"path","external_name":"path","restriction":"String"},{"name":"dest_seq_id","external_name":"dest_seq_id","restriction":"Int32"},{"name":"max_tokens","default_value":"1024","external_name":"max_tokens","restriction":"Int32"}],"args_string":"(path : String, dest_seq_id : Int32, max_tokens : Int32 = 1024) : Array(Int32)","args_html":"(path : String, dest_seq_id : Int32, max_tokens : Int32 = <span class=\"n\">1024</span>) : Array(Int32)","location":{"filename":"src/llama/state.cr","line_number":196,"url":"https://github.com/kojix2/llama.cr/blob/114c9694ea5c084fb272373b545d065f36513c0c/src/llama/state.cr#L196"},"def":{"name":"seq_load_file","args":[{"name":"path","external_name":"path","restriction":"String"},{"name":"dest_seq_id","external_name":"dest_seq_id","restriction":"Int32"},{"name":"max_tokens","default_value":"1024","external_name":"max_tokens","restriction":"Int32"}],"return_type":"Array(Int32)","visibility":"Public","body":"tokens = Pointer(LibLlama::LlamaToken).malloc(max_tokens)\ntoken_count = Pointer(LibC::SizeT).malloc(1)\ntoken_count.value = 0\nbytes_read = LibLlama.llama_state_seq_load_file(@ctx.to_unsafe, path, dest_seq_id, tokens, max_tokens, token_count)\nif bytes_read == 0\n  raise(Error.new(\"Failed to load sequence state from #{path}\"))\nend\nresult = Array(Int32).new(token_count.value)\ntoken_count.value.times do |i|\n  result << tokens[i]\nend\nresult\n"}},{"html_id":"seq_save_file(path:String,seq_id:Int32,tokens:Array(Int32)):LibC::SizeT-instance-method","name":"seq_save_file","doc":"Saves a sequence's state to a file\n\nParameters:\n- path: Path to save the sequence file\n- seq_id: The sequence ID to save\n- tokens: Array of tokens to save with the state\n\nReturns:\n- The number of bytes written, or 0 if failed","summary":"<p>Saves a sequence's state to a file</p>","abstract":false,"args":[{"name":"path","external_name":"path","restriction":"String"},{"name":"seq_id","external_name":"seq_id","restriction":"Int32"},{"name":"tokens","external_name":"tokens","restriction":"Array(Int32)"}],"args_string":"(path : String, seq_id : Int32, tokens : Array(Int32)) : LibC::SizeT","args_html":"(path : String, seq_id : Int32, tokens : Array(Int32)) : LibC::SizeT","location":{"filename":"src/llama/state.cr","line_number":170,"url":"https://github.com/kojix2/llama.cr/blob/114c9694ea5c084fb272373b545d065f36513c0c/src/llama/state.cr#L170"},"def":{"name":"seq_save_file","args":[{"name":"path","external_name":"path","restriction":"String"},{"name":"seq_id","external_name":"seq_id","restriction":"Int32"},{"name":"tokens","external_name":"tokens","restriction":"Array(Int32)"}],"return_type":"LibC::SizeT","visibility":"Public","body":"tokens_ptr = tokens.to_unsafe\nLibLlama.llama_state_seq_save_file(@ctx.to_unsafe, path, seq_id, tokens_ptr, tokens.size)\n"}},{"html_id":"seq_set_data(data:Bytes,dest_seq_id:Int32):LibC::SizeT-instance-method","name":"seq_set_data","doc":"Sets the state for a specific sequence from data\n\nParameters:\n- data: The state data to set\n- dest_seq_id: The destination sequence ID\n\nReturns:\n- The number of bytes read, or 0 if failed","summary":"<p>Sets the state for a specific sequence from data</p>","abstract":false,"args":[{"name":"data","external_name":"data","restriction":"Bytes"},{"name":"dest_seq_id","external_name":"dest_seq_id","restriction":"Int32"}],"args_string":"(data : Bytes, dest_seq_id : Int32) : LibC::SizeT","args_html":"(data : Bytes, dest_seq_id : Int32) : LibC::SizeT","location":{"filename":"src/llama/state.cr","line_number":152,"url":"https://github.com/kojix2/llama.cr/blob/114c9694ea5c084fb272373b545d065f36513c0c/src/llama/state.cr#L152"},"def":{"name":"seq_set_data","args":[{"name":"data","external_name":"data","restriction":"Bytes"},{"name":"dest_seq_id","external_name":"dest_seq_id","restriction":"Int32"}],"return_type":"LibC::SizeT","visibility":"Public","body":"LibLlama.llama_state_seq_set_data(@ctx.to_unsafe, data.to_unsafe, data.size, dest_seq_id)"}},{"html_id":"seq_size(seq_id:Int32):LibC::SizeT-instance-method","name":"seq_size","doc":"Gets the size needed to store a specific sequence's state\n\nParameters:\n- seq_id: The sequence ID\n\nReturns:\n- The size in bytes","summary":"<p>Gets the size needed to store a specific sequence's state</p>","abstract":false,"args":[{"name":"seq_id","external_name":"seq_id","restriction":"Int32"}],"args_string":"(seq_id : Int32) : LibC::SizeT","args_html":"(seq_id : Int32) : LibC::SizeT","location":{"filename":"src/llama/state.cr","line_number":114,"url":"https://github.com/kojix2/llama.cr/blob/114c9694ea5c084fb272373b545d065f36513c0c/src/llama/state.cr#L114"},"def":{"name":"seq_size","args":[{"name":"seq_id","external_name":"seq_id","restriction":"Int32"}],"return_type":"LibC::SizeT","visibility":"Public","body":"LibLlama.llama_state_seq_get_size(@ctx.to_unsafe, seq_id)"}},{"html_id":"set_data(data:Bytes):LibC::SizeT-instance-method","name":"set_data","doc":"Sets the state from data\n\nParameters:\n- data: The state data to set\n\nReturns:\n- The number of bytes read","summary":"<p>Sets the state from data</p>","abstract":false,"args":[{"name":"data","external_name":"data","restriction":"Bytes"}],"args_string":"(data : Bytes) : LibC::SizeT","args_html":"(data : Bytes) : LibC::SizeT","location":{"filename":"src/llama/state.cr","line_number":45,"url":"https://github.com/kojix2/llama.cr/blob/114c9694ea5c084fb272373b545d065f36513c0c/src/llama/state.cr#L45"},"def":{"name":"set_data","args":[{"name":"data","external_name":"data","restriction":"Bytes"}],"return_type":"LibC::SizeT","visibility":"Public","body":"LibLlama.llama_state_set_data(@ctx.to_unsafe, data.to_unsafe, data.size)"}},{"html_id":"size:LibC::SizeT-instance-method","name":"size","doc":"Returns the size in bytes needed to store the current state\n\nReturns:\n- The size in bytes","summary":"<p>Returns the size in bytes needed to store the current state</p>","abstract":false,"location":{"filename":"src/llama/state.cr","line_number":16,"url":"https://github.com/kojix2/llama.cr/blob/114c9694ea5c084fb272373b545d065f36513c0c/src/llama/state.cr#L16"},"def":{"name":"size","return_type":"LibC::SizeT","visibility":"Public","body":"LibLlama.llama_state_get_size(@ctx.to_unsafe)"}}]},{"html_id":"llama/Llama/TempSampler","path":"Llama/TempSampler.html","kind":"class","full_name":"Llama::TempSampler","name":"TempSampler","abstract":false,"superclass":{"html_id":"llama/Llama/Sampler","kind":"class","full_name":"Llama::Sampler","name":"Sampler"},"ancestors":[{"html_id":"llama/Llama/Sampler","kind":"class","full_name":"Llama::Sampler","name":"Sampler"},{"html_id":"llama/Reference","kind":"class","full_name":"Reference","name":"Reference"},{"html_id":"llama/Object","kind":"class","full_name":"Object","name":"Object"}],"locations":[{"filename":"src/llama/sampler.cr","line_number":103,"url":"https://github.com/kojix2/llama.cr/blob/114c9694ea5c084fb272373b545d065f36513c0c/src/llama/sampler.cr#L103"}],"repository_name":"llama","program":false,"enum":false,"alias":false,"const":false,"namespace":{"html_id":"llama/Llama","kind":"module","full_name":"Llama","name":"Llama"},"doc":"Temperature sampler","summary":"<p>Temperature sampler</p>","constructors":[{"html_id":"new(temp:Float32)-class-method","name":"new","doc":"Creates a new temperature sampler\n\nParameters:\n- temp: The temperature value (0.0 = greedy, 1.0 = normal, >1.0 = more random)\n\nRaises:\n- Llama::Error if the sampler cannot be created","summary":"<p>Creates a new temperature sampler</p>","abstract":false,"args":[{"name":"temp","external_name":"temp","restriction":"Float32"}],"args_string":"(temp : Float32)","args_html":"(temp : Float32)","location":{"filename":"src/llama/sampler.cr","line_number":111,"url":"https://github.com/kojix2/llama.cr/blob/114c9694ea5c084fb272373b545d065f36513c0c/src/llama/sampler.cr#L111"},"def":{"name":"new","args":[{"name":"temp","external_name":"temp","restriction":"Float32"}],"visibility":"Public","body":"_ = allocate\n_.initialize(temp)\nif _.responds_to?(:finalize)\n  ::GC.add_finalizer(_)\nend\n_\n"}}]},{"html_id":"llama/Llama/TopKSampler","path":"Llama/TopKSampler.html","kind":"class","full_name":"Llama::TopKSampler","name":"TopKSampler","abstract":false,"superclass":{"html_id":"llama/Llama/Sampler","kind":"class","full_name":"Llama::Sampler","name":"Sampler"},"ancestors":[{"html_id":"llama/Llama/Sampler","kind":"class","full_name":"Llama::Sampler","name":"Sampler"},{"html_id":"llama/Reference","kind":"class","full_name":"Reference","name":"Reference"},{"html_id":"llama/Object","kind":"class","full_name":"Object","name":"Object"}],"locations":[{"filename":"src/llama/sampler.cr","line_number":72,"url":"https://github.com/kojix2/llama.cr/blob/114c9694ea5c084fb272373b545d065f36513c0c/src/llama/sampler.cr#L72"}],"repository_name":"llama","program":false,"enum":false,"alias":false,"const":false,"namespace":{"html_id":"llama/Llama","kind":"module","full_name":"Llama","name":"Llama"},"doc":"Top-K sampler","summary":"<p>Top-K sampler</p>","constructors":[{"html_id":"new(k:Int32)-class-method","name":"new","doc":"Creates a new Top-K sampler\n\nParameters:\n- k: The number of top tokens to consider\n\nRaises:\n- Llama::Error if the sampler cannot be created","summary":"<p>Creates a new Top-K sampler</p>","abstract":false,"args":[{"name":"k","external_name":"k","restriction":"Int32"}],"args_string":"(k : Int32)","args_html":"(k : Int32)","location":{"filename":"src/llama/sampler.cr","line_number":80,"url":"https://github.com/kojix2/llama.cr/blob/114c9694ea5c084fb272373b545d065f36513c0c/src/llama/sampler.cr#L80"},"def":{"name":"new","args":[{"name":"k","external_name":"k","restriction":"Int32"}],"visibility":"Public","body":"_ = allocate\n_.initialize(k)\nif _.responds_to?(:finalize)\n  ::GC.add_finalizer(_)\nend\n_\n"}}]},{"html_id":"llama/Llama/TopPSampler","path":"Llama/TopPSampler.html","kind":"class","full_name":"Llama::TopPSampler","name":"TopPSampler","abstract":false,"superclass":{"html_id":"llama/Llama/Sampler","kind":"class","full_name":"Llama::Sampler","name":"Sampler"},"ancestors":[{"html_id":"llama/Llama/Sampler","kind":"class","full_name":"Llama::Sampler","name":"Sampler"},{"html_id":"llama/Reference","kind":"class","full_name":"Reference","name":"Reference"},{"html_id":"llama/Object","kind":"class","full_name":"Object","name":"Object"}],"locations":[{"filename":"src/llama/sampler.cr","line_number":87,"url":"https://github.com/kojix2/llama.cr/blob/114c9694ea5c084fb272373b545d065f36513c0c/src/llama/sampler.cr#L87"}],"repository_name":"llama","program":false,"enum":false,"alias":false,"const":false,"namespace":{"html_id":"llama/Llama","kind":"module","full_name":"Llama","name":"Llama"},"doc":"Top-P (nucleus) sampler","summary":"<p>Top-P (nucleus) sampler</p>","constructors":[{"html_id":"new(p:Float32,min_keep:Int32=1)-class-method","name":"new","doc":"Creates a new Top-P sampler\n\nParameters:\n- p: The cumulative probability threshold (0.0 to 1.0)\n- min_keep: Minimum number of tokens to keep\n\nRaises:\n- Llama::Error if the sampler cannot be created","summary":"<p>Creates a new Top-P sampler</p>","abstract":false,"args":[{"name":"p","external_name":"p","restriction":"Float32"},{"name":"min_keep","default_value":"1","external_name":"min_keep","restriction":"Int32"}],"args_string":"(p : Float32, min_keep : Int32 = 1)","args_html":"(p : Float32, min_keep : Int32 = <span class=\"n\">1</span>)","location":{"filename":"src/llama/sampler.cr","line_number":96,"url":"https://github.com/kojix2/llama.cr/blob/114c9694ea5c084fb272373b545d065f36513c0c/src/llama/sampler.cr#L96"},"def":{"name":"new","args":[{"name":"p","external_name":"p","restriction":"Float32"},{"name":"min_keep","default_value":"1","external_name":"min_keep","restriction":"Int32"}],"visibility":"Public","body":"_ = allocate\n_.initialize(p, min_keep)\nif _.responds_to?(:finalize)\n  ::GC.add_finalizer(_)\nend\n_\n"}}]},{"html_id":"llama/Llama/Vocab","path":"Llama/Vocab.html","kind":"class","full_name":"Llama::Vocab","name":"Vocab","abstract":false,"superclass":{"html_id":"llama/Reference","kind":"class","full_name":"Reference","name":"Reference"},"ancestors":[{"html_id":"llama/Reference","kind":"class","full_name":"Reference","name":"Reference"},{"html_id":"llama/Object","kind":"class","full_name":"Object","name":"Object"}],"locations":[{"filename":"src/llama/vocab.cr","line_number":3,"url":"https://github.com/kojix2/llama.cr/blob/114c9694ea5c084fb272373b545d065f36513c0c/src/llama/vocab.cr#L3"}],"repository_name":"llama","program":false,"enum":false,"alias":false,"const":false,"namespace":{"html_id":"llama/Llama","kind":"module","full_name":"Llama","name":"Llama"},"doc":"Wrapper for the llama_vocab structure","summary":"<p>Wrapper for the llama_vocab structure</p>","constructors":[{"html_id":"new(handle:Pointer(LibLlama::LlamaVocab))-class-method","name":"new","doc":"Creates a new Vocab instance from a raw pointer\n\nNote: This constructor is intended for internal use.\nUsers should obtain Vocab instances through Model#vocab.","summary":"<p>Creates a new Vocab instance from a raw pointer</p>","abstract":false,"args":[{"name":"handle","external_name":"handle","restriction":"::Pointer(LibLlama::LlamaVocab)"}],"args_string":"(handle : Pointer(LibLlama::LlamaVocab))","args_html":"(handle : Pointer(LibLlama::LlamaVocab))","location":{"filename":"src/llama/vocab.cr","line_number":8,"url":"https://github.com/kojix2/llama.cr/blob/114c9694ea5c084fb272373b545d065f36513c0c/src/llama/vocab.cr#L8"},"def":{"name":"new","args":[{"name":"handle","external_name":"handle","restriction":"::Pointer(LibLlama::LlamaVocab)"}],"visibility":"Public","body":"_ = allocate\n_.initialize(handle)\nif _.responds_to?(:finalize)\n  ::GC.add_finalizer(_)\nend\n_\n"}}],"instance_methods":[{"html_id":"bos:Int32-instance-method","name":"bos","doc":"Returns the beginning-of-sentence token ID","summary":"<p>Returns the beginning-of-sentence token ID</p>","abstract":false,"location":{"filename":"src/llama/vocab.cr","line_number":66,"url":"https://github.com/kojix2/llama.cr/blob/114c9694ea5c084fb272373b545d065f36513c0c/src/llama/vocab.cr#L66"},"def":{"name":"bos","return_type":"Int32","visibility":"Public","body":"LibLlama.llama_vocab_bos(@handle)"}},{"html_id":"eos:Int32-instance-method","name":"eos","doc":"Returns the end-of-sentence token ID","summary":"<p>Returns the end-of-sentence token ID</p>","abstract":false,"location":{"filename":"src/llama/vocab.cr","line_number":71,"url":"https://github.com/kojix2/llama.cr/blob/114c9694ea5c084fb272373b545d065f36513c0c/src/llama/vocab.cr#L71"},"def":{"name":"eos","return_type":"Int32","visibility":"Public","body":"LibLlama.llama_vocab_eos(@handle)"}},{"html_id":"eot:Int32-instance-method","name":"eot","doc":"Returns the end-of-turn token ID","summary":"<p>Returns the end-of-turn token ID</p>","abstract":false,"location":{"filename":"src/llama/vocab.cr","line_number":76,"url":"https://github.com/kojix2/llama.cr/blob/114c9694ea5c084fb272373b545d065f36513c0c/src/llama/vocab.cr#L76"},"def":{"name":"eot","return_type":"Int32","visibility":"Public","body":"LibLlama.llama_vocab_eot(@handle)"}},{"html_id":"n_tokens:Int32-instance-method","name":"n_tokens","doc":"Returns the number of tokens in the vocabulary","summary":"<p>Returns the number of tokens in the vocabulary</p>","abstract":false,"location":{"filename":"src/llama/vocab.cr","line_number":12,"url":"https://github.com/kojix2/llama.cr/blob/114c9694ea5c084fb272373b545d065f36513c0c/src/llama/vocab.cr#L12"},"def":{"name":"n_tokens","return_type":"Int32","visibility":"Public","body":"LibLlama.llama_vocab_n_tokens(@handle)"}},{"html_id":"nl:Int32-instance-method","name":"nl","doc":"Returns the newline token ID","summary":"<p>Returns the newline token ID</p>","abstract":false,"location":{"filename":"src/llama/vocab.cr","line_number":81,"url":"https://github.com/kojix2/llama.cr/blob/114c9694ea5c084fb272373b545d065f36513c0c/src/llama/vocab.cr#L81"},"def":{"name":"nl","return_type":"Int32","visibility":"Public","body":"LibLlama.llama_vocab_nl(@handle)"}},{"html_id":"pad:Int32-instance-method","name":"pad","doc":"Returns the padding token ID","summary":"<p>Returns the padding token ID</p>","abstract":false,"location":{"filename":"src/llama/vocab.cr","line_number":86,"url":"https://github.com/kojix2/llama.cr/blob/114c9694ea5c084fb272373b545d065f36513c0c/src/llama/vocab.cr#L86"},"def":{"name":"pad","return_type":"Int32","visibility":"Public","body":"LibLlama.llama_vocab_pad(@handle)"}},{"html_id":"to_unsafe:Pointer(Llama::LibLlama::LlamaVocab)-instance-method","name":"to_unsafe","doc":"Returns the raw pointer to the underlying llama_vocab structure","summary":"<p>Returns the raw pointer to the underlying llama_vocab structure</p>","abstract":false,"location":{"filename":"src/llama/vocab.cr","line_number":91,"url":"https://github.com/kojix2/llama.cr/blob/114c9694ea5c084fb272373b545d065f36513c0c/src/llama/vocab.cr#L91"},"def":{"name":"to_unsafe","visibility":"Public","body":"@handle"}},{"html_id":"token_to_text(token:Int32):String-instance-method","name":"token_to_text","doc":"Returns the text representation of a token","summary":"<p>Returns the text representation of a token</p>","abstract":false,"args":[{"name":"token","external_name":"token","restriction":"Int32"}],"args_string":"(token : Int32) : String","args_html":"(token : Int32) : String","location":{"filename":"src/llama/vocab.cr","line_number":17,"url":"https://github.com/kojix2/llama.cr/blob/114c9694ea5c084fb272373b545d065f36513c0c/src/llama/vocab.cr#L17"},"def":{"name":"token_to_text","args":[{"name":"token","external_name":"token","restriction":"Int32"}],"return_type":"String","visibility":"Public","body":"ptr = LibLlama.llama_vocab_get_text(@handle, token)\nString.new(ptr)\n"}},{"html_id":"tokenize(text:String,add_special:Bool=true,parse_special:Bool=true):Array(Int32)-instance-method","name":"tokenize","doc":"Tokenizes a string into an array of token IDs","summary":"<p>Tokenizes a string into an array of token IDs</p>","abstract":false,"args":[{"name":"text","external_name":"text","restriction":"String"},{"name":"add_special","default_value":"true","external_name":"add_special","restriction":"Bool"},{"name":"parse_special","default_value":"true","external_name":"parse_special","restriction":"Bool"}],"args_string":"(text : String, add_special : Bool = true, parse_special : Bool = true) : Array(Int32)","args_html":"(text : String, add_special : Bool = <span class=\"n\">true</span>, parse_special : Bool = <span class=\"n\">true</span>) : Array(Int32)","location":{"filename":"src/llama/vocab.cr","line_number":23,"url":"https://github.com/kojix2/llama.cr/blob/114c9694ea5c084fb272373b545d065f36513c0c/src/llama/vocab.cr#L23"},"def":{"name":"tokenize","args":[{"name":"text","external_name":"text","restriction":"String"},{"name":"add_special","default_value":"true","external_name":"add_special","restriction":"Bool"},{"name":"parse_special","default_value":"true","external_name":"parse_special","restriction":"Bool"}],"return_type":"Array(Int32)","visibility":"Public","body":"max_tokens = text.size * 2\ntokens = Pointer(LibLlama::LlamaToken).malloc(max_tokens)\nn_tokens = LibLlama.llama_tokenize(@handle, text, text.bytesize, tokens, max_tokens, add_special, parse_special)\nif n_tokens < 0\n  max_tokens = -n_tokens\n  tokens = Pointer(LibLlama::LlamaToken).malloc(max_tokens)\n  n_tokens = LibLlama.llama_tokenize(@handle, text, text.bytesize, tokens, max_tokens, add_special, parse_special)\nend\nif n_tokens < 0\n  raise(Error.new(\"Failed to tokenize text\"))\nend\nresult = Array(Int32).new(n_tokens)\nn_tokens.times do |i|\n  result << tokens[i]\nend\nresult\n"}}]}]}]}})