crystal_doc_search_index_callback({"repository_name":"llama","body":"# llama.cr\n\n[![test](https://github.com/kojix2/llama.cr/actions/workflows/test.yml/badge.svg)](https://github.com/kojix2/llama.cr/actions/workflows/test.yml)\n[![docs](https://img.shields.io/badge/docs-latest-blue.svg)](https://kojix2.github.io/llama.cr)\n\nCrystal bindings for [llama.cpp](https://github.com/ggml-org/llama.cpp), a C/C++ implementation of LLaMA, Falcon, GPT-2, and other large language models.\n\n## Features\n\n- Low-level bindings to the llama.cpp C API\n- High-level Crystal wrapper classes for easy usage\n- Memory management for C resources\n- Simple text generation interface\n\n## Installation\n\n### Prerequisites\n\nBefore using this shard, you need to have llama.cpp compiled and installed on your system:\n\n1. Clone and build llama.cpp using CMake:\n```bash\ngit clone https://github.com/ggml-org/llama.cpp.git\ncd llama.cpp\nmkdir build && cd build\ncmake ..\ncmake --build . --config Release\n```\n\n2. Install the library to your system:\n```bash\n# On Linux\nsudo cmake --install .\nsudo ldconfig\n\n# On macOS\nsudo cmake --install .\n```\n\n3. Alternatively, you can specify the library location without installing:\n```bash\n# Set the library path at compile time\ncrystal build examples/text_generation.cr --link-flags=\"-L/path/to/llama.cpp/build/bin\"\n\n# Or set the library path at runtime\nLD_LIBRARY_PATH=/path/to/llama.cpp/build/bin ./text_generation /path/to/model.gguf \"Your prompt here\"\n```\n\n### Obtaining GGUF Model Files\n\nYou'll need a model file in GGUF format to use with this library. Here are some options:\n\n1. **Download pre-converted models from Hugging Face**:\n\n   - [TheBloke's GGUF models](https://huggingface.co/TheBloke) - Large collection of models converted to GGUF format\n   - Popular models include:\n     - [Llama 3 8B Instruct](https://huggingface.co/TheBloke/Llama-3-8B-Instruct-GGUF)\n     - [Mistral 7B Instruct v0.2](https://huggingface.co/TheBloke/Mistral-7B-Instruct-v0.2-GGUF)\n     - [TinyLlama 1.1B](https://huggingface.co/TheBloke/TinyLlama-1.1B-Chat-v1.0-GGUF) (good for testing)\n\n2. **Convert models yourself using llama.cpp**:\n\n```bash\n# Example: Converting a Hugging Face model to GGUF\npython3 llama.cpp/convert.py /path/to/model --outfile model.gguf\n```\n\nFor testing, smaller quantized models (1-3B parameters) with Q4_K_M quantization are recommended.\n\n### Adding to Your Project\n\n1. Add the dependency to your `shard.yml`:\n\n```yaml\ndependencies:\n  llama:\n    github: kojix2/llama.cr\n```\n\n2. Run `shards install`\n\n## Usage\n\n```crystal\nrequire \"llama\"\n\n# Load a model\nmodel = Llama::Model.new(\"/path/to/model.gguf\")\n\n# Create a context\ncontext = model.context\n\n# Generate text\nresponse = context.generate(\"Once upon a time\", max_tokens: 100, temperature: 0.8)\nputs response\n\n# Or use the convenience method\nresponse = Llama.generate(\"/path/to/model.gguf\", \"Once upon a time\")\nputs response\n```\n\n## Examples\n\nThe `examples` directory contains sample code demonstrating how to use the library:\n\n### Text Generation\n\nA simple example showing how to generate text from a prompt:\n\n```bash\ncrystal build examples/text_generation.cr --link-flags=\"-L/path/to/llama.cpp/build/bin\"\nLD_LIBRARY_PATH=/path/to/llama.cpp/build/bin ./text_generation /path/to/model.gguf \"Once upon a time\"\n```\n\nOptional parameters:\n\n- Third argument: Maximum number of tokens to generate (default: 128)\n- Fourth argument: Temperature (default: 0.8)\n\n### Tokenization\n\nAn example demonstrating tokenization and vocabulary features:\n\n```bash\ncrystal build examples/tokenization.cr --link-flags=\"-L/path/to/llama.cpp/build/bin\"\nLD_LIBRARY_PATH=/path/to/llama.cpp/build/bin ./tokenization /path/to/model.gguf \"Hello, world!\"\n```\n\nThis example shows:\n\n- How to tokenize text into token IDs\n- How to convert token IDs back to text\n- How to access special tokens in the vocabulary\n\n## API Documentation\n\n### Llama::Model\n\nThe `Model` class represents a loaded LLaMA model.\n\n```crystal\n# Load a model\nmodel = Llama::Model.new(\"/path/to/model.gguf\")\n\n# Get model information\nputs model.n_params  # Number of parameters\nputs model.n_embd    # Embedding size\nputs model.n_layer   # Number of layers\nputs model.n_head    # Number of attention heads\n```\n\n### Llama::Context\n\nThe `Context` class handles the inference state for a model.\n\n```crystal\n# Create a context\ncontext = model.context\n\n# Generate text\nresponse = context.generate(\"Hello, I am a\", max_tokens: 50, temperature: 0.7)\n```\n\n### Llama::Vocab\n\nThe `Vocab` class provides access to the model's vocabulary.\n\n```crystal\n# Get the vocabulary\nvocab = model.vocab\n\n# Tokenize text\ntokens = vocab.tokenize(\"Hello, world!\")\n\n# Convert tokens back to text\ntext = tokens.map { |token| vocab.token_to_text(token) }.join\n```\n\n## Development\n\nSee [DEVELOPMENT.md](DEVELOPMENT.md) for development guidelines.\n\n## Contributing\n\n1. Fork it (<https://github.com/kojix2/llama.cr/fork>)\n2. Create your feature branch (`git checkout -b my-new-feature`)\n3. Commit your changes (`git commit -am 'Add some feature'`)\n4. Push to the branch (`git push origin my-new-feature`)\n5. Create a new Pull Request\n\n## License\n\nThis project is available under the MIT License. See the LICENSE file for more info.\n","program":{"html_id":"llama/toplevel","path":"toplevel.html","kind":"module","full_name":"Top Level Namespace","name":"Top Level Namespace","abstract":false,"locations":[],"repository_name":"llama","program":true,"enum":false,"alias":false,"const":false,"types":[{"html_id":"llama/Llama","path":"Llama.html","kind":"module","full_name":"Llama","name":"Llama","abstract":false,"locations":[{"filename":"src/llama.cr","line_number":12,"url":"https://github.com/kojix2/llama.cr/blob/e008ed302d03fcb414ca8352f854013484b58b19/src/llama.cr#L12"},{"filename":"src/llama/chat.cr","line_number":1,"url":"https://github.com/kojix2/llama.cr/blob/e008ed302d03fcb414ca8352f854013484b58b19/src/llama/chat.cr#L1"},{"filename":"src/llama/context.cr","line_number":1,"url":"https://github.com/kojix2/llama.cr/blob/e008ed302d03fcb414ca8352f854013484b58b19/src/llama/context.cr#L1"},{"filename":"src/llama/lib_llama.cr","line_number":1,"url":"https://github.com/kojix2/llama.cr/blob/e008ed302d03fcb414ca8352f854013484b58b19/src/llama/lib_llama.cr#L1"},{"filename":"src/llama/model.cr","line_number":1,"url":"https://github.com/kojix2/llama.cr/blob/e008ed302d03fcb414ca8352f854013484b58b19/src/llama/model.cr#L1"},{"filename":"src/llama/sampler.cr","line_number":1,"url":"https://github.com/kojix2/llama.cr/blob/e008ed302d03fcb414ca8352f854013484b58b19/src/llama/sampler.cr#L1"},{"filename":"src/llama/vocab.cr","line_number":1,"url":"https://github.com/kojix2/llama.cr/blob/e008ed302d03fcb414ca8352f854013484b58b19/src/llama/vocab.cr#L1"}],"repository_name":"llama","program":false,"enum":false,"alias":false,"const":false,"constants":[{"id":"VERSION","name":"VERSION","value":"\"0.1.0\""}],"class_methods":[{"html_id":"apply_chat_template(template:String|Nil,messages:Array(ChatMessage),add_assistant:Bool=true):String-class-method","name":"apply_chat_template","doc":"Applies a chat template to a list of messages\n\nParameters:\n- template: The template string (nil to use model's default)\n- messages: Array of chat messages\n- add_assistant: Whether to end with an assistant message prefix\n\nReturns:\n- The formatted prompt string\n\nRaises:\n- Llama::Error if template application fails","summary":"<p>Applies a chat template to a list of messages</p>","abstract":false,"args":[{"name":"template","external_name":"template","restriction":"String | ::Nil"},{"name":"messages","external_name":"messages","restriction":"Array(ChatMessage)"},{"name":"add_assistant","default_value":"true","external_name":"add_assistant","restriction":"Bool"}],"args_string":"(template : String | Nil, messages : Array(ChatMessage), add_assistant : Bool = true) : String","args_html":"(template : String | Nil, messages : Array(<a href=\"Llama/ChatMessage.html\">ChatMessage</a>), add_assistant : Bool = <span class=\"n\">true</span>) : String","location":{"filename":"src/llama/chat.cr","line_number":39,"url":"https://github.com/kojix2/llama.cr/blob/e008ed302d03fcb414ca8352f854013484b58b19/src/llama/chat.cr#L39"},"def":{"name":"apply_chat_template","args":[{"name":"template","external_name":"template","restriction":"String | ::Nil"},{"name":"messages","external_name":"messages","restriction":"Array(ChatMessage)"},{"name":"add_assistant","default_value":"true","external_name":"add_assistant","restriction":"Bool"}],"return_type":"String","visibility":"Public","body":"c_messages = messages.map(&.to_unsafe)\nestimated_size = messages.sum do |msg|\n  msg.content.size + msg.role.size\nend * 2\nbuffer = Pointer(LibC::Char).malloc(estimated_size)\nresult = LibLlama.llama_chat_apply_template((template || \"\").to_unsafe, c_messages.to_unsafe, messages.size, add_assistant, buffer, estimated_size)\nif result > estimated_size\n  buffer = Pointer(LibC::Char).malloc(result)\n  result = LibLlama.llama_chat_apply_template((template || \"\").to_unsafe, c_messages.to_unsafe, messages.size, add_assistant, buffer, result)\nend\nif result < 0\n  raise(Error.new(\"Failed to apply chat template\"))\nend\nString.new(buffer, result)\n"}},{"html_id":"builtin_chat_templates:Array(String)-class-method","name":"builtin_chat_templates","doc":"Gets the list of built-in chat templates\n\nReturns:\n- Array of template names","summary":"<p>Gets the list of built-in chat templates</p>","abstract":false,"location":{"filename":"src/llama/chat.cr","line_number":85,"url":"https://github.com/kojix2/llama.cr/blob/e008ed302d03fcb414ca8352f854013484b58b19/src/llama/chat.cr#L85"},"def":{"name":"builtin_chat_templates","return_type":"Array(String)","visibility":"Public","body":"output = Pointer(::Pointer(LibC::Char)).malloc(100)\ncount = LibLlama.llama_chat_builtin_templates(output, 100)\nresult = [] of String\ncount.times do |i|\n  result << (String.new(output[i]))\nend\nresult\n"}},{"html_id":"generate(model_path:String,prompt:String,max_tokens:Int32=128,temperature:Float32=0.8):String-class-method","name":"generate","doc":"A simple example of text generation\n\nParameters:\n- model_path: Path to the model file (.gguf format)\n- prompt: The input prompt\n- max_tokens: Maximum number of tokens to generate (must be positive)\n- temperature: Sampling temperature (0.0 = greedy, 1.0 = more random)\n\nReturns:\n- The generated text\n\nRaises:\n- ArgumentError if parameters are invalid\n- Llama::Error if model loading or text generation fails","summary":"<p>A simple example of text generation</p>","abstract":false,"args":[{"name":"model_path","external_name":"model_path","restriction":"String"},{"name":"prompt","external_name":"prompt","restriction":"String"},{"name":"max_tokens","default_value":"128","external_name":"max_tokens","restriction":"Int32"},{"name":"temperature","default_value":"0.8","external_name":"temperature","restriction":"Float32"}],"args_string":"(model_path : String, prompt : String, max_tokens : Int32 = 128, temperature : Float32 = 0.8) : String","args_html":"(model_path : String, prompt : String, max_tokens : Int32 = <span class=\"n\">128</span>, temperature : Float32 = <span class=\"n\">0.8</span>) : String","location":{"filename":"src/llama.cr","line_number":34,"url":"https://github.com/kojix2/llama.cr/blob/e008ed302d03fcb414ca8352f854013484b58b19/src/llama.cr#L34"},"def":{"name":"generate","args":[{"name":"model_path","external_name":"model_path","restriction":"String"},{"name":"prompt","external_name":"prompt","restriction":"String"},{"name":"max_tokens","default_value":"128","external_name":"max_tokens","restriction":"Int32"},{"name":"temperature","default_value":"0.8","external_name":"temperature","restriction":"Float32"}],"return_type":"String","visibility":"Public","body":"if max_tokens <= 0\n  raise(ArgumentError.new(\"max_tokens must be positive\"))\nend\nif temperature < 0\n  raise(ArgumentError.new(\"temperature must be non-negative\"))\nend\nmodel = Model.new(model_path)\ncontext = model.context\ncontext.generate(prompt, max_tokens, temperature)\n"}},{"html_id":"system_info:String-class-method","name":"system_info","doc":"Returns the llama.cpp system information","summary":"<p>Returns the llama.cpp system information</p>","abstract":false,"location":{"filename":"src/llama.cr","line_number":16,"url":"https://github.com/kojix2/llama.cr/blob/e008ed302d03fcb414ca8352f854013484b58b19/src/llama.cr#L16"},"def":{"name":"system_info","return_type":"String","visibility":"Public","body":"String.new(LibLlama.llama_print_system_info)"}}],"types":[{"html_id":"llama/Llama/ChatMessage","path":"Llama/ChatMessage.html","kind":"class","full_name":"Llama::ChatMessage","name":"ChatMessage","abstract":false,"superclass":{"html_id":"llama/Reference","kind":"class","full_name":"Reference","name":"Reference"},"ancestors":[{"html_id":"llama/Reference","kind":"class","full_name":"Reference","name":"Reference"},{"html_id":"llama/Object","kind":"class","full_name":"Object","name":"Object"}],"locations":[{"filename":"src/llama/chat.cr","line_number":3,"url":"https://github.com/kojix2/llama.cr/blob/e008ed302d03fcb414ca8352f854013484b58b19/src/llama/chat.cr#L3"}],"repository_name":"llama","program":false,"enum":false,"alias":false,"const":false,"namespace":{"html_id":"llama/Llama","kind":"module","full_name":"Llama","name":"Llama"},"doc":"Represents a message in a chat conversation","summary":"<p>Represents a message in a chat conversation</p>","constructors":[{"html_id":"new(role:String,content:String)-class-method","name":"new","doc":"Creates a new ChatMessage\n\nParameters:\n- role: The role of the message sender\n- content: The content of the message","summary":"<p>Creates a new ChatMessage</p>","abstract":false,"args":[{"name":"role","external_name":"role","restriction":"String"},{"name":"content","external_name":"content","restriction":"String"}],"args_string":"(role : String, content : String)","args_html":"(role : String, content : String)","location":{"filename":"src/llama/chat.cr","line_number":15,"url":"https://github.com/kojix2/llama.cr/blob/e008ed302d03fcb414ca8352f854013484b58b19/src/llama/chat.cr#L15"},"def":{"name":"new","args":[{"name":"role","external_name":"role","restriction":"String"},{"name":"content","external_name":"content","restriction":"String"}],"visibility":"Public","body":"_ = allocate\n_.initialize(role, content)\nif _.responds_to?(:finalize)\n  ::GC.add_finalizer(_)\nend\n_\n"}}],"instance_methods":[{"html_id":"content:String-instance-method","name":"content","doc":"The content of the message","summary":"<p>The content of the message</p>","abstract":false,"location":{"filename":"src/llama/chat.cr","line_number":8,"url":"https://github.com/kojix2/llama.cr/blob/e008ed302d03fcb414ca8352f854013484b58b19/src/llama/chat.cr#L8"},"def":{"name":"content","return_type":"String","visibility":"Public","body":"@content"}},{"html_id":"content=(content:String)-instance-method","name":"content=","doc":"The content of the message","summary":"<p>The content of the message</p>","abstract":false,"args":[{"name":"content","external_name":"content","restriction":"String"}],"args_string":"(content : String)","args_html":"(content : String)","location":{"filename":"src/llama/chat.cr","line_number":8,"url":"https://github.com/kojix2/llama.cr/blob/e008ed302d03fcb414ca8352f854013484b58b19/src/llama/chat.cr#L8"},"def":{"name":"content=","args":[{"name":"content","external_name":"content","restriction":"String"}],"visibility":"Public","body":"@content = content"}},{"html_id":"role:String-instance-method","name":"role","doc":"The role of the message sender (e.g., \"system\", \"user\", \"assistant\")","summary":"<p>The role of the message sender (e.g., &quot;system&quot;, &quot;user&quot;, &quot;assistant&quot;)</p>","abstract":false,"location":{"filename":"src/llama/chat.cr","line_number":5,"url":"https://github.com/kojix2/llama.cr/blob/e008ed302d03fcb414ca8352f854013484b58b19/src/llama/chat.cr#L5"},"def":{"name":"role","return_type":"String","visibility":"Public","body":"@role"}},{"html_id":"role=(role:String)-instance-method","name":"role=","doc":"The role of the message sender (e.g., \"system\", \"user\", \"assistant\")","summary":"<p>The role of the message sender (e.g., &quot;system&quot;, &quot;user&quot;, &quot;assistant&quot;)</p>","abstract":false,"args":[{"name":"role","external_name":"role","restriction":"String"}],"args_string":"(role : String)","args_html":"(role : String)","location":{"filename":"src/llama/chat.cr","line_number":5,"url":"https://github.com/kojix2/llama.cr/blob/e008ed302d03fcb414ca8352f854013484b58b19/src/llama/chat.cr#L5"},"def":{"name":"role=","args":[{"name":"role","external_name":"role","restriction":"String"}],"visibility":"Public","body":"@role = role"}},{"html_id":"to_unsafe:LibLlama::LlamaChatMessage-instance-method","name":"to_unsafe","doc":"Converts to the C structure","summary":"<p>Converts to the C structure</p>","abstract":false,"location":{"filename":"src/llama/chat.cr","line_number":19,"url":"https://github.com/kojix2/llama.cr/blob/e008ed302d03fcb414ca8352f854013484b58b19/src/llama/chat.cr#L19"},"def":{"name":"to_unsafe","return_type":"LibLlama::LlamaChatMessage","visibility":"Public","body":"msg = LibLlama::LlamaChatMessage.new\nmsg.role = @role.to_unsafe\nmsg.content = @content.to_unsafe\nmsg\n"}}]},{"html_id":"llama/Llama/Context","path":"Llama/Context.html","kind":"class","full_name":"Llama::Context","name":"Context","abstract":false,"superclass":{"html_id":"llama/Reference","kind":"class","full_name":"Reference","name":"Reference"},"ancestors":[{"html_id":"llama/Reference","kind":"class","full_name":"Reference","name":"Reference"},{"html_id":"llama/Object","kind":"class","full_name":"Object","name":"Object"}],"locations":[{"filename":"src/llama/context.cr","line_number":3,"url":"https://github.com/kojix2/llama.cr/blob/e008ed302d03fcb414ca8352f854013484b58b19/src/llama/context.cr#L3"}],"repository_name":"llama","program":false,"enum":false,"alias":false,"const":false,"namespace":{"html_id":"llama/Llama","kind":"module","full_name":"Llama","name":"Llama"},"doc":"Wrapper for the llama_context structure","summary":"<p>Wrapper for the llama_context structure</p>","constructors":[{"html_id":"new(model:Model,params=nil)-class-method","name":"new","doc":"Creates a new Context instance for a model\n\nParameters:\n- model: The Model to create a context for\n- params: Optional context parameters\n\nRaises:\n- Llama::Error if the context cannot be created","summary":"<p>Creates a new Context instance for a model</p>","abstract":false,"args":[{"name":"model","external_name":"model","restriction":"Model"},{"name":"params","default_value":"nil","external_name":"params","restriction":""}],"args_string":"(model : Model, params = nil)","args_html":"(model : <a href=\"../Llama/Model.html\">Model</a>, params = <span class=\"n\">nil</span>)","location":{"filename":"src/llama/context.cr","line_number":12,"url":"https://github.com/kojix2/llama.cr/blob/e008ed302d03fcb414ca8352f854013484b58b19/src/llama/context.cr#L12"},"def":{"name":"new","args":[{"name":"model","external_name":"model","restriction":"Model"},{"name":"params","default_value":"nil","external_name":"params","restriction":""}],"visibility":"Public","body":"_ = allocate\n_.initialize(model, params)\nif _.responds_to?(:finalize)\n  ::GC.add_finalizer(_)\nend\n_\n"}}],"instance_methods":[{"html_id":"chat(messages:Array(ChatMessage),max_tokens:Int32=128,temperature:Float32=0.8,template:String|Nil=nil):String-instance-method","name":"chat","doc":"Generates a response in a chat conversation\n\nParameters:\n- messages: Array of chat messages\n- max_tokens: Maximum number of tokens to generate\n- temperature: Sampling temperature\n- template: Optional chat template (nil to use model's default)\n\nReturns:\n- The generated response text\n\nRaises:\n- ArgumentError if parameters are invalid\n- Llama::Error if text generation fails","summary":"<p>Generates a response in a chat conversation</p>","abstract":false,"args":[{"name":"messages","external_name":"messages","restriction":"Array(ChatMessage)"},{"name":"max_tokens","default_value":"128","external_name":"max_tokens","restriction":"Int32"},{"name":"temperature","default_value":"0.8","external_name":"temperature","restriction":"Float32"},{"name":"template","default_value":"nil","external_name":"template","restriction":"String | ::Nil"}],"args_string":"(messages : Array(ChatMessage), max_tokens : Int32 = 128, temperature : Float32 = 0.8, template : String | Nil = nil) : String","args_html":"(messages : Array(<a href=\"../Llama/ChatMessage.html\">ChatMessage</a>), max_tokens : Int32 = <span class=\"n\">128</span>, temperature : Float32 = <span class=\"n\">0.8</span>, template : String | Nil = <span class=\"n\">nil</span>) : String","location":{"filename":"src/llama/context.cr","line_number":33,"url":"https://github.com/kojix2/llama.cr/blob/e008ed302d03fcb414ca8352f854013484b58b19/src/llama/context.cr#L33"},"def":{"name":"chat","args":[{"name":"messages","external_name":"messages","restriction":"Array(ChatMessage)"},{"name":"max_tokens","default_value":"128","external_name":"max_tokens","restriction":"Int32"},{"name":"temperature","default_value":"0.8","external_name":"temperature","restriction":"Float32"},{"name":"template","default_value":"nil","external_name":"template","restriction":"String | ::Nil"}],"return_type":"String","visibility":"Public","body":"prompt = Llama.apply_chat_template(template || @model.chat_template, messages, true)\ngenerate(prompt, max_tokens, temperature)\n"}},{"html_id":"decode(batch:LibLlama::LlamaBatch):Int32-instance-method","name":"decode","doc":"Processes a batch of tokens\n\nParameters:\n- batch: The batch to process\n\nReturns:\n- 0 on success\n- 1 if no KV slot was found for the batch\n- < 0 on error\n\nRaises:\n- Llama::Error on error","summary":"<p>Processes a batch of tokens</p>","abstract":false,"args":[{"name":"batch","external_name":"batch","restriction":"LibLlama::LlamaBatch"}],"args_string":"(batch : LibLlama::LlamaBatch) : Int32","args_html":"(batch : LibLlama::LlamaBatch) : Int32","location":{"filename":"src/llama/context.cr","line_number":128,"url":"https://github.com/kojix2/llama.cr/blob/e008ed302d03fcb414ca8352f854013484b58b19/src/llama/context.cr#L128"},"def":{"name":"decode","args":[{"name":"batch","external_name":"batch","restriction":"LibLlama::LlamaBatch"}],"return_type":"Int32","visibility":"Public","body":"result = LibLlama.llama_decode(@handle, batch)\nif result < 0\n  raise(Error.new(\"Failed to decode batch\"))\nend\nresult\n"}},{"html_id":"finalize-instance-method","name":"finalize","doc":"Frees the resources associated with this context","summary":"<p>Frees the resources associated with this context</p>","abstract":false,"location":{"filename":"src/llama/context.cr","line_number":300,"url":"https://github.com/kojix2/llama.cr/blob/e008ed302d03fcb414ca8352f854013484b58b19/src/llama/context.cr#L300"},"def":{"name":"finalize","visibility":"Public","body":"if @handle.null?\nelse\n  LibLlama.llama_free(@handle)\nend"}},{"html_id":"generate(prompt:String,max_tokens:Int32=128,temperature:Float32=0.8):String-instance-method","name":"generate","doc":"Generates text from a prompt\n\nParameters:\n- prompt: The input prompt\n- max_tokens: Maximum number of tokens to generate (must be positive)\n- temperature: Sampling temperature (0.0 = greedy, 1.0 = more random)\n\nReturns:\n- The generated text\n\nRaises:\n- ArgumentError if parameters are invalid","summary":"<p>Generates text from a prompt</p>","abstract":false,"args":[{"name":"prompt","external_name":"prompt","restriction":"String"},{"name":"max_tokens","default_value":"128","external_name":"max_tokens","restriction":"Int32"},{"name":"temperature","default_value":"0.8","external_name":"temperature","restriction":"Float32"}],"args_string":"(prompt : String, max_tokens : Int32 = 128, temperature : Float32 = 0.8) : String","args_html":"(prompt : String, max_tokens : Int32 = <span class=\"n\">128</span>, temperature : Float32 = <span class=\"n\">0.8</span>) : String","location":{"filename":"src/llama/context.cr","line_number":154,"url":"https://github.com/kojix2/llama.cr/blob/e008ed302d03fcb414ca8352f854013484b58b19/src/llama/context.cr#L154"},"def":{"name":"generate","args":[{"name":"prompt","external_name":"prompt","restriction":"String"},{"name":"max_tokens","default_value":"128","external_name":"max_tokens","restriction":"Int32"},{"name":"temperature","default_value":"0.8","external_name":"temperature","restriction":"Float32"}],"return_type":"String","visibility":"Public","body":"if max_tokens <= 0\n  raise(ArgumentError.new(\"max_tokens must be positive\"))\nend\nif temperature < 0\n  raise(ArgumentError.new(\"temperature must be non-negative\"))\nend\ninput_tokens = @model.vocab.tokenize(prompt)\nresult = \"\"\nall_tokens = input_tokens.dup\npos = input_tokens.size\nmax_tokens.times do\n  batch = LibLlama::LlamaBatch.new\n  if pos == input_tokens.size\n    batch.n_tokens = input_tokens.size\n    token_ptr = Pointer(LibLlama::LlamaToken).malloc(batch.n_tokens)\n    pos_ptr = Pointer(LibLlama::LlamaPos).malloc(batch.n_tokens)\n    logits_ptr = Pointer(Int8).malloc(batch.n_tokens)\n    input_tokens.each_with_index do |token, i|\n      token_ptr[i] = token\n      pos_ptr[i] = i.to_i32\n      logits_ptr[i] = i == (batch.n_tokens - 1) ? 1_i8 : 0_i8\n    end\n    batch.token = token_ptr\n    batch.pos = pos_ptr\n    batch.logits = logits_ptr\n  else\n    batch.n_tokens = 1\n    token_ptr = Pointer(LibLlama::LlamaToken).malloc(1)\n    pos_ptr = Pointer(LibLlama::LlamaPos).malloc(1)\n    logits_ptr = Pointer(Int8).malloc(1)\n    token_ptr[0] = all_tokens.last\n    pos_ptr[0] = (pos - 1).to_i32\n    logits_ptr[0] = 1_i8\n    batch.token = token_ptr\n    batch.pos = pos_ptr\n    batch.logits = logits_ptr\n  end\n  decode(batch)\n  logits = self.logits\n  next_token = if temperature <= 0.0\n    max_logit = -Float32::INFINITY\n    best_token = 0\n    n_vocab = @model.vocab.n_tokens\n    n_vocab.times do |i|\n      if logits[i] > max_logit\n        max_logit = logits[i]\n        best_token = i\n      end\n    end\n    best_token\n  else\n    n_vocab = @model.vocab.n_tokens\n    probs = Array(Float32).new(n_vocab, 0.0)\n    max_logit = -Float32::INFINITY\n    n_vocab.times do |i|\n      __temp_51 = i\n      logits[__temp_51] = logits[__temp_51] / temperature\n      if logits[i] > max_logit\n        max_logit = logits[i]\n      end\n    end\n    sum = 0.0_f32\n    n_vocab.times do |i|\n      probs[i] = Math.exp(logits[i] - max_logit)\n      sum = sum + probs[i]\n    end\n    n_vocab.times do |i|\n      __temp_53 = i\n      probs[__temp_53] = probs[__temp_53] / sum\n    end\n    r = rand\n    cdf = 0.0_f32\n    token = n_vocab - 1\n    n_vocab.times do |i|\n      cdf = cdf + probs[i]\n      if r < cdf\n        token = i\n        break\n      end\n    end\n    token\n  end\n  eos_token = @model.vocab.eos\n  if next_token == eos_token\n    break\n  end\n  token_text = @model.vocab.token_to_text(next_token)\n  result = result + token_text\n  all_tokens << next_token\n  pos = pos + 1\nend\nresult\n"}},{"html_id":"generate_with_sampler(prompt:String,sampler:SamplerChain,max_tokens:Int32=128):String-instance-method","name":"generate_with_sampler","doc":"Generates text using a sampler chain\n\nParameters:\n- prompt: The input prompt\n- sampler: The sampler chain to use\n- max_tokens: Maximum number of tokens to generate (must be positive)\n\nReturns:\n- The generated text\n\nRaises:\n- ArgumentError if parameters are invalid","summary":"<p>Generates text using a sampler chain</p>","abstract":false,"args":[{"name":"prompt","external_name":"prompt","restriction":"String"},{"name":"sampler","external_name":"sampler","restriction":"SamplerChain"},{"name":"max_tokens","default_value":"128","external_name":"max_tokens","restriction":"Int32"}],"args_string":"(prompt : String, sampler : SamplerChain, max_tokens : Int32 = 128) : String","args_html":"(prompt : String, sampler : <a href=\"../Llama/SamplerChain.html\">SamplerChain</a>, max_tokens : Int32 = <span class=\"n\">128</span>) : String","location":{"filename":"src/llama/context.cr","line_number":62,"url":"https://github.com/kojix2/llama.cr/blob/e008ed302d03fcb414ca8352f854013484b58b19/src/llama/context.cr#L62"},"def":{"name":"generate_with_sampler","args":[{"name":"prompt","external_name":"prompt","restriction":"String"},{"name":"sampler","external_name":"sampler","restriction":"SamplerChain"},{"name":"max_tokens","default_value":"128","external_name":"max_tokens","restriction":"Int32"}],"return_type":"String","visibility":"Public","body":"if max_tokens <= 0\n  raise(ArgumentError.new(\"max_tokens must be positive\"))\nend\ninput_tokens = @model.vocab.tokenize(prompt)\nresult = \"\"\nall_tokens = input_tokens.dup\npos = input_tokens.size\nbatch = LibLlama::LlamaBatch.new\ndecode(batch)\nmax_tokens.times do\n  next_token = sampler.sample(self)\n  sampler.accept(next_token)\n  eos_token = @model.vocab.eos\n  if next_token == eos_token\n    break\n  end\n  token_text = @model.vocab.token_to_text(next_token)\n  result = result + token_text\n  all_tokens << next_token\n  pos = pos + 1\n  decode(batch)\nend\nresult\n"}},{"html_id":"logits:Pointer(Float32)-instance-method","name":"logits","doc":"Gets the logits for the last token\n\nReturns:\n- A pointer to the logits array","summary":"<p>Gets the logits for the last token</p>","abstract":false,"location":{"filename":"src/llama/context.cr","line_number":138,"url":"https://github.com/kojix2/llama.cr/blob/e008ed302d03fcb414ca8352f854013484b58b19/src/llama/context.cr#L138"},"def":{"name":"logits","return_type":"Pointer(Float32)","visibility":"Public","body":"LibLlama.llama_get_logits(@handle)"}},{"html_id":"to_unsafe:Pointer(Llama::LibLlama::LlamaContext)-instance-method","name":"to_unsafe","doc":"Returns the raw pointer to the underlying llama_context structure","summary":"<p>Returns the raw pointer to the underlying llama_context structure</p>","abstract":false,"location":{"filename":"src/llama/context.cr","line_number":295,"url":"https://github.com/kojix2/llama.cr/blob/e008ed302d03fcb414ca8352f854013484b58b19/src/llama/context.cr#L295"},"def":{"name":"to_unsafe","visibility":"Public","body":"@handle"}}]},{"html_id":"llama/Llama/DistSampler","path":"Llama/DistSampler.html","kind":"class","full_name":"Llama::DistSampler","name":"DistSampler","abstract":false,"superclass":{"html_id":"llama/Llama/Sampler","kind":"class","full_name":"Llama::Sampler","name":"Sampler"},"ancestors":[{"html_id":"llama/Llama/Sampler","kind":"class","full_name":"Llama::Sampler","name":"Sampler"},{"html_id":"llama/Reference","kind":"class","full_name":"Reference","name":"Reference"},{"html_id":"llama/Object","kind":"class","full_name":"Object","name":"Object"}],"locations":[{"filename":"src/llama/sampler.cr","line_number":118,"url":"https://github.com/kojix2/llama.cr/blob/e008ed302d03fcb414ca8352f854013484b58b19/src/llama/sampler.cr#L118"}],"repository_name":"llama","program":false,"enum":false,"alias":false,"const":false,"namespace":{"html_id":"llama/Llama","kind":"module","full_name":"Llama","name":"Llama"},"doc":"Distribution sampler (final sampler in a chain)","summary":"<p>Distribution sampler (final sampler in a chain)</p>","constructors":[{"html_id":"new(seed:UInt32=LibLlama::LLAMA_DEFAULT_SEED)-class-method","name":"new","doc":"Creates a new distribution sampler\n\nParameters:\n- seed: Random seed for sampling (default: LLAMA_DEFAULT_SEED)\n\nRaises:\n- Llama::Error if the sampler cannot be created","summary":"<p>Creates a new distribution sampler</p>","abstract":false,"args":[{"name":"seed","default_value":"LibLlama::LLAMA_DEFAULT_SEED","external_name":"seed","restriction":"UInt32"}],"args_string":"(seed : UInt32 = LibLlama::LLAMA_DEFAULT_SEED)","args_html":"(seed : UInt32 = <span class=\"t\">LibLlama</span><span class=\"t\">::</span><span class=\"t\">LLAMA_DEFAULT_SEED</span>)","location":{"filename":"src/llama/sampler.cr","line_number":126,"url":"https://github.com/kojix2/llama.cr/blob/e008ed302d03fcb414ca8352f854013484b58b19/src/llama/sampler.cr#L126"},"def":{"name":"new","args":[{"name":"seed","default_value":"LibLlama::LLAMA_DEFAULT_SEED","external_name":"seed","restriction":"UInt32"}],"visibility":"Public","body":"_ = allocate\n_.initialize(seed)\nif _.responds_to?(:finalize)\n  ::GC.add_finalizer(_)\nend\n_\n"}}]},{"html_id":"llama/Llama/Error","path":"Llama/Error.html","kind":"class","full_name":"Llama::Error","name":"Error","abstract":false,"superclass":{"html_id":"llama/Exception","kind":"class","full_name":"Exception","name":"Exception"},"ancestors":[{"html_id":"llama/Exception","kind":"class","full_name":"Exception","name":"Exception"},{"html_id":"llama/Reference","kind":"class","full_name":"Reference","name":"Reference"},{"html_id":"llama/Object","kind":"class","full_name":"Object","name":"Object"}],"locations":[{"filename":"src/llama/model.cr","line_number":3,"url":"https://github.com/kojix2/llama.cr/blob/e008ed302d03fcb414ca8352f854013484b58b19/src/llama/model.cr#L3"}],"repository_name":"llama","program":false,"enum":false,"alias":false,"const":false,"namespace":{"html_id":"llama/Llama","kind":"module","full_name":"Llama","name":"Llama"},"doc":"Represents an error that occurred in the Llama library","summary":"<p>Represents an error that occurred in the Llama library</p>"},{"html_id":"llama/Llama/Model","path":"Llama/Model.html","kind":"class","full_name":"Llama::Model","name":"Model","abstract":false,"superclass":{"html_id":"llama/Reference","kind":"class","full_name":"Reference","name":"Reference"},"ancestors":[{"html_id":"llama/Reference","kind":"class","full_name":"Reference","name":"Reference"},{"html_id":"llama/Object","kind":"class","full_name":"Object","name":"Object"}],"locations":[{"filename":"src/llama/model.cr","line_number":6,"url":"https://github.com/kojix2/llama.cr/blob/e008ed302d03fcb414ca8352f854013484b58b19/src/llama/model.cr#L6"}],"repository_name":"llama","program":false,"enum":false,"alias":false,"const":false,"namespace":{"html_id":"llama/Llama","kind":"module","full_name":"Llama","name":"Llama"},"doc":"Wrapper for the llama_model structure","summary":"<p>Wrapper for the llama_model structure</p>","constructors":[{"html_id":"new(path:String)-class-method","name":"new","doc":"Creates a new Model instance by loading a model from a file\n\nParameters:\n- path: Path to the model file (.gguf format)\n\nRaises:\n- Llama::Error if the model cannot be loaded","summary":"<p>Creates a new Model instance by loading a model from a file</p>","abstract":false,"args":[{"name":"path","external_name":"path","restriction":"String"}],"args_string":"(path : String)","args_html":"(path : String)","location":{"filename":"src/llama/model.cr","line_number":14,"url":"https://github.com/kojix2/llama.cr/blob/e008ed302d03fcb414ca8352f854013484b58b19/src/llama/model.cr#L14"},"def":{"name":"new","args":[{"name":"path","external_name":"path","restriction":"String"}],"visibility":"Public","body":"_ = allocate\n_.initialize(path)\nif _.responds_to?(:finalize)\n  ::GC.add_finalizer(_)\nend\n_\n"}}],"instance_methods":[{"html_id":"chat_template(name:String|Nil=nil):String|Nil-instance-method","name":"chat_template","doc":"Gets the default chat template for this model\n\nParameters:\n- name: Optional template name (nil for default)\n\nReturns:\n- The chat template string, or nil if not available","summary":"<p>Gets the default chat template for this model</p>","abstract":false,"args":[{"name":"name","default_value":"nil","external_name":"name","restriction":"String | ::Nil"}],"args_string":"(name : String | Nil = nil) : String | Nil","args_html":"(name : String | Nil = <span class=\"n\">nil</span>) : String | Nil","location":{"filename":"src/llama/model.cr","line_number":27,"url":"https://github.com/kojix2/llama.cr/blob/e008ed302d03fcb414ca8352f854013484b58b19/src/llama/model.cr#L27"},"def":{"name":"chat_template","args":[{"name":"name","default_value":"nil","external_name":"name","restriction":"String | ::Nil"}],"return_type":"String | ::Nil","visibility":"Public","body":"ptr = LibLlama.llama_model_chat_template(@handle, name.nil? ? nil : name.to_unsafe)\nif ptr.null?\n  return nil\nend\nString.new(ptr)\n"}},{"html_id":"context(params=nil):Context-instance-method","name":"context","doc":"Creates a new Context for this model","summary":"<p>Creates a new Context for this model</p>","abstract":false,"args":[{"name":"params","default_value":"nil","external_name":"params","restriction":""}],"args_string":"(params = nil) : Context","args_html":"(params = <span class=\"n\">nil</span>) : <a href=\"../Llama/Context.html\">Context</a>","location":{"filename":"src/llama/model.cr","line_number":60,"url":"https://github.com/kojix2/llama.cr/blob/e008ed302d03fcb414ca8352f854013484b58b19/src/llama/model.cr#L60"},"def":{"name":"context","args":[{"name":"params","default_value":"nil","external_name":"params","restriction":""}],"return_type":"Context","visibility":"Public","body":"params || (params = LibLlama.llama_context_default_params)\nContext.new(self, params)\n"}},{"html_id":"finalize-instance-method","name":"finalize","doc":"Frees the resources associated with this model","summary":"<p>Frees the resources associated with this model</p>","abstract":false,"location":{"filename":"src/llama/model.cr","line_number":71,"url":"https://github.com/kojix2/llama.cr/blob/e008ed302d03fcb414ca8352f854013484b58b19/src/llama/model.cr#L71"},"def":{"name":"finalize","visibility":"Public","body":"if @handle.null?\nelse\n  LibLlama.llama_model_free(@handle)\nend"}},{"html_id":"n_embd:Int32-instance-method","name":"n_embd","doc":"Returns the number of embedding dimensions in the model","summary":"<p>Returns the number of embedding dimensions in the model</p>","abstract":false,"location":{"filename":"src/llama/model.cr","line_number":45,"url":"https://github.com/kojix2/llama.cr/blob/e008ed302d03fcb414ca8352f854013484b58b19/src/llama/model.cr#L45"},"def":{"name":"n_embd","return_type":"Int32","visibility":"Public","body":"LibLlama.llama_model_n_embd(@handle)"}},{"html_id":"n_head:Int32-instance-method","name":"n_head","doc":"Returns the number of attention heads in the model","summary":"<p>Returns the number of attention heads in the model</p>","abstract":false,"location":{"filename":"src/llama/model.cr","line_number":55,"url":"https://github.com/kojix2/llama.cr/blob/e008ed302d03fcb414ca8352f854013484b58b19/src/llama/model.cr#L55"},"def":{"name":"n_head","return_type":"Int32","visibility":"Public","body":"LibLlama.llama_model_n_head(@handle)"}},{"html_id":"n_layer:Int32-instance-method","name":"n_layer","doc":"Returns the number of layers in the model","summary":"<p>Returns the number of layers in the model</p>","abstract":false,"location":{"filename":"src/llama/model.cr","line_number":50,"url":"https://github.com/kojix2/llama.cr/blob/e008ed302d03fcb414ca8352f854013484b58b19/src/llama/model.cr#L50"},"def":{"name":"n_layer","return_type":"Int32","visibility":"Public","body":"LibLlama.llama_model_n_layer(@handle)"}},{"html_id":"n_params:UInt64-instance-method","name":"n_params","doc":"Returns the number of parameters in the model","summary":"<p>Returns the number of parameters in the model</p>","abstract":false,"location":{"filename":"src/llama/model.cr","line_number":40,"url":"https://github.com/kojix2/llama.cr/blob/e008ed302d03fcb414ca8352f854013484b58b19/src/llama/model.cr#L40"},"def":{"name":"n_params","return_type":"UInt64","visibility":"Public","body":"LibLlama.llama_model_n_params(@handle)"}},{"html_id":"to_unsafe:Pointer(Llama::LibLlama::LlamaModel)-instance-method","name":"to_unsafe","doc":"Returns the raw pointer to the underlying llama_model structure","summary":"<p>Returns the raw pointer to the underlying llama_model structure</p>","abstract":false,"location":{"filename":"src/llama/model.cr","line_number":66,"url":"https://github.com/kojix2/llama.cr/blob/e008ed302d03fcb414ca8352f854013484b58b19/src/llama/model.cr#L66"},"def":{"name":"to_unsafe","visibility":"Public","body":"@handle"}},{"html_id":"vocab:Vocab-instance-method","name":"vocab","doc":"Returns the vocabulary associated with this model","summary":"<p>Returns the vocabulary associated with this model</p>","abstract":false,"location":{"filename":"src/llama/model.cr","line_number":34,"url":"https://github.com/kojix2/llama.cr/blob/e008ed302d03fcb414ca8352f854013484b58b19/src/llama/model.cr#L34"},"def":{"name":"vocab","return_type":"Vocab","visibility":"Public","body":"vocab_ptr = LibLlama.llama_model_get_vocab(@handle)\nVocab.new(vocab_ptr)\n"}}]},{"html_id":"llama/Llama/Sampler","path":"Llama/Sampler.html","kind":"class","full_name":"Llama::Sampler","name":"Sampler","abstract":false,"superclass":{"html_id":"llama/Reference","kind":"class","full_name":"Reference","name":"Reference"},"ancestors":[{"html_id":"llama/Reference","kind":"class","full_name":"Reference","name":"Reference"},{"html_id":"llama/Object","kind":"class","full_name":"Object","name":"Object"}],"locations":[{"filename":"src/llama/sampler.cr","line_number":3,"url":"https://github.com/kojix2/llama.cr/blob/e008ed302d03fcb414ca8352f854013484b58b19/src/llama/sampler.cr#L3"}],"repository_name":"llama","program":false,"enum":false,"alias":false,"const":false,"subclasses":[{"html_id":"llama/Llama/DistSampler","kind":"class","full_name":"Llama::DistSampler","name":"DistSampler"},{"html_id":"llama/Llama/SamplerChain","kind":"class","full_name":"Llama::SamplerChain","name":"SamplerChain"},{"html_id":"llama/Llama/TempSampler","kind":"class","full_name":"Llama::TempSampler","name":"TempSampler"},{"html_id":"llama/Llama/TopKSampler","kind":"class","full_name":"Llama::TopKSampler","name":"TopKSampler"},{"html_id":"llama/Llama/TopPSampler","kind":"class","full_name":"Llama::TopPSampler","name":"TopPSampler"}],"namespace":{"html_id":"llama/Llama","kind":"module","full_name":"Llama","name":"Llama"},"doc":"Wrapper for the llama_sampler structure","summary":"<p>Wrapper for the llama_sampler structure</p>","constructors":[{"html_id":"new(handle:Pointer(LibLlama::LlamaSampler))-class-method","name":"new","doc":"Creates a new Sampler instance from a raw pointer\n\nNote: This constructor is intended for internal use.","summary":"<p>Creates a new Sampler instance from a raw pointer</p>","abstract":false,"args":[{"name":"handle","external_name":"handle","restriction":"::Pointer(LibLlama::LlamaSampler)"}],"args_string":"(handle : Pointer(LibLlama::LlamaSampler))","args_html":"(handle : Pointer(LibLlama::LlamaSampler))","location":{"filename":"src/llama/sampler.cr","line_number":7,"url":"https://github.com/kojix2/llama.cr/blob/e008ed302d03fcb414ca8352f854013484b58b19/src/llama/sampler.cr#L7"},"def":{"name":"new","args":[{"name":"handle","external_name":"handle","restriction":"::Pointer(LibLlama::LlamaSampler)"}],"visibility":"Public","body":"_ = allocate\n_.initialize(handle)\nif _.responds_to?(:finalize)\n  ::GC.add_finalizer(_)\nend\n_\n"}}],"instance_methods":[{"html_id":"finalize-instance-method","name":"finalize","doc":"Frees the resources associated with this sampler","summary":"<p>Frees the resources associated with this sampler</p>","abstract":false,"location":{"filename":"src/llama/sampler.cr","line_number":16,"url":"https://github.com/kojix2/llama.cr/blob/e008ed302d03fcb414ca8352f854013484b58b19/src/llama/sampler.cr#L16"},"def":{"name":"finalize","visibility":"Public","body":"if @handle.null?\nelse\n  LibLlama.llama_sampler_free(@handle)\nend"}},{"html_id":"to_unsafe:Pointer(Llama::LibLlama::LlamaSampler)-instance-method","name":"to_unsafe","doc":"Returns the raw pointer to the underlying llama_sampler structure","summary":"<p>Returns the raw pointer to the underlying llama_sampler structure</p>","abstract":false,"location":{"filename":"src/llama/sampler.cr","line_number":11,"url":"https://github.com/kojix2/llama.cr/blob/e008ed302d03fcb414ca8352f854013484b58b19/src/llama/sampler.cr#L11"},"def":{"name":"to_unsafe","visibility":"Public","body":"@handle"}}]},{"html_id":"llama/Llama/SamplerChain","path":"Llama/SamplerChain.html","kind":"class","full_name":"Llama::SamplerChain","name":"SamplerChain","abstract":false,"superclass":{"html_id":"llama/Llama/Sampler","kind":"class","full_name":"Llama::Sampler","name":"Sampler"},"ancestors":[{"html_id":"llama/Llama/Sampler","kind":"class","full_name":"Llama::Sampler","name":"Sampler"},{"html_id":"llama/Reference","kind":"class","full_name":"Reference","name":"Reference"},{"html_id":"llama/Object","kind":"class","full_name":"Object","name":"Object"}],"locations":[{"filename":"src/llama/sampler.cr","line_number":24,"url":"https://github.com/kojix2/llama.cr/blob/e008ed302d03fcb414ca8352f854013484b58b19/src/llama/sampler.cr#L24"}],"repository_name":"llama","program":false,"enum":false,"alias":false,"const":false,"namespace":{"html_id":"llama/Llama","kind":"module","full_name":"Llama","name":"Llama"},"doc":"Wrapper for a chain of samplers","summary":"<p>Wrapper for a chain of samplers</p>","constructors":[{"html_id":"new(params=nil)-class-method","name":"new","doc":"Creates a new SamplerChain with optional parameters\n\nParameters:\n- params: Optional sampler chain parameters\n\nRaises:\n- Llama::Error if the sampler chain cannot be created","summary":"<p>Creates a new SamplerChain with optional parameters</p>","abstract":false,"args":[{"name":"params","default_value":"nil","external_name":"params","restriction":""}],"args_string":"(params = nil)","args_html":"(params = <span class=\"n\">nil</span>)","location":{"filename":"src/llama/sampler.cr","line_number":32,"url":"https://github.com/kojix2/llama.cr/blob/e008ed302d03fcb414ca8352f854013484b58b19/src/llama/sampler.cr#L32"},"def":{"name":"new","args":[{"name":"params","default_value":"nil","external_name":"params","restriction":""}],"visibility":"Public","body":"_ = allocate\n_.initialize(params)\nif _.responds_to?(:finalize)\n  ::GC.add_finalizer(_)\nend\n_\n"}}],"instance_methods":[{"html_id":"accept(token:Int32)-instance-method","name":"accept","doc":"Accepts a token, updating the internal state of the samplers\n\nParameters:\n- token: The token to accept","summary":"<p>Accepts a token, updating the internal state of the samplers</p>","abstract":false,"args":[{"name":"token","external_name":"token","restriction":"Int32"}],"args_string":"(token : Int32)","args_html":"(token : Int32)","location":{"filename":"src/llama/sampler.cr","line_number":64,"url":"https://github.com/kojix2/llama.cr/blob/e008ed302d03fcb414ca8352f854013484b58b19/src/llama/sampler.cr#L64"},"def":{"name":"accept","args":[{"name":"token","external_name":"token","restriction":"Int32"}],"visibility":"Public","body":"LibLlama.llama_sampler_accept(@handle, token)"}},{"html_id":"add(sampler:Sampler)-instance-method","name":"add","doc":"Adds a sampler to the chain\n\nParameters:\n- sampler: The sampler to add to the chain","summary":"<p>Adds a sampler to the chain</p>","abstract":false,"args":[{"name":"sampler","external_name":"sampler","restriction":"Sampler"}],"args_string":"(sampler : Sampler)","args_html":"(sampler : <a href=\"../Llama/Sampler.html\">Sampler</a>)","location":{"filename":"src/llama/sampler.cr","line_number":43,"url":"https://github.com/kojix2/llama.cr/blob/e008ed302d03fcb414ca8352f854013484b58b19/src/llama/sampler.cr#L43"},"def":{"name":"add","args":[{"name":"sampler","external_name":"sampler","restriction":"Sampler"}],"visibility":"Public","body":"LibLlama.llama_sampler_chain_add(@handle, sampler.to_unsafe)\n@samplers << sampler\n"}},{"html_id":"sample(ctx:Context,idx:Int32=-1):Int32-instance-method","name":"sample","doc":"Samples a token using the sampler chain\n\nParameters:\n- ctx: The context to sample from\n- idx: The index of the logits to sample from (-1 for the last token)\n\nReturns:\n- The sampled token","summary":"<p>Samples a token using the sampler chain</p>","abstract":false,"args":[{"name":"ctx","external_name":"ctx","restriction":"Context"},{"name":"idx","default_value":"-1","external_name":"idx","restriction":"Int32"}],"args_string":"(ctx : Context, idx : Int32 = -1) : Int32","args_html":"(ctx : <a href=\"../Llama/Context.html\">Context</a>, idx : Int32 = <span class=\"n\">-1</span>) : Int32","location":{"filename":"src/llama/sampler.cr","line_number":56,"url":"https://github.com/kojix2/llama.cr/blob/e008ed302d03fcb414ca8352f854013484b58b19/src/llama/sampler.cr#L56"},"def":{"name":"sample","args":[{"name":"ctx","external_name":"ctx","restriction":"Context"},{"name":"idx","default_value":"-1","external_name":"idx","restriction":"Int32"}],"return_type":"Int32","visibility":"Public","body":"LibLlama.llama_sampler_sample(@handle, ctx.to_unsafe, idx)"}}]},{"html_id":"llama/Llama/TempSampler","path":"Llama/TempSampler.html","kind":"class","full_name":"Llama::TempSampler","name":"TempSampler","abstract":false,"superclass":{"html_id":"llama/Llama/Sampler","kind":"class","full_name":"Llama::Sampler","name":"Sampler"},"ancestors":[{"html_id":"llama/Llama/Sampler","kind":"class","full_name":"Llama::Sampler","name":"Sampler"},{"html_id":"llama/Reference","kind":"class","full_name":"Reference","name":"Reference"},{"html_id":"llama/Object","kind":"class","full_name":"Object","name":"Object"}],"locations":[{"filename":"src/llama/sampler.cr","line_number":103,"url":"https://github.com/kojix2/llama.cr/blob/e008ed302d03fcb414ca8352f854013484b58b19/src/llama/sampler.cr#L103"}],"repository_name":"llama","program":false,"enum":false,"alias":false,"const":false,"namespace":{"html_id":"llama/Llama","kind":"module","full_name":"Llama","name":"Llama"},"doc":"Temperature sampler","summary":"<p>Temperature sampler</p>","constructors":[{"html_id":"new(temp:Float32)-class-method","name":"new","doc":"Creates a new temperature sampler\n\nParameters:\n- temp: The temperature value (0.0 = greedy, 1.0 = normal, >1.0 = more random)\n\nRaises:\n- Llama::Error if the sampler cannot be created","summary":"<p>Creates a new temperature sampler</p>","abstract":false,"args":[{"name":"temp","external_name":"temp","restriction":"Float32"}],"args_string":"(temp : Float32)","args_html":"(temp : Float32)","location":{"filename":"src/llama/sampler.cr","line_number":111,"url":"https://github.com/kojix2/llama.cr/blob/e008ed302d03fcb414ca8352f854013484b58b19/src/llama/sampler.cr#L111"},"def":{"name":"new","args":[{"name":"temp","external_name":"temp","restriction":"Float32"}],"visibility":"Public","body":"_ = allocate\n_.initialize(temp)\nif _.responds_to?(:finalize)\n  ::GC.add_finalizer(_)\nend\n_\n"}}]},{"html_id":"llama/Llama/TopKSampler","path":"Llama/TopKSampler.html","kind":"class","full_name":"Llama::TopKSampler","name":"TopKSampler","abstract":false,"superclass":{"html_id":"llama/Llama/Sampler","kind":"class","full_name":"Llama::Sampler","name":"Sampler"},"ancestors":[{"html_id":"llama/Llama/Sampler","kind":"class","full_name":"Llama::Sampler","name":"Sampler"},{"html_id":"llama/Reference","kind":"class","full_name":"Reference","name":"Reference"},{"html_id":"llama/Object","kind":"class","full_name":"Object","name":"Object"}],"locations":[{"filename":"src/llama/sampler.cr","line_number":72,"url":"https://github.com/kojix2/llama.cr/blob/e008ed302d03fcb414ca8352f854013484b58b19/src/llama/sampler.cr#L72"}],"repository_name":"llama","program":false,"enum":false,"alias":false,"const":false,"namespace":{"html_id":"llama/Llama","kind":"module","full_name":"Llama","name":"Llama"},"doc":"Top-K sampler","summary":"<p>Top-K sampler</p>","constructors":[{"html_id":"new(k:Int32)-class-method","name":"new","doc":"Creates a new Top-K sampler\n\nParameters:\n- k: The number of top tokens to consider\n\nRaises:\n- Llama::Error if the sampler cannot be created","summary":"<p>Creates a new Top-K sampler</p>","abstract":false,"args":[{"name":"k","external_name":"k","restriction":"Int32"}],"args_string":"(k : Int32)","args_html":"(k : Int32)","location":{"filename":"src/llama/sampler.cr","line_number":80,"url":"https://github.com/kojix2/llama.cr/blob/e008ed302d03fcb414ca8352f854013484b58b19/src/llama/sampler.cr#L80"},"def":{"name":"new","args":[{"name":"k","external_name":"k","restriction":"Int32"}],"visibility":"Public","body":"_ = allocate\n_.initialize(k)\nif _.responds_to?(:finalize)\n  ::GC.add_finalizer(_)\nend\n_\n"}}]},{"html_id":"llama/Llama/TopPSampler","path":"Llama/TopPSampler.html","kind":"class","full_name":"Llama::TopPSampler","name":"TopPSampler","abstract":false,"superclass":{"html_id":"llama/Llama/Sampler","kind":"class","full_name":"Llama::Sampler","name":"Sampler"},"ancestors":[{"html_id":"llama/Llama/Sampler","kind":"class","full_name":"Llama::Sampler","name":"Sampler"},{"html_id":"llama/Reference","kind":"class","full_name":"Reference","name":"Reference"},{"html_id":"llama/Object","kind":"class","full_name":"Object","name":"Object"}],"locations":[{"filename":"src/llama/sampler.cr","line_number":87,"url":"https://github.com/kojix2/llama.cr/blob/e008ed302d03fcb414ca8352f854013484b58b19/src/llama/sampler.cr#L87"}],"repository_name":"llama","program":false,"enum":false,"alias":false,"const":false,"namespace":{"html_id":"llama/Llama","kind":"module","full_name":"Llama","name":"Llama"},"doc":"Top-P (nucleus) sampler","summary":"<p>Top-P (nucleus) sampler</p>","constructors":[{"html_id":"new(p:Float32,min_keep:Int32=1)-class-method","name":"new","doc":"Creates a new Top-P sampler\n\nParameters:\n- p: The cumulative probability threshold (0.0 to 1.0)\n- min_keep: Minimum number of tokens to keep\n\nRaises:\n- Llama::Error if the sampler cannot be created","summary":"<p>Creates a new Top-P sampler</p>","abstract":false,"args":[{"name":"p","external_name":"p","restriction":"Float32"},{"name":"min_keep","default_value":"1","external_name":"min_keep","restriction":"Int32"}],"args_string":"(p : Float32, min_keep : Int32 = 1)","args_html":"(p : Float32, min_keep : Int32 = <span class=\"n\">1</span>)","location":{"filename":"src/llama/sampler.cr","line_number":96,"url":"https://github.com/kojix2/llama.cr/blob/e008ed302d03fcb414ca8352f854013484b58b19/src/llama/sampler.cr#L96"},"def":{"name":"new","args":[{"name":"p","external_name":"p","restriction":"Float32"},{"name":"min_keep","default_value":"1","external_name":"min_keep","restriction":"Int32"}],"visibility":"Public","body":"_ = allocate\n_.initialize(p, min_keep)\nif _.responds_to?(:finalize)\n  ::GC.add_finalizer(_)\nend\n_\n"}}]},{"html_id":"llama/Llama/Vocab","path":"Llama/Vocab.html","kind":"class","full_name":"Llama::Vocab","name":"Vocab","abstract":false,"superclass":{"html_id":"llama/Reference","kind":"class","full_name":"Reference","name":"Reference"},"ancestors":[{"html_id":"llama/Reference","kind":"class","full_name":"Reference","name":"Reference"},{"html_id":"llama/Object","kind":"class","full_name":"Object","name":"Object"}],"locations":[{"filename":"src/llama/vocab.cr","line_number":3,"url":"https://github.com/kojix2/llama.cr/blob/e008ed302d03fcb414ca8352f854013484b58b19/src/llama/vocab.cr#L3"}],"repository_name":"llama","program":false,"enum":false,"alias":false,"const":false,"namespace":{"html_id":"llama/Llama","kind":"module","full_name":"Llama","name":"Llama"},"doc":"Wrapper for the llama_vocab structure","summary":"<p>Wrapper for the llama_vocab structure</p>","constructors":[{"html_id":"new(handle:Pointer(LibLlama::LlamaVocab))-class-method","name":"new","doc":"Creates a new Vocab instance from a raw pointer\n\nNote: This constructor is intended for internal use.\nUsers should obtain Vocab instances through Model#vocab.","summary":"<p>Creates a new Vocab instance from a raw pointer</p>","abstract":false,"args":[{"name":"handle","external_name":"handle","restriction":"::Pointer(LibLlama::LlamaVocab)"}],"args_string":"(handle : Pointer(LibLlama::LlamaVocab))","args_html":"(handle : Pointer(LibLlama::LlamaVocab))","location":{"filename":"src/llama/vocab.cr","line_number":8,"url":"https://github.com/kojix2/llama.cr/blob/e008ed302d03fcb414ca8352f854013484b58b19/src/llama/vocab.cr#L8"},"def":{"name":"new","args":[{"name":"handle","external_name":"handle","restriction":"::Pointer(LibLlama::LlamaVocab)"}],"visibility":"Public","body":"_ = allocate\n_.initialize(handle)\nif _.responds_to?(:finalize)\n  ::GC.add_finalizer(_)\nend\n_\n"}}],"instance_methods":[{"html_id":"bos:Int32-instance-method","name":"bos","doc":"Returns the beginning-of-sentence token ID","summary":"<p>Returns the beginning-of-sentence token ID</p>","abstract":false,"location":{"filename":"src/llama/vocab.cr","line_number":66,"url":"https://github.com/kojix2/llama.cr/blob/e008ed302d03fcb414ca8352f854013484b58b19/src/llama/vocab.cr#L66"},"def":{"name":"bos","return_type":"Int32","visibility":"Public","body":"LibLlama.llama_vocab_bos(@handle)"}},{"html_id":"eos:Int32-instance-method","name":"eos","doc":"Returns the end-of-sentence token ID","summary":"<p>Returns the end-of-sentence token ID</p>","abstract":false,"location":{"filename":"src/llama/vocab.cr","line_number":71,"url":"https://github.com/kojix2/llama.cr/blob/e008ed302d03fcb414ca8352f854013484b58b19/src/llama/vocab.cr#L71"},"def":{"name":"eos","return_type":"Int32","visibility":"Public","body":"LibLlama.llama_vocab_eos(@handle)"}},{"html_id":"eot:Int32-instance-method","name":"eot","doc":"Returns the end-of-turn token ID","summary":"<p>Returns the end-of-turn token ID</p>","abstract":false,"location":{"filename":"src/llama/vocab.cr","line_number":76,"url":"https://github.com/kojix2/llama.cr/blob/e008ed302d03fcb414ca8352f854013484b58b19/src/llama/vocab.cr#L76"},"def":{"name":"eot","return_type":"Int32","visibility":"Public","body":"LibLlama.llama_vocab_eot(@handle)"}},{"html_id":"n_tokens:Int32-instance-method","name":"n_tokens","doc":"Returns the number of tokens in the vocabulary","summary":"<p>Returns the number of tokens in the vocabulary</p>","abstract":false,"location":{"filename":"src/llama/vocab.cr","line_number":12,"url":"https://github.com/kojix2/llama.cr/blob/e008ed302d03fcb414ca8352f854013484b58b19/src/llama/vocab.cr#L12"},"def":{"name":"n_tokens","return_type":"Int32","visibility":"Public","body":"LibLlama.llama_vocab_n_tokens(@handle)"}},{"html_id":"nl:Int32-instance-method","name":"nl","doc":"Returns the newline token ID","summary":"<p>Returns the newline token ID</p>","abstract":false,"location":{"filename":"src/llama/vocab.cr","line_number":81,"url":"https://github.com/kojix2/llama.cr/blob/e008ed302d03fcb414ca8352f854013484b58b19/src/llama/vocab.cr#L81"},"def":{"name":"nl","return_type":"Int32","visibility":"Public","body":"LibLlama.llama_vocab_nl(@handle)"}},{"html_id":"pad:Int32-instance-method","name":"pad","doc":"Returns the padding token ID","summary":"<p>Returns the padding token ID</p>","abstract":false,"location":{"filename":"src/llama/vocab.cr","line_number":86,"url":"https://github.com/kojix2/llama.cr/blob/e008ed302d03fcb414ca8352f854013484b58b19/src/llama/vocab.cr#L86"},"def":{"name":"pad","return_type":"Int32","visibility":"Public","body":"LibLlama.llama_vocab_pad(@handle)"}},{"html_id":"to_unsafe:Pointer(Llama::LibLlama::LlamaVocab)-instance-method","name":"to_unsafe","doc":"Returns the raw pointer to the underlying llama_vocab structure","summary":"<p>Returns the raw pointer to the underlying llama_vocab structure</p>","abstract":false,"location":{"filename":"src/llama/vocab.cr","line_number":91,"url":"https://github.com/kojix2/llama.cr/blob/e008ed302d03fcb414ca8352f854013484b58b19/src/llama/vocab.cr#L91"},"def":{"name":"to_unsafe","visibility":"Public","body":"@handle"}},{"html_id":"token_to_text(token:Int32):String-instance-method","name":"token_to_text","doc":"Returns the text representation of a token","summary":"<p>Returns the text representation of a token</p>","abstract":false,"args":[{"name":"token","external_name":"token","restriction":"Int32"}],"args_string":"(token : Int32) : String","args_html":"(token : Int32) : String","location":{"filename":"src/llama/vocab.cr","line_number":17,"url":"https://github.com/kojix2/llama.cr/blob/e008ed302d03fcb414ca8352f854013484b58b19/src/llama/vocab.cr#L17"},"def":{"name":"token_to_text","args":[{"name":"token","external_name":"token","restriction":"Int32"}],"return_type":"String","visibility":"Public","body":"ptr = LibLlama.llama_vocab_get_text(@handle, token)\nString.new(ptr)\n"}},{"html_id":"tokenize(text:String,add_special:Bool=true,parse_special:Bool=true):Array(Int32)-instance-method","name":"tokenize","doc":"Tokenizes a string into an array of token IDs","summary":"<p>Tokenizes a string into an array of token IDs</p>","abstract":false,"args":[{"name":"text","external_name":"text","restriction":"String"},{"name":"add_special","default_value":"true","external_name":"add_special","restriction":"Bool"},{"name":"parse_special","default_value":"true","external_name":"parse_special","restriction":"Bool"}],"args_string":"(text : String, add_special : Bool = true, parse_special : Bool = true) : Array(Int32)","args_html":"(text : String, add_special : Bool = <span class=\"n\">true</span>, parse_special : Bool = <span class=\"n\">true</span>) : Array(Int32)","location":{"filename":"src/llama/vocab.cr","line_number":23,"url":"https://github.com/kojix2/llama.cr/blob/e008ed302d03fcb414ca8352f854013484b58b19/src/llama/vocab.cr#L23"},"def":{"name":"tokenize","args":[{"name":"text","external_name":"text","restriction":"String"},{"name":"add_special","default_value":"true","external_name":"add_special","restriction":"Bool"},{"name":"parse_special","default_value":"true","external_name":"parse_special","restriction":"Bool"}],"return_type":"Array(Int32)","visibility":"Public","body":"max_tokens = text.size * 2\ntokens = Pointer(LibLlama::LlamaToken).malloc(max_tokens)\nn_tokens = LibLlama.llama_tokenize(@handle, text, text.bytesize, tokens, max_tokens, add_special, parse_special)\nif n_tokens < 0\n  max_tokens = -n_tokens\n  tokens = Pointer(LibLlama::LlamaToken).malloc(max_tokens)\n  n_tokens = LibLlama.llama_tokenize(@handle, text, text.bytesize, tokens, max_tokens, add_special, parse_special)\nend\nif n_tokens < 0\n  raise(Error.new(\"Failed to tokenize text\"))\nend\nresult = Array(Int32).new(n_tokens)\nn_tokens.times do |i|\n  result << tokens[i]\nend\nresult\n"}}]}]}]}})