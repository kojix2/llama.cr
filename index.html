<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="utf-8" />
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1.0">
<meta name="generator" content="Crystal Docs 1.15.1">
<meta name="crystal_docs.project_version" content="main">
<meta name="crystal_docs.project_name" content="llama">



<link href="css/style.css" rel="stylesheet" type="text/css" />
<script type="text/javascript" src="js/doc.js"></script>

  <meta name="repository-name" content="llama">
  <title>llama main</title>
  <script type="text/javascript">
  CrystalDocs.base_path = "";
  </script>
</head>
<body>

<svg class="hidden">
  <symbol id="octicon-link" viewBox="0 0 16 16">
    <path fill="currentColor" fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path>
  </symbol>
</svg>
<input type="checkbox" id="sidebar-btn">
<label for="sidebar-btn" id="sidebar-btn-label">
  <svg class="open" xmlns="http://www.w3.org/2000/svg" height="2em" width="2em" viewBox="0 0 512 512"><title>Open Sidebar</title><path fill="currentColor" d="M80 96v64h352V96H80zm0 112v64h352v-64H80zm0 112v64h352v-64H80z"></path></svg>
  <svg class="close" xmlns="http://www.w3.org/2000/svg" width="2em" height="2em" viewBox="0 0 512 512"><title>Close Sidebar</title><path fill="currentColor" d="m118.6 73.4-45.2 45.2L210.7 256 73.4 393.4l45.2 45.2L256 301.3l137.4 137.3 45.2-45.2L301.3 256l137.3-137.4-45.2-45.2L256 210.7Z"></path></svg>
</label>
<div class="sidebar">
  <div class="sidebar-header">
    <div class="search-box">
      <input type="search" class="search-input" placeholder="Search..." spellcheck="false" aria-label="Search">
    </div>

    <div class="project-summary">
      <h1 class="project-name">
        <a href="index.html">
          llama
        </a>
      </h1>

      <span class="project-version">
        main
      </span>
    </div>
  </div>

  <div class="search-results hidden">
    <ul class="search-list"></ul>
  </div>

  <div class="types-list">
    <ul>
  
  <li class="parent " data-id="llama/Llama" data-name="llama">
      <a href="Llama.html">Llama</a>
      
        <ul>
  
  <li class="parent " data-id="llama/Llama/Batch" data-name="llama::batch">
      <a href="Llama/Batch.html">Batch</a>
      
        <ul>
  
  <li class=" " data-id="llama/Llama/Batch/Error" data-name="llama::batch::error">
      <a href="Llama/Batch/Error.html">Error</a>
      
    </li>
  
</ul>

      
    </li>
  
  <li class=" " data-id="llama/Llama/ChatMessage" data-name="llama::chatmessage">
      <a href="Llama/ChatMessage.html">ChatMessage</a>
      
    </li>
  
  <li class="parent " data-id="llama/Llama/Context" data-name="llama::context">
      <a href="Llama/Context.html">Context</a>
      
        <ul>
  
  <li class=" " data-id="llama/Llama/Context/Error" data-name="llama::context::error">
      <a href="Llama/Context/Error.html">Error</a>
      
    </li>
  
  <li class=" " data-id="llama/Llama/Context/TokenizationError" data-name="llama::context::tokenizationerror">
      <a href="Llama/Context/TokenizationError.html">TokenizationError</a>
      
    </li>
  
</ul>

      
    </li>
  
  <li class=" " data-id="llama/Llama/Error" data-name="llama::error">
      <a href="Llama/Error.html">Error</a>
      
    </li>
  
  <li class="parent " data-id="llama/Llama/KvCache" data-name="llama::kvcache">
      <a href="Llama/KvCache.html">KvCache</a>
      
        <ul>
  
  <li class=" " data-id="llama/Llama/KvCache/Error" data-name="llama::kvcache::error">
      <a href="Llama/KvCache/Error.html">Error</a>
      
    </li>
  
</ul>

      
    </li>
  
  <li class="parent " data-id="llama/Llama/Model" data-name="llama::model">
      <a href="Llama/Model.html">Model</a>
      
        <ul>
  
  <li class=" " data-id="llama/Llama/Model/Error" data-name="llama::model::error">
      <a href="Llama/Model/Error.html">Error</a>
      
    </li>
  
</ul>

      
    </li>
  
  <li class="parent " data-id="llama/Llama/Sampler" data-name="llama::sampler">
      <a href="Llama/Sampler.html">Sampler</a>
      
        <ul>
  
  <li class=" " data-id="llama/Llama/Sampler/Base" data-name="llama::sampler::base">
      <a href="Llama/Sampler/Base.html">Base</a>
      
    </li>
  
  <li class=" " data-id="llama/Llama/Sampler/Chain" data-name="llama::sampler::chain">
      <a href="Llama/Sampler/Chain.html">Chain</a>
      
    </li>
  
  <li class=" " data-id="llama/Llama/Sampler/Dist" data-name="llama::sampler::dist">
      <a href="Llama/Sampler/Dist.html">Dist</a>
      
    </li>
  
  <li class=" " data-id="llama/Llama/Sampler/Error" data-name="llama::sampler::error">
      <a href="Llama/Sampler/Error.html">Error</a>
      
    </li>
  
  <li class=" " data-id="llama/Llama/Sampler/Grammar" data-name="llama::sampler::grammar">
      <a href="Llama/Sampler/Grammar.html">Grammar</a>
      
    </li>
  
  <li class=" " data-id="llama/Llama/Sampler/GrammarLazyPatterns" data-name="llama::sampler::grammarlazypatterns">
      <a href="Llama/Sampler/GrammarLazyPatterns.html">GrammarLazyPatterns</a>
      
    </li>
  
  <li class=" " data-id="llama/Llama/Sampler/Greedy" data-name="llama::sampler::greedy">
      <a href="Llama/Sampler/Greedy.html">Greedy</a>
      
    </li>
  
  <li class=" " data-id="llama/Llama/Sampler/Infill" data-name="llama::sampler::infill">
      <a href="Llama/Sampler/Infill.html">Infill</a>
      
    </li>
  
  <li class=" " data-id="llama/Llama/Sampler/MinP" data-name="llama::sampler::minp">
      <a href="Llama/Sampler/MinP.html">MinP</a>
      
    </li>
  
  <li class=" " data-id="llama/Llama/Sampler/Mirostat" data-name="llama::sampler::mirostat">
      <a href="Llama/Sampler/Mirostat.html">Mirostat</a>
      
    </li>
  
  <li class=" " data-id="llama/Llama/Sampler/MirostatV2" data-name="llama::sampler::mirostatv2">
      <a href="Llama/Sampler/MirostatV2.html">MirostatV2</a>
      
    </li>
  
  <li class=" " data-id="llama/Llama/Sampler/Penalties" data-name="llama::sampler::penalties">
      <a href="Llama/Sampler/Penalties.html">Penalties</a>
      
    </li>
  
  <li class=" " data-id="llama/Llama/Sampler/Temp" data-name="llama::sampler::temp">
      <a href="Llama/Sampler/Temp.html">Temp</a>
      
    </li>
  
  <li class=" " data-id="llama/Llama/Sampler/TempExt" data-name="llama::sampler::tempext">
      <a href="Llama/Sampler/TempExt.html">TempExt</a>
      
    </li>
  
  <li class=" " data-id="llama/Llama/Sampler/TokenizationError" data-name="llama::sampler::tokenizationerror">
      <a href="Llama/Sampler/TokenizationError.html">TokenizationError</a>
      
    </li>
  
  <li class=" " data-id="llama/Llama/Sampler/TopK" data-name="llama::sampler::topk">
      <a href="Llama/Sampler/TopK.html">TopK</a>
      
    </li>
  
  <li class=" " data-id="llama/Llama/Sampler/TopNSigma" data-name="llama::sampler::topnsigma">
      <a href="Llama/Sampler/TopNSigma.html">TopNSigma</a>
      
    </li>
  
  <li class=" " data-id="llama/Llama/Sampler/TopP" data-name="llama::sampler::topp">
      <a href="Llama/Sampler/TopP.html">TopP</a>
      
    </li>
  
  <li class=" " data-id="llama/Llama/Sampler/Typical" data-name="llama::sampler::typical">
      <a href="Llama/Sampler/Typical.html">Typical</a>
      
    </li>
  
  <li class=" " data-id="llama/Llama/Sampler/Xtc" data-name="llama::sampler::xtc">
      <a href="Llama/Sampler/Xtc.html">Xtc</a>
      
    </li>
  
</ul>

      
    </li>
  
  <li class="parent " data-id="llama/Llama/State" data-name="llama::state">
      <a href="Llama/State.html">State</a>
      
        <ul>
  
  <li class=" " data-id="llama/Llama/State/Error" data-name="llama::state::error">
      <a href="Llama/State/Error.html">Error</a>
      
    </li>
  
</ul>

      
    </li>
  
  <li class=" " data-id="llama/Llama/Vocab" data-name="llama::vocab">
      <a href="Llama/Vocab.html">Vocab</a>
      
    </li>
  
</ul>

      
    </li>
  
</ul>

  </div>
</div>


<div class="main-content">
<h1><a id="llama.cr" class="anchor" href="#llama.cr">  <svg class="octicon-link" aria-hidden="true">
    <use href="#octicon-link"/>
  </svg>
</a>llama.cr</h1>
<p><a href="https://github.com/kojix2/llama.cr/actions/workflows/test.yml"><img src="https://github.com/kojix2/llama.cr/actions/workflows/test.yml/badge.svg" alt="test" /></a>
<a href="https://kojix2.github.io/llama.cr"><img src="https://img.shields.io/badge/docs-latest-blue.svg" alt="docs" /></a>
<a href="https://tokei.kojix2.net/github/kojix2/llama.cr"><img src="https://img.shields.io/endpoint?url=https://tokei.kojix2.net/badge/github/kojix2/llama.cr/lines" alt="Lines of Code" /></a></p>
<p>Crystal bindings for <a href="https://github.com/ggml-org/llama.cpp">llama.cpp</a>, a C/C++ implementation of LLaMA, Falcon, GPT-2, and other large language models.</p>
<p>Please check the <a href="LLAMA_VERSION">LLAMA_VERSION</a> file for the current compatible version of llama.cpp.</p>
<p>This project is under active development and may change rapidly.</p>
<h2><a id="features" class="anchor" href="#features">
  <svg class="octicon-link" aria-hidden="true">
    <use href="#octicon-link"/>
  </svg>
</a>Features</h2>
<ul>
<li>Low-level bindings to the llama.cpp C API</li>
<li>High-level Crystal wrapper classes for easy usage</li>
<li>Memory management for C resources</li>
<li>Simple text generation interface</li>
<li>Advanced sampling methods (Min-P, Typical, Mirostat, etc.)</li>
<li>Batch processing for efficient token handling</li>
<li>KV cache management for optimized inference</li>
<li>State saving and loading</li>
</ul>
<h2><a id="installation" class="anchor" href="#installation">
  <svg class="octicon-link" aria-hidden="true">
    <use href="#octicon-link"/>
  </svg>
</a>Installation</h2>
<h3><a id="prerequisites" class="anchor" href="#prerequisites">
  <svg class="octicon-link" aria-hidden="true">
    <use href="#octicon-link"/>
  </svg>
</a>Prerequisites</h3>
<p>You need the llama.cpp shared library (libllama) available on your system.</p>
<h4><a id="1.-download-prebuilt-binary-recommended" class="anchor" href="#1.-download-prebuilt-binary-recommended">
  <svg class="octicon-link" aria-hidden="true">
    <use href="#octicon-link"/>
  </svg>
</a>1. Download Prebuilt Binary (Recommended)</h4>
<p>You can download a prebuilt binary matching the required version automatically:</p>
<p><strong>Linux/macOS (bash):</strong></p>
<pre><code class="language-sh">LLAMA_VERSION=$(cat LLAMA_VERSION)
wget &quot;https://github.com/ggerganov/llama.cpp/releases/download/v${LLAMA_VERSION}/llama-linux-x64.zip&quot;
unzip &quot;llama-linux-x64.zip&quot;
# Move libllama.so to ./lib or set LD_LIBRARY_PATH
export LD_LIBRARY_PATH=$PWD/lib:$LD_LIBRARY_PATH</code></pre>
<p>You can also specify the library location without installing:</p>
<pre><code class="language-bash">crystal build examples/simple.cr --link-flags=&quot;-L/path/to/lib&quot;
LD_LIBRARY_PATH=/path/to/lib ./simple /path/to/model.gguf &quot;Your prompt here&quot;</code></pre>
<details>
<summary>Build from source (advanced users)</summary>
<p>You can build llama.cpp from source if you prefer:</p>
<pre><code class="language-bash">git clone https://github.com/ggml-org/llama.cpp.git
cd llama.cpp
git checkout v$(cat ../LLAMA_VERSION)
mkdir build &amp;&amp; cd build
cmake ..
cmake --build . --config Release
sudo cmake --install .
sudo ldconfig</code></pre>
</details>
<h3><a id="obtaining-gguf-model-files" class="anchor" href="#obtaining-gguf-model-files">
  <svg class="octicon-link" aria-hidden="true">
    <use href="#octicon-link"/>
  </svg>
</a>Obtaining GGUF Model Files</h3>
<p>You'll need a model file in GGUF format. For testing, smaller quantized models (1-3B parameters) with Q4_K_M quantization are recommended.</p>
<p>Popular options:</p>
<ul>
<li><a href="https://huggingface.co/TheBloke/TinyLlama-1.1B-Chat-v1.0-GGUF">TinyLlama 1.1B</a> <a href="https://huggingface.co/TheBloke/TinyLlama-1.1B-Chat-v1.0-GGUF/resolve/main/tinyllama-1.1b-chat-v1.0.Q4_K_M.gguf">[raw]</a></li>
<li><a href="https://huggingface.co/mmnga/Meta-Llama-3-70B-Instruct-gguf">Llama 3 8B Instruct</a></li>
<li><a href="https://huggingface.co/TheBloke/Mistral-7B-Instruct-v0.2-GGUF">Mistral 7B Instruct v0.2</a></li>
</ul>
<h3><a id="adding-to-your-project" class="anchor" href="#adding-to-your-project">
  <svg class="octicon-link" aria-hidden="true">
    <use href="#octicon-link"/>
  </svg>
</a>Adding to Your Project</h3>
<p>Add the dependency to your <code>shard.yml</code>:</p>
<pre><code class="language-yaml">dependencies:
  llama:
    github: kojix2/llama.cr</code></pre>
<p>Then run <code>shards install</code>.</p>
<h2><a id="usage" class="anchor" href="#usage">
  <svg class="octicon-link" aria-hidden="true">
    <use href="#octicon-link"/>
  </svg>
</a>Usage</h2>
<h3><a id="basic-text-generation" class="anchor" href="#basic-text-generation">
  <svg class="octicon-link" aria-hidden="true">
    <use href="#octicon-link"/>
  </svg>
</a>Basic Text Generation</h3>
<pre><code class="language-crystal"><span class="k">require</span> <span class="s">&quot;llama&quot;</span>

<span class="c"># Load a model</span>
model <span class="o">=</span> <span class="t">Llama</span><span class="t">::</span><span class="t">Model</span>.new(<span class="s">&quot;/path/to/model.gguf&quot;</span>)

<span class="c"># Create a context</span>
context <span class="o">=</span> model.context

<span class="c"># Generate text</span>
response <span class="o">=</span> context.generate(<span class="s">&quot;Once upon a time&quot;</span>, max_tokens: <span class="n">100</span>, temperature: <span class="n">0.8</span>)
puts response

<span class="c"># Or use the convenience method</span>
response <span class="o">=</span> <span class="t">Llama</span>.generate(<span class="s">&quot;/path/to/model.gguf&quot;</span>, <span class="s">&quot;Once upon a time&quot;</span>)
puts response</code></pre>
<h3><a id="advanced-sampling" class="anchor" href="#advanced-sampling">
  <svg class="octicon-link" aria-hidden="true">
    <use href="#octicon-link"/>
  </svg>
</a>Advanced Sampling</h3>
<pre><code class="language-crystal"><span class="k">require</span> <span class="s">&quot;llama&quot;</span>

model <span class="o">=</span> <span class="t">Llama</span><span class="t">::</span><span class="t">Model</span>.new(<span class="s">&quot;/path/to/model.gguf&quot;</span>)
context <span class="o">=</span> model.context

<span class="c"># Create a sampler chain with multiple sampling methods</span>
chain <span class="o">=</span> <span class="t">Llama</span><span class="t">::</span><span class="t">Sampler</span><span class="t">::</span><span class="t">Chain</span>.new
chain.add(<span class="t">Llama</span><span class="t">::</span><span class="t">Sampler</span><span class="t">::</span><span class="t">TopK</span>.new(<span class="n">40</span>))
chain.add(<span class="t">Llama</span><span class="t">::</span><span class="t">Sampler</span><span class="t">::</span><span class="t">MinP</span>.new(<span class="n">0.05</span>, <span class="n">1</span>))
chain.add(<span class="t">Llama</span><span class="t">::</span><span class="t">Sampler</span><span class="t">::</span><span class="t">Temp</span>.new(<span class="n">0.8</span>))
chain.add(<span class="t">Llama</span><span class="t">::</span><span class="t">Sampler</span><span class="t">::</span><span class="t">Dist</span>.new(<span class="n">42</span>))

<span class="c"># Generate text with the custom sampler chain</span>
result <span class="o">=</span> context.generate_with_sampler(<span class="s">&quot;Write a short poem about AI:&quot;</span>, chain, <span class="n">150</span>)
puts result</code></pre>
<h3><a id="chat-conversations" class="anchor" href="#chat-conversations">
  <svg class="octicon-link" aria-hidden="true">
    <use href="#octicon-link"/>
  </svg>
</a>Chat Conversations</h3>
<pre><code class="language-crystal"><span class="k">require</span> <span class="s">&quot;llama&quot;</span>
<span class="k">require</span> <span class="s">&quot;llama/chat&quot;</span>

model <span class="o">=</span> <span class="t">Llama</span><span class="t">::</span><span class="t">Model</span>.new(<span class="s">&quot;/path/to/model.gguf&quot;</span>)
context <span class="o">=</span> model.context

<span class="c"># Create a chat conversation</span>
messages <span class="o">=</span> [
  <span class="t">Llama</span><span class="t">::</span><span class="t">ChatMessage</span>.new(<span class="s">&quot;system&quot;</span>, <span class="s">&quot;You are a helpful assistant.&quot;</span>),
  <span class="t">Llama</span><span class="t">::</span><span class="t">ChatMessage</span>.new(<span class="s">&quot;user&quot;</span>, <span class="s">&quot;Hello, who are you?&quot;</span>)
]

<span class="c"># Generate a response</span>
response <span class="o">=</span> context.chat(messages)
puts <span class="s">&quot;Assistant: </span><span class="i">#{</span>response<span class="i">}</span><span class="s">&quot;</span>

<span class="c"># Continue the conversation</span>
messages <span class="o">&lt;&lt;</span> <span class="t">Llama</span><span class="t">::</span><span class="t">ChatMessage</span>.new(<span class="s">&quot;assistant&quot;</span>, response)
messages <span class="o">&lt;&lt;</span> <span class="t">Llama</span><span class="t">::</span><span class="t">ChatMessage</span>.new(<span class="s">&quot;user&quot;</span>, <span class="s">&quot;Tell me a joke&quot;</span>)
response <span class="o">=</span> context.chat(messages)
puts <span class="s">&quot;Assistant: </span><span class="i">#{</span>response<span class="i">}</span><span class="s">&quot;</span></code></pre>
<h3><a id="embeddings" class="anchor" href="#embeddings">
  <svg class="octicon-link" aria-hidden="true">
    <use href="#octicon-link"/>
  </svg>
</a>Embeddings</h3>
<pre><code class="language-crystal"><span class="k">require</span> <span class="s">&quot;llama&quot;</span>

model <span class="o">=</span> <span class="t">Llama</span><span class="t">::</span><span class="t">Model</span>.new(<span class="s">&quot;/path/to/model.gguf&quot;</span>)

<span class="c"># Create a context with embeddings enabled</span>
context <span class="o">=</span> model.context(embeddings: <span class="n">true</span>)

<span class="c"># Get embeddings for text</span>
text <span class="o">=</span> <span class="s">&quot;Hello, world!&quot;</span>
tokens <span class="o">=</span> model.vocab.tokenize(text)
batch <span class="o">=</span> <span class="t">Llama</span><span class="t">::</span><span class="t">Batch</span>.get_one(tokens)
context.decode(batch)
embeddings <span class="o">=</span> context.get_embeddings_seq(<span class="n">0</span>)

puts <span class="s">&quot;Embedding dimension: </span><span class="i">#{</span>embeddings.size<span class="i">}</span><span class="s">&quot;</span></code></pre>
<h3><a id="utilities" class="anchor" href="#utilities">
  <svg class="octicon-link" aria-hidden="true">
    <use href="#octicon-link"/>
  </svg>
</a>Utilities</h3>
<h4><a id="system-info" class="anchor" href="#system-info">
  <svg class="octicon-link" aria-hidden="true">
    <use href="#octicon-link"/>
  </svg>
</a>System Info</h4>
<pre><code class="language-crystal">puts <span class="t">Llama</span>.system_info</code></pre>
<h4><a id="tokenization-utility" class="anchor" href="#tokenization-utility">
  <svg class="octicon-link" aria-hidden="true">
    <use href="#octicon-link"/>
  </svg>
</a>Tokenization Utility</h4>
<pre><code class="language-crystal">model <span class="o">=</span> <span class="t">Llama</span><span class="t">::</span><span class="t">Model</span>.new(<span class="s">&quot;/path/to/model.gguf&quot;</span>)
puts <span class="t">Llama</span>.tokenize_and_format(model.vocab, <span class="s">&quot;Hello, world!&quot;</span>, ids_only: <span class="n">true</span>)</code></pre>
<h2><a id="examples" class="anchor" href="#examples">
  <svg class="octicon-link" aria-hidden="true">
    <use href="#octicon-link"/>
  </svg>
</a>Examples</h2>
<p>The <code>examples</code> directory contains sample code demonstrating various features:</p>
<ul>
<li><code>simple.cr</code> - Basic text generation</li>
<li><code>chat.cr</code> - Chat conversations with models</li>
<li><code>tokenize.cr</code> - Tokenization and vocabulary features</li>
</ul>
<h2><a id="api-documentation" class="anchor" href="#api-documentation">
  <svg class="octicon-link" aria-hidden="true">
    <use href="#octicon-link"/>
  </svg>
</a>API Documentation</h2>
<p>See <a href="https://kojix2.github.io/llama.cr">kojix2.github.io/llama.cr</a> for full API docs.</p>
<h3><a id="core-classes" class="anchor" href="#core-classes">
  <svg class="octicon-link" aria-hidden="true">
    <use href="#octicon-link"/>
  </svg>
</a>Core Classes</h3>
<ul>
<li><strong>Llama::Model</strong> - Represents a loaded LLaMA model</li>
<li><strong>Llama::Context</strong> - Handles inference state for a model</li>
<li><strong>Llama::Vocab</strong> - Provides access to the model's vocabulary</li>
<li><strong>Llama::Batch</strong> - Manages batches of tokens for efficient processing</li>
<li><strong>Llama::KvCache</strong> - Controls the key-value cache for optimized inference</li>
<li><strong>Llama::State</strong> - Handles saving and loading model state</li>
<li><strong>Llama::Sampler::Chain</strong> - Combines multiple sampling methods</li>
</ul>
<h3><a id="samplers" class="anchor" href="#samplers">
  <svg class="octicon-link" aria-hidden="true">
    <use href="#octicon-link"/>
  </svg>
</a>Samplers</h3>
<ul>
<li><strong>Llama::Sampler::TopK</strong> - Keeps only the top K most likely tokens</li>
<li><strong>Llama::Sampler::TopP</strong> - Nucleus sampling (keeps tokens until cumulative probability exceeds P)</li>
<li><strong>Llama::Sampler::Temp</strong> - Applies temperature to logits</li>
<li><strong>Llama::Sampler::Dist</strong> - Samples from the final probability distribution</li>
<li><strong>Llama::Sampler::MinP</strong> - Keeps tokens with probability &gt;= P * max_probability</li>
<li><strong>Llama::Sampler::Typical</strong> - Selects tokens based on their &quot;typicality&quot; (entropy)</li>
<li><strong>Llama::Sampler::Mirostat</strong> - Dynamically adjusts sampling to maintain target entropy</li>
<li><strong>Llama::Sampler::Penalties</strong> - Applies penalties to reduce repetition</li>
</ul>
<h2><a id="development" class="anchor" href="#development">
  <svg class="octicon-link" aria-hidden="true">
    <use href="#octicon-link"/>
  </svg>
</a>Development</h2>
<p>See <a href="DEVELOPMENT.md">DEVELOPMENT.md</a> for development guidelines.</p>
<h2><a id="contributing" class="anchor" href="#contributing">
  <svg class="octicon-link" aria-hidden="true">
    <use href="#octicon-link"/>
  </svg>
</a>Contributing</h2>
<ol>
<li>Fork it (<a href="https://github.com/kojix2/llama.cr/fork">https://github.com/kojix2/llama.cr/fork</a>)</li>
<li>Create your feature branch (<code>git checkout -b my-new-feature</code>)</li>
<li>Commit your changes (<code>git commit -am 'Add some feature'</code>)</li>
<li>Push to the branch (<code>git push origin my-new-feature</code>)</li>
<li>Create a new Pull Request</li>
</ol>
<h2><a id="license" class="anchor" href="#license">
  <svg class="octicon-link" aria-hidden="true">
    <use href="#octicon-link"/>
  </svg>
</a>License</h2>
<p>This project is available under the MIT License. See the LICENSE file for more info.</p>
</div>
</body>
</html>
