{"repository_name":"llama","body":"# llama.cr\n\nCrystal bindings for [llama.cpp](https://github.com/ggml-org/llama.cpp), a C/C++ implementation of LLaMA, Falcon, GPT-2, and other large language models.\n\n## Features\n\n- Low-level bindings to the llama.cpp C API\n- High-level Crystal wrapper classes for easy usage\n- Memory management for C resources\n- Simple text generation interface\n\n## Installation\n\n### Prerequisites\n\nBefore using this shard, you need to have llama.cpp compiled and installed on your system:\n\n1. Clone and build llama.cpp:\n\n   ```bash\n   git clone https://github.com/ggml-org/llama.cpp.git\n   cd llama.cpp\n   make\n   ```\n\n2. Install the library to your system (optional):\n   ```bash\n   sudo make install\n   ```\n\n### Obtaining GGUF Model Files\n\nYou'll need a model file in GGUF format to use with this library. Here are some options:\n\n1. **Download pre-converted models from Hugging Face**:\n\n   - [TheBloke's GGUF models](https://huggingface.co/TheBloke) - Large collection of models converted to GGUF format\n   - Popular models include:\n     - [Llama 3 8B Instruct](https://huggingface.co/TheBloke/Llama-3-8B-Instruct-GGUF)\n     - [Mistral 7B Instruct v0.2](https://huggingface.co/TheBloke/Mistral-7B-Instruct-v0.2-GGUF)\n     - [TinyLlama 1.1B](https://huggingface.co/TheBloke/TinyLlama-1.1B-Chat-v1.0-GGUF) (good for testing)\n\n2. **Convert models yourself using llama.cpp**:\n   ```bash\n   # Example: Converting a Hugging Face model to GGUF\n   python3 llama.cpp/convert.py /path/to/model --outfile model.gguf\n   ```\n\nFor testing, smaller quantized models (1-3B parameters) with Q4_K_M quantization are recommended.\n\n### Adding to Your Project\n\n1. Add the dependency to your `shard.yml`:\n\n   ```yaml\n   dependencies:\n     llama:\n       github: kojix2/llama.cr\n   ```\n\n2. Run `shards install`\n\n## Usage\n\n```crystal\nrequire \"llama\"\n\n# Load a model\nmodel = Llama::Model.new(\"/path/to/model.gguf\")\n\n# Create a context\ncontext = model.context\n\n# Generate text\nresponse = context.generate(\"Once upon a time\", max_tokens: 100, temperature: 0.8)\nputs response\n\n# Or use the convenience method\nresponse = Llama.generate(\"/path/to/model.gguf\", \"Once upon a time\")\nputs response\n```\n\n## Examples\n\nThe `examples` directory contains sample code demonstrating how to use the library:\n\n### Text Generation\n\nA simple example showing how to generate text from a prompt:\n\n```bash\ncrystal examples/text_generation.cr -- /path/to/model.gguf \"Once upon a time\"\n```\n\nOptional parameters:\n\n- Third argument: Maximum number of tokens to generate (default: 128)\n- Fourth argument: Temperature (default: 0.8)\n\n### Tokenization\n\nAn example demonstrating tokenization and vocabulary features:\n\n```bash\ncrystal examples/tokenization.cr -- /path/to/model.gguf \"Hello, world!\"\n```\n\nThis example shows:\n\n- How to tokenize text into token IDs\n- How to convert token IDs back to text\n- How to access special tokens in the vocabulary\n\n## API Documentation\n\n### Llama::Model\n\nThe `Model` class represents a loaded LLaMA model.\n\n```crystal\n# Load a model\nmodel = Llama::Model.new(\"/path/to/model.gguf\")\n\n# Get model information\nputs model.n_params  # Number of parameters\nputs model.n_embd    # Embedding size\nputs model.n_layer   # Number of layers\nputs model.n_head    # Number of attention heads\n```\n\n### Llama::Context\n\nThe `Context` class handles the inference state for a model.\n\n```crystal\n# Create a context\ncontext = model.context\n\n# Generate text\nresponse = context.generate(\"Hello, I am a\", max_tokens: 50, temperature: 0.7)\n```\n\n### Llama::Vocab\n\nThe `Vocab` class provides access to the model's vocabulary.\n\n```crystal\n# Get the vocabulary\nvocab = model.vocab\n\n# Tokenize text\ntokens = vocab.tokenize(\"Hello, world!\")\n\n# Convert tokens back to text\ntext = tokens.map { |token| vocab.token_to_text(token) }.join\n```\n\n## Development\n\nSee [DEVELOPMENT.md](DEVELOPMENT.md) for development guidelines.\n\n## Contributing\n\n1. Fork it (<https://github.com/kojix2/llama.cr/fork>)\n2. Create your feature branch (`git checkout -b my-new-feature`)\n3. Commit your changes (`git commit -am 'Add some feature'`)\n4. Push to the branch (`git push origin my-new-feature`)\n5. Create a new Pull Request\n\n## Contributors\n\n- [kojix2](https://github.com/kojix2) - creator and maintainer\n\n## License\n\nThis project is available under the MIT License. See the LICENSE file for more info.\n","program":{"html_id":"llama/toplevel","path":"toplevel.html","kind":"module","full_name":"Top Level Namespace","name":"Top Level Namespace","abstract":false,"locations":[],"repository_name":"llama","program":true,"enum":false,"alias":false,"const":false,"types":[{"html_id":"llama/Llama","path":"Llama.html","kind":"module","full_name":"Llama","name":"Llama","abstract":false,"locations":[{"filename":"src/llama.cr","line_number":11,"url":"https://github.com/kojix2/llama.cr/blob/d623ce41efcce0263db4f6cfe91d2c753aa45008/src/llama.cr#L11"},{"filename":"src/llama/context.cr","line_number":1,"url":"https://github.com/kojix2/llama.cr/blob/d623ce41efcce0263db4f6cfe91d2c753aa45008/src/llama/context.cr#L1"},{"filename":"src/llama/lib_llama.cr","line_number":1,"url":"https://github.com/kojix2/llama.cr/blob/d623ce41efcce0263db4f6cfe91d2c753aa45008/src/llama/lib_llama.cr#L1"},{"filename":"src/llama/model.cr","line_number":1,"url":"https://github.com/kojix2/llama.cr/blob/d623ce41efcce0263db4f6cfe91d2c753aa45008/src/llama/model.cr#L1"},{"filename":"src/llama/vocab.cr","line_number":1,"url":"https://github.com/kojix2/llama.cr/blob/d623ce41efcce0263db4f6cfe91d2c753aa45008/src/llama/vocab.cr#L1"}],"repository_name":"llama","program":false,"enum":false,"alias":false,"const":false,"constants":[{"id":"VERSION","name":"VERSION","value":"\"0.1.0\""}],"class_methods":[{"html_id":"generate(model_path:String,prompt:String,max_tokens:Int32=128,temperature:Float32=0.8):String-class-method","name":"generate","doc":"A simple example of text generation\n\nParameters:\n- model_path: Path to the model file (.gguf format)\n- prompt: The input prompt\n- max_tokens: Maximum number of tokens to generate\n- temperature: Sampling temperature (0.0 = greedy, 1.0 = more random)\n\nReturns:\n- The generated text","summary":"<p>A simple example of text generation</p>","abstract":false,"args":[{"name":"model_path","external_name":"model_path","restriction":"String"},{"name":"prompt","external_name":"prompt","restriction":"String"},{"name":"max_tokens","default_value":"128","external_name":"max_tokens","restriction":"Int32"},{"name":"temperature","default_value":"0.8","external_name":"temperature","restriction":"Float32"}],"args_string":"(model_path : String, prompt : String, max_tokens : Int32 = 128, temperature : Float32 = 0.8) : String","args_html":"(model_path : String, prompt : String, max_tokens : Int32 = <span class=\"n\">128</span>, temperature : Float32 = <span class=\"n\">0.8</span>) : String","location":{"filename":"src/llama.cr","line_number":29,"url":"https://github.com/kojix2/llama.cr/blob/d623ce41efcce0263db4f6cfe91d2c753aa45008/src/llama.cr#L29"},"def":{"name":"generate","args":[{"name":"model_path","external_name":"model_path","restriction":"String"},{"name":"prompt","external_name":"prompt","restriction":"String"},{"name":"max_tokens","default_value":"128","external_name":"max_tokens","restriction":"Int32"},{"name":"temperature","default_value":"0.8","external_name":"temperature","restriction":"Float32"}],"return_type":"String","visibility":"Public","body":"model = Model.new(model_path)\ncontext = model.context\ncontext.generate(prompt, max_tokens, temperature)\n"}},{"html_id":"system_info:String-class-method","name":"system_info","doc":"Returns the llama.cpp system information","summary":"<p>Returns the llama.cpp system information</p>","abstract":false,"location":{"filename":"src/llama.cr","line_number":15,"url":"https://github.com/kojix2/llama.cr/blob/d623ce41efcce0263db4f6cfe91d2c753aa45008/src/llama.cr#L15"},"def":{"name":"system_info","return_type":"String","visibility":"Public","body":"String.new(LibLlama.llama_print_system_info)"}}],"types":[{"html_id":"llama/Llama/Context","path":"Llama/Context.html","kind":"class","full_name":"Llama::Context","name":"Context","abstract":false,"superclass":{"html_id":"llama/Reference","kind":"class","full_name":"Reference","name":"Reference"},"ancestors":[{"html_id":"llama/Reference","kind":"class","full_name":"Reference","name":"Reference"},{"html_id":"llama/Object","kind":"class","full_name":"Object","name":"Object"}],"locations":[{"filename":"src/llama/context.cr","line_number":3,"url":"https://github.com/kojix2/llama.cr/blob/d623ce41efcce0263db4f6cfe91d2c753aa45008/src/llama/context.cr#L3"}],"repository_name":"llama","program":false,"enum":false,"alias":false,"const":false,"namespace":{"html_id":"llama/Llama","kind":"module","full_name":"Llama","name":"Llama"},"doc":"Wrapper for the llama_context structure","summary":"<p>Wrapper for the llama_context structure</p>","constructors":[{"html_id":"new(model:Model,params=nil)-class-method","name":"new","doc":"Creates a new Context instance for a model\n\nParameters:\n- model: The Model to create a context for\n- params: Optional context parameters\n\nRaises:\n- Llama::Error if the context cannot be created","summary":"<p>Creates a new Context instance for a model</p>","abstract":false,"args":[{"name":"model","external_name":"model","restriction":"Model"},{"name":"params","default_value":"nil","external_name":"params","restriction":""}],"args_string":"(model : Model, params = nil)","args_html":"(model : <a href=\"../Llama/Model.html\">Model</a>, params = <span class=\"n\">nil</span>)","location":{"filename":"src/llama/context.cr","line_number":12,"url":"https://github.com/kojix2/llama.cr/blob/d623ce41efcce0263db4f6cfe91d2c753aa45008/src/llama/context.cr#L12"},"def":{"name":"new","args":[{"name":"model","external_name":"model","restriction":"Model"},{"name":"params","default_value":"nil","external_name":"params","restriction":""}],"visibility":"Public","body":"_ = allocate\n_.initialize(model, params)\nif _.responds_to?(:finalize)\n  ::GC.add_finalizer(_)\nend\n_\n"}}],"instance_methods":[{"html_id":"decode(batch:LibLlama::LlamaBatch):Int32-instance-method","name":"decode","doc":"Processes a batch of tokens\n\nParameters:\n- batch: The batch to process\n\nReturns:\n- 0 on success\n- 1 if no KV slot was found for the batch\n- < 0 on error\n\nRaises:\n- Llama::Error on error","summary":"<p>Processes a batch of tokens</p>","abstract":false,"args":[{"name":"batch","external_name":"batch","restriction":"LibLlama::LlamaBatch"}],"args_string":"(batch : LibLlama::LlamaBatch) : Int32","args_html":"(batch : LibLlama::LlamaBatch) : Int32","location":{"filename":"src/llama/context.cr","line_number":31,"url":"https://github.com/kojix2/llama.cr/blob/d623ce41efcce0263db4f6cfe91d2c753aa45008/src/llama/context.cr#L31"},"def":{"name":"decode","args":[{"name":"batch","external_name":"batch","restriction":"LibLlama::LlamaBatch"}],"return_type":"Int32","visibility":"Public","body":"result = LibLlama.llama_decode(@handle, batch)\nif result < 0\n  raise(Error.new(\"Failed to decode batch\"))\nend\nresult\n"}},{"html_id":"finalize-instance-method","name":"finalize","doc":"Frees the resources associated with this context","summary":"<p>Frees the resources associated with this context</p>","abstract":false,"location":{"filename":"src/llama/context.cr","line_number":109,"url":"https://github.com/kojix2/llama.cr/blob/d623ce41efcce0263db4f6cfe91d2c753aa45008/src/llama/context.cr#L109"},"def":{"name":"finalize","visibility":"Public","body":"if @handle.null?\nelse\n  LibLlama.llama_free(@handle)\nend"}},{"html_id":"generate(prompt:String,max_tokens:Int32=128,temperature:Float32=0.8):String-instance-method","name":"generate","doc":"Generates text from a prompt\n\nParameters:\n- prompt: The input prompt\n- max_tokens: Maximum number of tokens to generate\n- temperature: Sampling temperature (0.0 = greedy, 1.0 = more random)\n\nReturns:\n- The generated text","summary":"<p>Generates text from a prompt</p>","abstract":false,"args":[{"name":"prompt","external_name":"prompt","restriction":"String"},{"name":"max_tokens","default_value":"128","external_name":"max_tokens","restriction":"Int32"},{"name":"temperature","default_value":"0.8","external_name":"temperature","restriction":"Float32"}],"args_string":"(prompt : String, max_tokens : Int32 = 128, temperature : Float32 = 0.8) : String","args_html":"(prompt : String, max_tokens : Int32 = <span class=\"n\">128</span>, temperature : Float32 = <span class=\"n\">0.8</span>) : String","location":{"filename":"src/llama/context.cr","line_number":54,"url":"https://github.com/kojix2/llama.cr/blob/d623ce41efcce0263db4f6cfe91d2c753aa45008/src/llama/context.cr#L54"},"def":{"name":"generate","args":[{"name":"prompt","external_name":"prompt","restriction":"String"},{"name":"max_tokens","default_value":"128","external_name":"max_tokens","restriction":"Int32"},{"name":"temperature","default_value":"0.8","external_name":"temperature","restriction":"Float32"}],"return_type":"String","visibility":"Public","body":"tokens = @model.vocab.tokenize(prompt)\nbatch = LibLlama::LlamaBatch.new\nbatch.n_tokens = tokens.size\ntoken_ptr = Pointer(LibLlama::LlamaToken).malloc(tokens.size)\npos_ptr = Pointer(LibLlama::LlamaPos).malloc(tokens.size)\nlogits_ptr = Pointer(Int8).malloc(tokens.size)\ntokens.each_with_index do |token, i|\n  token_ptr[i] = token\n  pos_ptr[i] = i.to_i32\n  logits_ptr[i] = i == (tokens.size - 1) ? 1_i8 : 0_i8\nend\nbatch.token = token_ptr\nbatch.pos = pos_ptr\nbatch.logits = logits_ptr\ndecode(batch)\nlogits = self.logits\nnext_token = 0\nmax_logit = -Float32::INFINITY\nn_vocab = @model.vocab.n_tokens\nn_vocab.times do |i|\n  if logits[i] > max_logit\n    max_logit = logits[i]\n    next_token = i\n  end\nend\n@model.vocab.token_to_text(next_token)\n"}},{"html_id":"logits:Pointer(Float32)-instance-method","name":"logits","doc":"Gets the logits for the last token\n\nReturns:\n- A pointer to the logits array","summary":"<p>Gets the logits for the last token</p>","abstract":false,"location":{"filename":"src/llama/context.cr","line_number":41,"url":"https://github.com/kojix2/llama.cr/blob/d623ce41efcce0263db4f6cfe91d2c753aa45008/src/llama/context.cr#L41"},"def":{"name":"logits","return_type":"Pointer(Float32)","visibility":"Public","body":"LibLlama.llama_get_logits(@handle)"}},{"html_id":"to_unsafe:Pointer(Llama::LibLlama::LlamaContext)-instance-method","name":"to_unsafe","doc":"Returns the raw pointer to the underlying llama_context structure","summary":"<p>Returns the raw pointer to the underlying llama_context structure</p>","abstract":false,"location":{"filename":"src/llama/context.cr","line_number":104,"url":"https://github.com/kojix2/llama.cr/blob/d623ce41efcce0263db4f6cfe91d2c753aa45008/src/llama/context.cr#L104"},"def":{"name":"to_unsafe","visibility":"Public","body":"@handle"}}]},{"html_id":"llama/Llama/Error","path":"Llama/Error.html","kind":"class","full_name":"Llama::Error","name":"Error","abstract":false,"superclass":{"html_id":"llama/Exception","kind":"class","full_name":"Exception","name":"Exception"},"ancestors":[{"html_id":"llama/Exception","kind":"class","full_name":"Exception","name":"Exception"},{"html_id":"llama/Reference","kind":"class","full_name":"Reference","name":"Reference"},{"html_id":"llama/Object","kind":"class","full_name":"Object","name":"Object"}],"locations":[{"filename":"src/llama/model.cr","line_number":3,"url":"https://github.com/kojix2/llama.cr/blob/d623ce41efcce0263db4f6cfe91d2c753aa45008/src/llama/model.cr#L3"}],"repository_name":"llama","program":false,"enum":false,"alias":false,"const":false,"namespace":{"html_id":"llama/Llama","kind":"module","full_name":"Llama","name":"Llama"},"doc":"Represents an error that occurred in the Llama library","summary":"<p>Represents an error that occurred in the Llama library</p>"},{"html_id":"llama/Llama/Model","path":"Llama/Model.html","kind":"class","full_name":"Llama::Model","name":"Model","abstract":false,"superclass":{"html_id":"llama/Reference","kind":"class","full_name":"Reference","name":"Reference"},"ancestors":[{"html_id":"llama/Reference","kind":"class","full_name":"Reference","name":"Reference"},{"html_id":"llama/Object","kind":"class","full_name":"Object","name":"Object"}],"locations":[{"filename":"src/llama/model.cr","line_number":6,"url":"https://github.com/kojix2/llama.cr/blob/d623ce41efcce0263db4f6cfe91d2c753aa45008/src/llama/model.cr#L6"}],"repository_name":"llama","program":false,"enum":false,"alias":false,"const":false,"namespace":{"html_id":"llama/Llama","kind":"module","full_name":"Llama","name":"Llama"},"doc":"Wrapper for the llama_model structure","summary":"<p>Wrapper for the llama_model structure</p>","constructors":[{"html_id":"new(path:String)-class-method","name":"new","doc":"Creates a new Model instance by loading a model from a file\n\nParameters:\n- path: Path to the model file (.gguf format)\n\nRaises:\n- Llama::Error if the model cannot be loaded","summary":"<p>Creates a new Model instance by loading a model from a file</p>","abstract":false,"args":[{"name":"path","external_name":"path","restriction":"String"}],"args_string":"(path : String)","args_html":"(path : String)","location":{"filename":"src/llama/model.cr","line_number":14,"url":"https://github.com/kojix2/llama.cr/blob/d623ce41efcce0263db4f6cfe91d2c753aa45008/src/llama/model.cr#L14"},"def":{"name":"new","args":[{"name":"path","external_name":"path","restriction":"String"}],"visibility":"Public","body":"_ = allocate\n_.initialize(path)\nif _.responds_to?(:finalize)\n  ::GC.add_finalizer(_)\nend\n_\n"}}],"instance_methods":[{"html_id":"context(params=nil):Context-instance-method","name":"context","doc":"Creates a new Context for this model","summary":"<p>Creates a new Context for this model</p>","abstract":false,"args":[{"name":"params","default_value":"nil","external_name":"params","restriction":""}],"args_string":"(params = nil) : Context","args_html":"(params = <span class=\"n\">nil</span>) : <a href=\"../Llama/Context.html\">Context</a>","location":{"filename":"src/llama/model.cr","line_number":47,"url":"https://github.com/kojix2/llama.cr/blob/d623ce41efcce0263db4f6cfe91d2c753aa45008/src/llama/model.cr#L47"},"def":{"name":"context","args":[{"name":"params","default_value":"nil","external_name":"params","restriction":""}],"return_type":"Context","visibility":"Public","body":"params || (params = LibLlama.llama_context_default_params)\nContext.new(self, params)\n"}},{"html_id":"finalize-instance-method","name":"finalize","doc":"Frees the resources associated with this model","summary":"<p>Frees the resources associated with this model</p>","abstract":false,"location":{"filename":"src/llama/model.cr","line_number":58,"url":"https://github.com/kojix2/llama.cr/blob/d623ce41efcce0263db4f6cfe91d2c753aa45008/src/llama/model.cr#L58"},"def":{"name":"finalize","visibility":"Public","body":"if @handle.null?\nelse\n  LibLlama.llama_model_free(@handle)\nend"}},{"html_id":"n_embd:Int32-instance-method","name":"n_embd","doc":"Returns the number of embedding dimensions in the model","summary":"<p>Returns the number of embedding dimensions in the model</p>","abstract":false,"location":{"filename":"src/llama/model.cr","line_number":32,"url":"https://github.com/kojix2/llama.cr/blob/d623ce41efcce0263db4f6cfe91d2c753aa45008/src/llama/model.cr#L32"},"def":{"name":"n_embd","return_type":"Int32","visibility":"Public","body":"LibLlama.llama_model_n_embd(@handle)"}},{"html_id":"n_head:Int32-instance-method","name":"n_head","doc":"Returns the number of attention heads in the model","summary":"<p>Returns the number of attention heads in the model</p>","abstract":false,"location":{"filename":"src/llama/model.cr","line_number":42,"url":"https://github.com/kojix2/llama.cr/blob/d623ce41efcce0263db4f6cfe91d2c753aa45008/src/llama/model.cr#L42"},"def":{"name":"n_head","return_type":"Int32","visibility":"Public","body":"LibLlama.llama_model_n_head(@handle)"}},{"html_id":"n_layer:Int32-instance-method","name":"n_layer","doc":"Returns the number of layers in the model","summary":"<p>Returns the number of layers in the model</p>","abstract":false,"location":{"filename":"src/llama/model.cr","line_number":37,"url":"https://github.com/kojix2/llama.cr/blob/d623ce41efcce0263db4f6cfe91d2c753aa45008/src/llama/model.cr#L37"},"def":{"name":"n_layer","return_type":"Int32","visibility":"Public","body":"LibLlama.llama_model_n_layer(@handle)"}},{"html_id":"n_params:UInt64-instance-method","name":"n_params","doc":"Returns the number of parameters in the model","summary":"<p>Returns the number of parameters in the model</p>","abstract":false,"location":{"filename":"src/llama/model.cr","line_number":27,"url":"https://github.com/kojix2/llama.cr/blob/d623ce41efcce0263db4f6cfe91d2c753aa45008/src/llama/model.cr#L27"},"def":{"name":"n_params","return_type":"UInt64","visibility":"Public","body":"LibLlama.llama_model_n_params(@handle)"}},{"html_id":"to_unsafe:Pointer(Llama::LibLlama::LlamaModel)-instance-method","name":"to_unsafe","doc":"Returns the raw pointer to the underlying llama_model structure","summary":"<p>Returns the raw pointer to the underlying llama_model structure</p>","abstract":false,"location":{"filename":"src/llama/model.cr","line_number":53,"url":"https://github.com/kojix2/llama.cr/blob/d623ce41efcce0263db4f6cfe91d2c753aa45008/src/llama/model.cr#L53"},"def":{"name":"to_unsafe","visibility":"Public","body":"@handle"}},{"html_id":"vocab:Vocab-instance-method","name":"vocab","doc":"Returns the vocabulary associated with this model","summary":"<p>Returns the vocabulary associated with this model</p>","abstract":false,"location":{"filename":"src/llama/model.cr","line_number":21,"url":"https://github.com/kojix2/llama.cr/blob/d623ce41efcce0263db4f6cfe91d2c753aa45008/src/llama/model.cr#L21"},"def":{"name":"vocab","return_type":"Vocab","visibility":"Public","body":"vocab_ptr = LibLlama.llama_model_get_vocab(@handle)\nVocab.new(vocab_ptr)\n"}}]},{"html_id":"llama/Llama/Vocab","path":"Llama/Vocab.html","kind":"class","full_name":"Llama::Vocab","name":"Vocab","abstract":false,"superclass":{"html_id":"llama/Reference","kind":"class","full_name":"Reference","name":"Reference"},"ancestors":[{"html_id":"llama/Reference","kind":"class","full_name":"Reference","name":"Reference"},{"html_id":"llama/Object","kind":"class","full_name":"Object","name":"Object"}],"locations":[{"filename":"src/llama/vocab.cr","line_number":3,"url":"https://github.com/kojix2/llama.cr/blob/d623ce41efcce0263db4f6cfe91d2c753aa45008/src/llama/vocab.cr#L3"}],"repository_name":"llama","program":false,"enum":false,"alias":false,"const":false,"namespace":{"html_id":"llama/Llama","kind":"module","full_name":"Llama","name":"Llama"},"doc":"Wrapper for the llama_vocab structure","summary":"<p>Wrapper for the llama_vocab structure</p>","constructors":[{"html_id":"new(handle:Pointer(LibLlama::LlamaVocab))-class-method","name":"new","doc":"Creates a new Vocab instance from a raw pointer\n\nNote: This constructor is intended for internal use.\nUsers should obtain Vocab instances through Model#vocab.","summary":"<p>Creates a new Vocab instance from a raw pointer</p>","abstract":false,"args":[{"name":"handle","external_name":"handle","restriction":"::Pointer(LibLlama::LlamaVocab)"}],"args_string":"(handle : Pointer(LibLlama::LlamaVocab))","args_html":"(handle : Pointer(LibLlama::LlamaVocab))","location":{"filename":"src/llama/vocab.cr","line_number":8,"url":"https://github.com/kojix2/llama.cr/blob/d623ce41efcce0263db4f6cfe91d2c753aa45008/src/llama/vocab.cr#L8"},"def":{"name":"new","args":[{"name":"handle","external_name":"handle","restriction":"::Pointer(LibLlama::LlamaVocab)"}],"visibility":"Public","body":"_ = allocate\n_.initialize(handle)\nif _.responds_to?(:finalize)\n  ::GC.add_finalizer(_)\nend\n_\n"}}],"instance_methods":[{"html_id":"n_tokens:Int32-instance-method","name":"n_tokens","doc":"Returns the number of tokens in the vocabulary","summary":"<p>Returns the number of tokens in the vocabulary</p>","abstract":false,"location":{"filename":"src/llama/vocab.cr","line_number":12,"url":"https://github.com/kojix2/llama.cr/blob/d623ce41efcce0263db4f6cfe91d2c753aa45008/src/llama/vocab.cr#L12"},"def":{"name":"n_tokens","return_type":"Int32","visibility":"Public","body":"LibLlama.llama_vocab_n_tokens(@handle)"}},{"html_id":"to_unsafe:Pointer(Llama::LibLlama::LlamaVocab)-instance-method","name":"to_unsafe","doc":"Returns the raw pointer to the underlying llama_vocab structure","summary":"<p>Returns the raw pointer to the underlying llama_vocab structure</p>","abstract":false,"location":{"filename":"src/llama/vocab.cr","line_number":64,"url":"https://github.com/kojix2/llama.cr/blob/d623ce41efcce0263db4f6cfe91d2c753aa45008/src/llama/vocab.cr#L64"},"def":{"name":"to_unsafe","visibility":"Public","body":"@handle"}},{"html_id":"token_to_text(token:Int32):String-instance-method","name":"token_to_text","doc":"Returns the text representation of a token","summary":"<p>Returns the text representation of a token</p>","abstract":false,"args":[{"name":"token","external_name":"token","restriction":"Int32"}],"args_string":"(token : Int32) : String","args_html":"(token : Int32) : String","location":{"filename":"src/llama/vocab.cr","line_number":17,"url":"https://github.com/kojix2/llama.cr/blob/d623ce41efcce0263db4f6cfe91d2c753aa45008/src/llama/vocab.cr#L17"},"def":{"name":"token_to_text","args":[{"name":"token","external_name":"token","restriction":"Int32"}],"return_type":"String","visibility":"Public","body":"ptr = LibLlama.llama_vocab_get_text(@handle, token)\nString.new(ptr)\n"}},{"html_id":"tokenize(text:String,add_special:Bool=true,parse_special:Bool=true):Array(Int32)-instance-method","name":"tokenize","doc":"Tokenizes a string into an array of token IDs","summary":"<p>Tokenizes a string into an array of token IDs</p>","abstract":false,"args":[{"name":"text","external_name":"text","restriction":"String"},{"name":"add_special","default_value":"true","external_name":"add_special","restriction":"Bool"},{"name":"parse_special","default_value":"true","external_name":"parse_special","restriction":"Bool"}],"args_string":"(text : String, add_special : Bool = true, parse_special : Bool = true) : Array(Int32)","args_html":"(text : String, add_special : Bool = <span class=\"n\">true</span>, parse_special : Bool = <span class=\"n\">true</span>) : Array(Int32)","location":{"filename":"src/llama/vocab.cr","line_number":23,"url":"https://github.com/kojix2/llama.cr/blob/d623ce41efcce0263db4f6cfe91d2c753aa45008/src/llama/vocab.cr#L23"},"def":{"name":"tokenize","args":[{"name":"text","external_name":"text","restriction":"String"},{"name":"add_special","default_value":"true","external_name":"add_special","restriction":"Bool"},{"name":"parse_special","default_value":"true","external_name":"parse_special","restriction":"Bool"}],"return_type":"Array(Int32)","visibility":"Public","body":"max_tokens = text.size * 2\ntokens = Pointer(LibLlama::LlamaToken).malloc(max_tokens)\nn_tokens = LibLlama.llama_tokenize(@handle, text, text.bytesize, tokens, max_tokens, add_special, parse_special)\nif n_tokens < 0\n  max_tokens = -n_tokens\n  tokens = Pointer(LibLlama::LlamaToken).malloc(max_tokens)\n  n_tokens = LibLlama.llama_tokenize(@handle, text, text.bytesize, tokens, max_tokens, add_special, parse_special)\nend\nif n_tokens < 0\n  raise(Error.new(\"Failed to tokenize text\"))\nend\nresult = Array(Int32).new(n_tokens)\nn_tokens.times do |i|\n  result << tokens[i]\nend\nresult\n"}}]}]}]}}