<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="utf-8" />
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1.0">
<meta name="generator" content="Crystal Docs 1.15.1">
<meta name="crystal_docs.project_version" content="main">
<meta name="crystal_docs.project_name" content="llama">



<link href="../css/style.css" rel="stylesheet" type="text/css" />
<script type="text/javascript" src="../js/doc.js"></script>

  <meta name="repository-name" content="llama">
  <title>Llama::Context - llama main</title>
  <script type="text/javascript">
    CrystalDocs.base_path = "../";
  </script>
</head>
<body>

<svg class="hidden">
  <symbol id="octicon-link" viewBox="0 0 16 16">
    <path fill="currentColor" fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path>
  </symbol>
</svg>
<input type="checkbox" id="sidebar-btn">
<label for="sidebar-btn" id="sidebar-btn-label">
  <svg class="open" xmlns="http://www.w3.org/2000/svg" height="2em" width="2em" viewBox="0 0 512 512"><title>Open Sidebar</title><path fill="currentColor" d="M80 96v64h352V96H80zm0 112v64h352v-64H80zm0 112v64h352v-64H80z"></path></svg>
  <svg class="close" xmlns="http://www.w3.org/2000/svg" width="2em" height="2em" viewBox="0 0 512 512"><title>Close Sidebar</title><path fill="currentColor" d="m118.6 73.4-45.2 45.2L210.7 256 73.4 393.4l45.2 45.2L256 301.3l137.4 137.3 45.2-45.2L301.3 256l137.3-137.4-45.2-45.2L256 210.7Z"></path></svg>
</label>
<div class="sidebar">
  <div class="sidebar-header">
    <div class="search-box">
      <input type="search" class="search-input" placeholder="Search..." spellcheck="false" aria-label="Search">
    </div>

    <div class="project-summary">
      <h1 class="project-name">
        <a href="../index.html">
          llama
        </a>
      </h1>

      <span class="project-version">
        main
      </span>
    </div>
  </div>

  <div class="search-results hidden">
    <ul class="search-list"></ul>
  </div>

  <div class="types-list">
    <ul>
  
  <li class="parent open current" data-id="llama/Llama" data-name="llama">
      <a href="../Llama.html">Llama</a>
      
        <ul>
  
  <li class="parent " data-id="llama/Llama/AdapterLora" data-name="llama::adapterlora">
      <a href="../Llama/AdapterLora.html">AdapterLora</a>
      
        <ul>
  
  <li class=" " data-id="llama/Llama/AdapterLora/Error" data-name="llama::adapterlora::error">
      <a href="../Llama/AdapterLora/Error.html">Error</a>
      
    </li>
  
</ul>

      
    </li>
  
  <li class="parent " data-id="llama/Llama/Batch" data-name="llama::batch">
      <a href="../Llama/Batch.html">Batch</a>
      
        <ul>
  
  <li class=" " data-id="llama/Llama/Batch/Error" data-name="llama::batch::error">
      <a href="../Llama/Batch/Error.html">Error</a>
      
    </li>
  
</ul>

      
    </li>
  
  <li class=" " data-id="llama/Llama/ChatMessage" data-name="llama::chatmessage">
      <a href="../Llama/ChatMessage.html">ChatMessage</a>
      
    </li>
  
  <li class="parent open current" data-id="llama/Llama/Context" data-name="llama::context">
      <a href="../Llama/Context.html">Context</a>
      
        <ul>
  
  <li class=" " data-id="llama/Llama/Context/Error" data-name="llama::context::error">
      <a href="../Llama/Context/Error.html">Error</a>
      
    </li>
  
  <li class=" " data-id="llama/Llama/Context/TokenizationError" data-name="llama::context::tokenizationerror">
      <a href="../Llama/Context/TokenizationError.html">TokenizationError</a>
      
    </li>
  
</ul>

      
    </li>
  
  <li class=" " data-id="llama/Llama/Error" data-name="llama::error">
      <a href="../Llama/Error.html">Error</a>
      
    </li>
  
  <li class="parent " data-id="llama/Llama/Memory" data-name="llama::memory">
      <a href="../Llama/Memory.html">Memory</a>
      
        <ul>
  
  <li class=" " data-id="llama/Llama/Memory/Error" data-name="llama::memory::error">
      <a href="../Llama/Memory/Error.html">Error</a>
      
    </li>
  
</ul>

      
    </li>
  
  <li class="parent " data-id="llama/Llama/Model" data-name="llama::model">
      <a href="../Llama/Model.html">Model</a>
      
        <ul>
  
  <li class=" " data-id="llama/Llama/Model/Error" data-name="llama::model::error">
      <a href="../Llama/Model/Error.html">Error</a>
      
    </li>
  
</ul>

      
    </li>
  
  <li class="parent " data-id="llama/Llama/Sampler" data-name="llama::sampler">
      <a href="../Llama/Sampler.html">Sampler</a>
      
        <ul>
  
  <li class=" " data-id="llama/Llama/Sampler/Base" data-name="llama::sampler::base">
      <a href="../Llama/Sampler/Base.html">Base</a>
      
    </li>
  
  <li class=" " data-id="llama/Llama/Sampler/Dist" data-name="llama::sampler::dist">
      <a href="../Llama/Sampler/Dist.html">Dist</a>
      
    </li>
  
  <li class=" " data-id="llama/Llama/Sampler/Error" data-name="llama::sampler::error">
      <a href="../Llama/Sampler/Error.html">Error</a>
      
    </li>
  
  <li class=" " data-id="llama/Llama/Sampler/Grammar" data-name="llama::sampler::grammar">
      <a href="../Llama/Sampler/Grammar.html">Grammar</a>
      
    </li>
  
  <li class=" " data-id="llama/Llama/Sampler/GrammarLazyPatterns" data-name="llama::sampler::grammarlazypatterns">
      <a href="../Llama/Sampler/GrammarLazyPatterns.html">GrammarLazyPatterns</a>
      
    </li>
  
  <li class=" " data-id="llama/Llama/Sampler/Greedy" data-name="llama::sampler::greedy">
      <a href="../Llama/Sampler/Greedy.html">Greedy</a>
      
    </li>
  
  <li class=" " data-id="llama/Llama/Sampler/Infill" data-name="llama::sampler::infill">
      <a href="../Llama/Sampler/Infill.html">Infill</a>
      
    </li>
  
  <li class=" " data-id="llama/Llama/Sampler/MinP" data-name="llama::sampler::minp">
      <a href="../Llama/Sampler/MinP.html">MinP</a>
      
    </li>
  
  <li class=" " data-id="llama/Llama/Sampler/Mirostat" data-name="llama::sampler::mirostat">
      <a href="../Llama/Sampler/Mirostat.html">Mirostat</a>
      
    </li>
  
  <li class=" " data-id="llama/Llama/Sampler/MirostatV2" data-name="llama::sampler::mirostatv2">
      <a href="../Llama/Sampler/MirostatV2.html">MirostatV2</a>
      
    </li>
  
  <li class=" " data-id="llama/Llama/Sampler/Penalties" data-name="llama::sampler::penalties">
      <a href="../Llama/Sampler/Penalties.html">Penalties</a>
      
    </li>
  
  <li class=" " data-id="llama/Llama/Sampler/Temp" data-name="llama::sampler::temp">
      <a href="../Llama/Sampler/Temp.html">Temp</a>
      
    </li>
  
  <li class=" " data-id="llama/Llama/Sampler/TempExt" data-name="llama::sampler::tempext">
      <a href="../Llama/Sampler/TempExt.html">TempExt</a>
      
    </li>
  
  <li class=" " data-id="llama/Llama/Sampler/TokenizationError" data-name="llama::sampler::tokenizationerror">
      <a href="../Llama/Sampler/TokenizationError.html">TokenizationError</a>
      
    </li>
  
  <li class=" " data-id="llama/Llama/Sampler/TopK" data-name="llama::sampler::topk">
      <a href="../Llama/Sampler/TopK.html">TopK</a>
      
    </li>
  
  <li class=" " data-id="llama/Llama/Sampler/TopNSigma" data-name="llama::sampler::topnsigma">
      <a href="../Llama/Sampler/TopNSigma.html">TopNSigma</a>
      
    </li>
  
  <li class=" " data-id="llama/Llama/Sampler/TopP" data-name="llama::sampler::topp">
      <a href="../Llama/Sampler/TopP.html">TopP</a>
      
    </li>
  
  <li class=" " data-id="llama/Llama/Sampler/Typical" data-name="llama::sampler::typical">
      <a href="../Llama/Sampler/Typical.html">Typical</a>
      
    </li>
  
  <li class=" " data-id="llama/Llama/Sampler/Xtc" data-name="llama::sampler::xtc">
      <a href="../Llama/Sampler/Xtc.html">Xtc</a>
      
    </li>
  
</ul>

      
    </li>
  
  <li class=" " data-id="llama/Llama/SamplerChain" data-name="llama::samplerchain">
      <a href="../Llama/SamplerChain.html">SamplerChain</a>
      
    </li>
  
  <li class="parent " data-id="llama/Llama/State" data-name="llama::state">
      <a href="../Llama/State.html">State</a>
      
        <ul>
  
  <li class=" " data-id="llama/Llama/State/Error" data-name="llama::state::error">
      <a href="../Llama/State/Error.html">Error</a>
      
    </li>
  
</ul>

      
    </li>
  
  <li class=" " data-id="llama/Llama/Vocab" data-name="llama::vocab">
      <a href="../Llama/Vocab.html">Vocab</a>
      
    </li>
  
</ul>

      
    </li>
  
</ul>

  </div>
</div>


<div class="main-content">
<h1 class="type-name">

  <span class="kind">
    class
  </span> Llama::<wbr>Context

</h1>


  <ul class="superclass-hierarchy"><li class="superclass"><a href="../Llama/Context.html">Llama::Context</a></li><li class="superclass">Reference</li><li class="superclass">Object</li></ul>




  <h2>
    <a id="overview" class="anchor" href="#overview">
  <svg class="octicon-link" aria-hidden="true">
    <use href="#octicon-link"/>
  </svg>
</a>
    Overview
  </h2>

  <p>Wrapper for the llama_context structure</p>
















  <h2>
    <a id="defined-in" class="anchor" href="#defined-in">
  <svg class="octicon-link" aria-hidden="true">
    <use href="#octicon-link"/>
  </svg>
</a>
    Defined in:
  </h2>
  
    
      <a href="https://github.com/kojix2/llama.cr/blob/85572b7dcb2cf4bca0b3f881f0546a93e2892ef1/src/llama/context.cr#L5" target="_blank">
        llama/context.cr
      </a>
    
    <br/>
  
    
      <a href="https://github.com/kojix2/llama.cr/blob/85572b7dcb2cf4bca0b3f881f0546a93e2892ef1/src/llama/context/error.cr#L4" target="_blank">
        llama/context/error.cr
      </a>
    
    <br/>
  





  
  <h2>
    <a id="constructors" class="anchor" href="#constructors">
  <svg class="octicon-link" aria-hidden="true">
    <use href="#octicon-link"/>
  </svg>
</a>
    Constructors
  </h2>
  <ul class="list-summary">
    
      <li class="entry-summary">
        <a href="#new%28model%3AModel%2Cn_ctx%3AUInt32%3D0%2Cn_batch%3AUInt32%3D512%2Cn_threads%3AInt32%3D0%2Cn_threads_batch%3AInt32%3D0%2Cembeddings%3ABool%3Dfalse%2Coffload_kqv%3ABool%3Dfalse%29-class-method" class="signature"><strong>.new</strong>(model : Model, n_ctx : UInt32 = <span class="n">0</span>, n_batch : UInt32 = <span class="n">512</span>, n_threads : Int32 = <span class="n">0</span>, n_threads_batch : Int32 = <span class="n">0</span>, embeddings : Bool = <span class="n">false</span>, offload_kqv : Bool = <span class="n">false</span>)</a>
        
          <div class="summary"><p>Creates a new Context instance for a model.</p></div>
        
      </li>
    
  </ul>


  

  

  
  <h2>
    <a id="instance-method-summary" class="anchor" href="#instance-method-summary">
  <svg class="octicon-link" aria-hidden="true">
    <use href="#octicon-link"/>
  </svg>
</a>
    Instance Method Summary
  </h2>
  <ul class="list-summary">
    
      <li class="entry-summary">
        <a href="#apply_adapter_cvec%28data%3ASlice%28Float32%29%2Cn_embd%3AInt32%2Cil_start%3AInt32%2Cil_end%3AInt32%29%3AInt32-instance-method" class="signature"><strong>#apply_adapter_cvec</strong>(data : Slice(Float32), n_embd : Int32, il_start : Int32, il_end : Int32) : Int32</a>
        
          <div class="summary"><p>Applies a control vector to the LoRA adapter</p></div>
        
      </li>
    
      <li class="entry-summary">
        <a href="#apply_chat_template%28messages%3AArray%28ChatMessage%29%2Cadd_assistant%3ABool%3Dtrue%2Ctemplate%3AString%7CNil%3Dnil%29%3AString-instance-method" class="signature"><strong>#apply_chat_template</strong>(messages : Array(ChatMessage), add_assistant : Bool = <span class="n">true</span>, template : String | Nil = <span class="n">nil</span>) : String</a>
        
          <div class="summary"><p>Applies the chat template to the given messages and returns the formatted prompt.</p></div>
        
      </li>
    
      <li class="entry-summary">
        <a href="#attach_adapter_lora%28adapter%3AAdapterLora%2Cscale%3AFloat32%3D1.0%29%3AInt32-instance-method" class="signature"><strong>#attach_adapter_lora</strong>(adapter : AdapterLora, scale : Float32 = <span class="n">1.0</span>) : Int32</a>
        
          <div class="summary"><p>Attaches a LoRA adapter to this context</p></div>
        
      </li>
    
      <li class="entry-summary">
        <a href="#chat%28messages%3AArray%28ChatMessage%29%2Cmax_tokens%3AInt32%3D128%2Ctemperature%3AFloat32%3D0.8%2Ctemplate%3AString%7CNil%3Dnil%29%3AString-instance-method" class="signature"><strong>#chat</strong>(messages : Array(ChatMessage), max_tokens : Int32 = <span class="n">128</span>, temperature : Float32 = <span class="n">0.8</span>, template : String | Nil = <span class="n">nil</span>) : String</a>
        
          <div class="summary"><p>Generates a response in a chat conversation</p></div>
        
      </li>
    
      <li class="entry-summary">
        <a href="#clear_adapters_lora-instance-method" class="signature"><strong>#clear_adapters_lora</strong></a>
        
          <div class="summary"><p>Clears all LoRA adapters from this context</p></div>
        
      </li>
    
      <li class="entry-summary">
        <a href="#decode%28batch%3ALibLlama%3A%3ALlamaBatch%7CBatch%29%3AInt32-instance-method" class="signature"><strong>#decode</strong>(batch : LibLlama::LlamaBatch | Batch) : Int32</a>
        
          <div class="summary"><p>Processes a batch of tokens with the decoder part of the model</p></div>
        
      </li>
    
      <li class="entry-summary">
        <a href="#detach_adapter_lora%28adapter%3AAdapterLora%29%3AInt32-instance-method" class="signature"><strong>#detach_adapter_lora</strong>(adapter : AdapterLora) : Int32</a>
        
          <div class="summary"><p>Detaches a LoRA adapter from this context</p></div>
        
      </li>
    
      <li class="entry-summary">
        <a href="#encode%28batch%3ALibLlama%3A%3ALlamaBatch%7CBatch%29%3AInt32-instance-method" class="signature"><strong>#encode</strong>(batch : LibLlama::LlamaBatch | Batch) : Int32</a>
        
          <div class="summary"><p>Processes a batch of tokens with the encoder part of the model</p></div>
        
      </li>
    
      <li class="entry-summary">
        <a href="#finalize-instance-method" class="signature"><strong>#finalize</strong></a>
        
          <div class="summary"><p>Frees the resources associated with this context</p></div>
        
      </li>
    
      <li class="entry-summary">
        <a href="#generate%28prompt%3AString%2Cmax_tokens%3AInt32%3D128%2Ctemperature%3AFloat32%3D0.8%29%3AString-instance-method" class="signature"><strong>#generate</strong>(prompt : String, max_tokens : Int32 = <span class="n">128</span>, temperature : Float32 = <span class="n">0.8</span>) : String</a>
        
          <div class="summary"><p>Generates text from a prompt</p></div>
        
      </li>
    
      <li class="entry-summary">
        <a href="#generate_with_sampler%28prompt%3AString%2Csampler%3ASamplerChain%2Cmax_tokens%3AInt32%3D128%29%3AString-instance-method" class="signature"><strong>#generate_with_sampler</strong>(prompt : String, sampler : SamplerChain, max_tokens : Int32 = <span class="n">128</span>) : String</a>
        
          <div class="summary"><p>Generates text using a sampler chain</p></div>
        
      </li>
    
      <li class="entry-summary">
        <a href="#get_embeddings%3AArray%28Float32%29%7CNil-instance-method" class="signature"><strong>#get_embeddings</strong> : Array(Float32) | Nil</a>
        
          <div class="summary"><p>Gets all output token embeddings Only available when embeddings mode is enabled</p></div>
        
      </li>
    
      <li class="entry-summary">
        <a href="#get_embeddings_ith%28i%3AInt32%29%3AArray%28Float32%29%7CNil-instance-method" class="signature"><strong>#get_embeddings_ith</strong>(i : Int32) : Array(Float32) | Nil</a>
        
          <div class="summary"><p>Gets the embeddings for a specific token</p></div>
        
      </li>
    
      <li class="entry-summary">
        <a href="#get_embeddings_seq%28seq_id%3AInt32%29%3AArray%28Float32%29%7CNil-instance-method" class="signature"><strong>#get_embeddings_seq</strong>(seq_id : Int32) : Array(Float32) | Nil</a>
        
          <div class="summary"><p>Gets the embeddings for a specific sequence</p></div>
        
      </li>
    
      <li class="entry-summary">
        <a href="#logits%3APointer%28Float32%29-instance-method" class="signature"><strong>#logits</strong> : Pointer(Float32)</a>
        
          <div class="summary"><p>Gets the logits for the last token</p></div>
        
      </li>
    
      <li class="entry-summary">
        <a href="#memory%3AMemory-instance-method" class="signature"><strong>#memory</strong> : Memory</a>
        
          <div class="summary"><p>Returns the memory for this context (modern API)</p></div>
        
      </li>
    
      <li class="entry-summary">
        <a href="#n_batch%3AUInt32-instance-method" class="signature"><strong>#n_batch</strong> : UInt32</a>
        
          <div class="summary"><p>Returns the logical batch size (n_batch)</p></div>
        
      </li>
    
      <li class="entry-summary">
        <a href="#n_ctx%3AUInt32-instance-method" class="signature"><strong>#n_ctx</strong> : UInt32</a>
        
          <div class="summary"><p>Returns the context window size (n_ctx)</p></div>
        
      </li>
    
      <li class="entry-summary">
        <a href="#n_seq_max%3AUInt32-instance-method" class="signature"><strong>#n_seq_max</strong> : UInt32</a>
        
          <div class="summary"><p>Returns the maximum number of sequence IDs per token (n_seq_max)</p></div>
        
      </li>
    
      <li class="entry-summary">
        <a href="#n_threads%3AInt32-instance-method" class="signature"><strong>#n_threads</strong> : Int32</a>
        
          <div class="summary"><p>Returns the number of threads used for generation</p></div>
        
      </li>
    
      <li class="entry-summary">
        <a href="#n_threads_batch%3AInt32-instance-method" class="signature"><strong>#n_threads_batch</strong> : Int32</a>
        
          <div class="summary"><p>Returns the number of threads used for batch processing</p></div>
        
      </li>
    
      <li class="entry-summary">
        <a href="#n_ubatch%3AUInt32-instance-method" class="signature"><strong>#n_ubatch</strong> : UInt32</a>
        
          <div class="summary"><p>Returns the micro-batch size (n_ubatch)</p></div>
        
      </li>
    
      <li class="entry-summary">
        <a href="#pooling_type%3ALibLlama%3A%3ALlamaPoolingType-instance-method" class="signature"><strong>#pooling_type</strong> : LibLlama::LlamaPoolingType</a>
        
          <div class="summary"><p>Gets the pooling type used for embeddings</p></div>
        
      </li>
    
      <li class="entry-summary">
        <a href="#print_perf-instance-method" class="signature"><strong>#print_perf</strong></a>
        
          <div class="summary"><p>Print performance information for this context</p></div>
        
      </li>
    
      <li class="entry-summary">
        <a href="#process_embeddings%28embeddings%3AArray%28Array%28Float32%29%29%2Cseq_ids%3AArray%28Int32%29%7CNil%3Dnil%2Cn_seq_max%3AInt32%3D8%29%3AInt32-instance-method" class="signature"><strong>#process_embeddings</strong>(embeddings : Array(Array(Float32)), seq_ids : Array(Int32) | Nil = <span class="n">nil</span>, n_seq_max : Int32 = <span class="n">8</span>) : Int32</a>
        
          <div class="summary"><p>Process embeddings</p></div>
        
      </li>
    
      <li class="entry-summary">
        <a href="#process_prompts%28prompts%3AArray%28String%29%29%3AArray%28Int32%29-instance-method" class="signature"><strong>#process_prompts</strong>(prompts : Array(String)) : Array(Int32)</a>
        
          <div class="summary"><p>Process multiple prompts in batch</p></div>
        
      </li>
    
      <li class="entry-summary">
        <a href="#process_tokens%28tokens%3AArray%28Int32%29%2Ccompute_logits_for_last%3ABool%3Dtrue%2Cseq_ids%3AArray%28Int32%29%7CNil%3Dnil%2Cn_seq_max%3AInt32%3D8%29%3AInt32-instance-method" class="signature"><strong>#process_tokens</strong>(tokens : Array(Int32), compute_logits_for_last : Bool = <span class="n">true</span>, seq_ids : Array(Int32) | Nil = <span class="n">nil</span>, n_seq_max : Int32 = <span class="n">8</span>) : Int32</a>
        
          <div class="summary"><p>Process a sequence of tokens</p></div>
        
      </li>
    
      <li class="entry-summary">
        <a href="#reset_perf-instance-method" class="signature"><strong>#reset_perf</strong></a>
        
          <div class="summary"><p>Reset performance counters for this context</p></div>
        
      </li>
    
      <li class="entry-summary">
        <a href="#set_embeddings%28enabled%3ABool%29-instance-method" class="signature"><strong>#set_embeddings</strong>(enabled : Bool)</a>
        
          <div class="summary"><p>Sets whether the model is in embeddings mode or not If true, embeddings will be returned but logits will not</p></div>
        
      </li>
    
      <li class="entry-summary">
        <a href="#state%3AState-instance-method" class="signature"><strong>#state</strong> : State</a>
        
          <div class="summary"><p>Returns the state manager for this context Lazily initializes the state if it doesn't exist yet</p></div>
        
      </li>
    
      <li class="entry-summary">
        <a href="#to_unsafe%3APointer%28Llama%3A%3ALibLlama%3A%3ALlamaContext%29-instance-method" class="signature"><strong>#to_unsafe</strong> : Pointer(Llama::LibLlama::LlamaContext)</a>
        
          <div class="summary"><p>Returns the raw pointer to the underlying llama_context structure</p></div>
        
      </li>
    
  </ul>



  <div class="methods-inherited">
    
      


      


      


      


    
      


      


      


      


    
  </div>

  
  <h2>
    <a id="constructor-detail" class="anchor" href="#constructor-detail">
  <svg class="octicon-link" aria-hidden="true">
    <use href="#octicon-link"/>
  </svg>
</a>
    Constructor Detail
  </h2>
  
    <div class="entry-detail" id="new(model:Model,n_ctx:UInt32=0,n_batch:UInt32=512,n_threads:Int32=0,n_threads_batch:Int32=0,embeddings:Bool=false,offload_kqv:Bool=false)-class-method">
      <div class="signature">
        
        def self.<strong>new</strong>(model : <a href="../Llama/Model.html">Model</a>, n_ctx : UInt32 = <span class="n">0</span>, n_batch : UInt32 = <span class="n">512</span>, n_threads : Int32 = <span class="n">0</span>, n_threads_batch : Int32 = <span class="n">0</span>, embeddings : Bool = <span class="n">false</span>, offload_kqv : Bool = <span class="n">false</span>)

        <a class="method-permalink" href="#new%28model%3AModel%2Cn_ctx%3AUInt32%3D0%2Cn_batch%3AUInt32%3D512%2Cn_threads%3AInt32%3D0%2Cn_threads_batch%3AInt32%3D0%2Cembeddings%3ABool%3Dfalse%2Coffload_kqv%3ABool%3Dfalse%29-class-method">#</a>
      </div>
      
        <div class="doc">
          
          <p>Creates a new Context instance for a model.</p>
<p>Parameters:</p>
<ul>
<li>model: The Model to create a context for.</li>
<li>n_ctx: Text context (default: 0). The maximum context size. If 0, a minimum context size of 512 is used.</li>
<li>n_batch: Logical maximum batch size that can be submitted to llama_decode (default: 512).</li>
<li>n_threads: Number of threads to use for generation (default: 0). If 0, uses the number of hardware threads.</li>
<li>n_threads_batch: Number of threads to use for batch processing (default: 0). If 0, uses the number of hardware threads.</li>
<li>embeddings: Extract embeddings (together with logits) (default: false). If true, extract embeddings (together with logits).</li>
<li>offload_kqv: Whether to offload the KQV ops (including the KV cache) to GPU (default: false). Requires a GPU build of llama.cpp.</li>
</ul>
<p>Raises:</p>
<ul>
<li>Llama::Context::Error if the context cannot be created.</li>
</ul>
        </div>
      
      <br/>
      <div>
        
          [<a href="https://github.com/kojix2/llama.cr/blob/85572b7dcb2cf4bca0b3f881f0546a93e2892ef1/src/llama/context.cr#L19" target="_blank">View source</a>]
        
      </div>
    </div>
  


  

  

  
  <h2>
    <a id="instance-method-detail" class="anchor" href="#instance-method-detail">
  <svg class="octicon-link" aria-hidden="true">
    <use href="#octicon-link"/>
  </svg>
</a>
    Instance Method Detail
  </h2>
  
    <div class="entry-detail" id="apply_adapter_cvec(data:Slice(Float32),n_embd:Int32,il_start:Int32,il_end:Int32):Int32-instance-method">
      <div class="signature">
        
        def <strong>apply_adapter_cvec</strong>(data : Slice(Float32), n_embd : Int32, il_start : Int32, il_end : Int32) : Int32

        <a class="method-permalink" href="#apply_adapter_cvec%28data%3ASlice%28Float32%29%2Cn_embd%3AInt32%2Cil_start%3AInt32%2Cil_end%3AInt32%29%3AInt32-instance-method">#</a>
      </div>
      
        <div class="doc">
          
          <p>Applies a control vector to the LoRA adapter</p>
<p>Parameters:</p>
<ul>
<li>data: The control vector data</li>
<li>n_embd: Embedding dimension per layer</li>
<li>il_start: Start layer index (inclusive, 1-based)</li>
<li>il_end: End layer index (inclusive, 1-based)</li>
</ul>
<p>Returns:</p>
<ul>
<li>0 on success, non-zero on error</li>
</ul>
<p>Raises:</p>
<ul>
<li>Llama::Context::Error if the control vector cannot be applied</li>
</ul>
        </div>
      
      <br/>
      <div>
        
          [<a href="https://github.com/kojix2/llama.cr/blob/85572b7dcb2cf4bca0b3f881f0546a93e2892ef1/src/llama/context.cr#L730" target="_blank">View source</a>]
        
      </div>
    </div>
  
    <div class="entry-detail" id="apply_chat_template(messages:Array(ChatMessage),add_assistant:Bool=true,template:String|Nil=nil):String-instance-method">
      <div class="signature">
        
        def <strong>apply_chat_template</strong>(messages : Array(<a href="../Llama/ChatMessage.html">ChatMessage</a>), add_assistant : Bool = <span class="n">true</span>, template : String | Nil = <span class="n">nil</span>) : String

        <a class="method-permalink" href="#apply_chat_template%28messages%3AArray%28ChatMessage%29%2Cadd_assistant%3ABool%3Dtrue%2Ctemplate%3AString%7CNil%3Dnil%29%3AString-instance-method">#</a>
      </div>
      
        <div class="doc">
          
          <p>Applies the chat template to the given messages and returns the formatted prompt.</p>
<p>Parameters:</p>
<ul>
<li>messages: Array of ChatMessage (user/assistant/system)</li>
<li>add_assistant: Whether to add assistant role (default: true)</li>
<li>template: Optional template string (default: model's template)</li>
</ul>
<p>Returns:</p>
<ul>
<li>The formatted prompt string.</li>
</ul>
        </div>
      
      <br/>
      <div>
        
          [<a href="https://github.com/kojix2/llama.cr/blob/85572b7dcb2cf4bca0b3f881f0546a93e2892ef1/src/llama/context.cr#L852" target="_blank">View source</a>]
        
      </div>
    </div>
  
    <div class="entry-detail" id="attach_adapter_lora(adapter:AdapterLora,scale:Float32=1.0):Int32-instance-method">
      <div class="signature">
        
        def <strong>attach_adapter_lora</strong>(adapter : <a href="../Llama/AdapterLora.html">AdapterLora</a>, scale : Float32 = <span class="n">1.0</span>) : Int32

        <a class="method-permalink" href="#attach_adapter_lora%28adapter%3AAdapterLora%2Cscale%3AFloat32%3D1.0%29%3AInt32-instance-method">#</a>
      </div>
      
        <div class="doc">
          
          <p>Attaches a LoRA adapter to this context</p>
<p>Parameters:</p>
<ul>
<li>adapter: The LoRA adapter to attach</li>
<li>scale: Scaling factor for the adapter (default: 1.0)</li>
</ul>
<p>Returns:</p>
<ul>
<li>0 on success, non-zero on error</li>
</ul>
<p>Raises:</p>
<ul>
<li>Llama::Context::Error if the adapter cannot be attached</li>
</ul>
        </div>
      
      <br/>
      <div>
        
          [<a href="https://github.com/kojix2/llama.cr/blob/85572b7dcb2cf4bca0b3f881f0546a93e2892ef1/src/llama/context.cr#L672" target="_blank">View source</a>]
        
      </div>
    </div>
  
    <div class="entry-detail" id="chat(messages:Array(ChatMessage),max_tokens:Int32=128,temperature:Float32=0.8,template:String|Nil=nil):String-instance-method">
      <div class="signature">
        
        def <strong>chat</strong>(messages : Array(<a href="../Llama/ChatMessage.html">ChatMessage</a>), max_tokens : Int32 = <span class="n">128</span>, temperature : Float32 = <span class="n">0.8</span>, template : String | Nil = <span class="n">nil</span>) : String

        <a class="method-permalink" href="#chat%28messages%3AArray%28ChatMessage%29%2Cmax_tokens%3AInt32%3D128%2Ctemperature%3AFloat32%3D0.8%2Ctemplate%3AString%7CNil%3Dnil%29%3AString-instance-method">#</a>
      </div>
      
        <div class="doc">
          
          <p>Generates a response in a chat conversation</p>
<p>Parameters:</p>
<ul>
<li>messages: Array of chat messages</li>
<li>max_tokens: Maximum number of tokens to generate</li>
<li>temperature: Sampling temperature</li>
<li>template: Optional chat template (nil to use model's default)</li>
</ul>
<p>Returns:</p>
<ul>
<li>The generated response text</li>
</ul>
<p>Raises:</p>
<ul>
<li>ArgumentError if parameters are invalid</li>
<li>Llama::Context::Error if text generation fails</li>
<li>Llama::TokenizationError if the prompt cannot be tokenized</li>
</ul>
        </div>
      
      <br/>
      <div>
        
          [<a href="https://github.com/kojix2/llama.cr/blob/85572b7dcb2cf4bca0b3f881f0546a93e2892ef1/src/llama/context.cr#L139" target="_blank">View source</a>]
        
      </div>
    </div>
  
    <div class="entry-detail" id="clear_adapters_lora-instance-method">
      <div class="signature">
        
        def <strong>clear_adapters_lora</strong>

        <a class="method-permalink" href="#clear_adapters_lora-instance-method">#</a>
      </div>
      
        <div class="doc">
          
          <p>Clears all LoRA adapters from this context</p>
        </div>
      
      <br/>
      <div>
        
          [<a href="https://github.com/kojix2/llama.cr/blob/85572b7dcb2cf4bca0b3f881f0546a93e2892ef1/src/llama/context.cr#L713" target="_blank">View source</a>]
        
      </div>
    </div>
  
    <div class="entry-detail" id="decode(batch:LibLlama::LlamaBatch|Batch):Int32-instance-method">
      <div class="signature">
        
        def <strong>decode</strong>(batch : LibLlama::LlamaBatch | <a href="../Llama/Batch.html">Batch</a>) : Int32

        <a class="method-permalink" href="#decode%28batch%3ALibLlama%3A%3ALlamaBatch%7CBatch%29%3AInt32-instance-method">#</a>
      </div>
      
        <div class="doc">
          
          <p>Processes a batch of tokens with the decoder part of the model</p>
<p>Parameters:</p>
<ul>
<li>batch: The batch to process (can be a LibLlama::LlamaBatch or a Batch instance)</li>
</ul>
<p>Returns:</p>
<ul>
<li>0 on success</li>
<li>1 if no KV slot was found for the batch</li>
<li>&lt; 0 on error</li>
</ul>
<p>Raises:</p>
<ul>
<li>Llama::Batch::Error on error</li>
</ul>
        </div>
      
      <br/>
      <div>
        
          [<a href="https://github.com/kojix2/llama.cr/blob/85572b7dcb2cf4bca0b3f881f0546a93e2892ef1/src/llama/context.cr#L389" target="_blank">View source</a>]
        
      </div>
    </div>
  
    <div class="entry-detail" id="detach_adapter_lora(adapter:AdapterLora):Int32-instance-method">
      <div class="signature">
        
        def <strong>detach_adapter_lora</strong>(adapter : <a href="../Llama/AdapterLora.html">AdapterLora</a>) : Int32

        <a class="method-permalink" href="#detach_adapter_lora%28adapter%3AAdapterLora%29%3AInt32-instance-method">#</a>
      </div>
      
        <div class="doc">
          
          <p>Detaches a LoRA adapter from this context</p>
<p>Parameters:</p>
<ul>
<li>adapter: The LoRA adapter to detach</li>
</ul>
<p>Returns:</p>
<ul>
<li>0 on success, non-zero on error</li>
</ul>
<p>Raises:</p>
<ul>
<li>Llama::Context::Error if the adapter cannot be detached</li>
</ul>
        </div>
      
      <br/>
      <div>
        
          [<a href="https://github.com/kojix2/llama.cr/blob/85572b7dcb2cf4bca0b3f881f0546a93e2892ef1/src/llama/context.cr#L697" target="_blank">View source</a>]
        
      </div>
    </div>
  
    <div class="entry-detail" id="encode(batch:LibLlama::LlamaBatch|Batch):Int32-instance-method">
      <div class="signature">
        
        def <strong>encode</strong>(batch : LibLlama::LlamaBatch | <a href="../Llama/Batch.html">Batch</a>) : Int32

        <a class="method-permalink" href="#encode%28batch%3ALibLlama%3A%3ALlamaBatch%7CBatch%29%3AInt32-instance-method">#</a>
      </div>
      
        <div class="doc">
          
          <p>Processes a batch of tokens with the encoder part of the model</p>
<p>This function is used for encoder-decoder models to encode the input
before generating text with the decoder.</p>
<p>Parameters:</p>
<ul>
<li>batch: The batch to process (can be a LibLlama::LlamaBatch or a Batch instance)</li>
</ul>
<p>Returns:</p>
<ul>
<li>0 on success</li>
<li>&lt; 0 on error</li>
</ul>
<p>Raises:</p>
<ul>
<li>Llama::Batch::Error on error</li>
</ul>
        </div>
      
      <br/>
      <div>
        
          [<a href="https://github.com/kojix2/llama.cr/blob/85572b7dcb2cf4bca0b3f881f0546a93e2892ef1/src/llama/context.cr#L361" target="_blank">View source</a>]
        
      </div>
    </div>
  
    <div class="entry-detail" id="finalize-instance-method">
      <div class="signature">
        
        def <strong>finalize</strong>

        <a class="method-permalink" href="#finalize-instance-method">#</a>
      </div>
      
        <div class="doc">
          
          <p>Frees the resources associated with this context</p>
        </div>
      
      <br/>
      <div>
        
          [<a href="https://github.com/kojix2/llama.cr/blob/85572b7dcb2cf4bca0b3f881f0546a93e2892ef1/src/llama/context.cr#L642" target="_blank">View source</a>]
        
      </div>
    </div>
  
    <div class="entry-detail" id="generate(prompt:String,max_tokens:Int32=128,temperature:Float32=0.8):String-instance-method">
      <div class="signature">
        
        def <strong>generate</strong>(prompt : String, max_tokens : Int32 = <span class="n">128</span>, temperature : Float32 = <span class="n">0.8</span>) : String

        <a class="method-permalink" href="#generate%28prompt%3AString%2Cmax_tokens%3AInt32%3D128%2Ctemperature%3AFloat32%3D0.8%29%3AString-instance-method">#</a>
      </div>
      
        <div class="doc">
          
          <p>Generates text from a prompt</p>
<p>Parameters:</p>
<ul>
<li>prompt: The input prompt</li>
<li>max_tokens: Maximum number of tokens to generate (must be positive)</li>
<li>temperature: Sampling temperature (0.0 = greedy, 1.0 = more random)</li>
</ul>
<p>Returns:</p>
<ul>
<li>The generated text</li>
</ul>
<p>Raises:</p>
<ul>
<li>ArgumentError if parameters are invalid</li>
<li>Llama::Context::Error if text generation fails</li>
<li>Llama::TokenizationError if the prompt cannot be tokenized</li>
</ul>
        </div>
      
      <br/>
      <div>
        
          [<a href="https://github.com/kojix2/llama.cr/blob/85572b7dcb2cf4bca0b3f881f0546a93e2892ef1/src/llama/context.cr#L438" target="_blank">View source</a>]
        
      </div>
    </div>
  
    <div class="entry-detail" id="generate_with_sampler(prompt:String,sampler:SamplerChain,max_tokens:Int32=128):String-instance-method">
      <div class="signature">
        
        def <strong>generate_with_sampler</strong>(prompt : String, sampler : <a href="../Llama/SamplerChain.html">SamplerChain</a>, max_tokens : Int32 = <span class="n">128</span>) : String

        <a class="method-permalink" href="#generate_with_sampler%28prompt%3AString%2Csampler%3ASamplerChain%2Cmax_tokens%3AInt32%3D128%29%3AString-instance-method">#</a>
      </div>
      
        <div class="doc">
          
          <p>Generates text using a sampler chain</p>
<p>Parameters:</p>
<ul>
<li>prompt: The input prompt</li>
<li>sampler: The sampler chain to use</li>
<li>max_tokens: Maximum number of tokens to generate (must be positive)</li>
</ul>
<p>Returns:</p>
<ul>
<li>The generated text</li>
</ul>
<p>Raises:</p>
<ul>
<li>ArgumentError if parameters are invalid</li>
<li>Llama::Context::Error if text generation fails</li>
<li>Llama::TokenizationError if the prompt cannot be tokenized</li>
<li>Llama::Sampler::Error if sampling fails</li>
</ul>
        </div>
      
      <br/>
      <div>
        
          [<a href="https://github.com/kojix2/llama.cr/blob/85572b7dcb2cf4bca0b3f881f0546a93e2892ef1/src/llama/context.cr#L320" target="_blank">View source</a>]
        
      </div>
    </div>
  
    <div class="entry-detail" id="get_embeddings:Array(Float32)|Nil-instance-method">
      <div class="signature">
        
        def <strong>get_embeddings</strong> : Array(Float32) | Nil

        <a class="method-permalink" href="#get_embeddings%3AArray%28Float32%29%7CNil-instance-method">#</a>
      </div>
      
        <div class="doc">
          
          <p>Gets all output token embeddings
Only available when embeddings mode is enabled</p>
<p>Returns:</p>
<ul>
<li>An array of embeddings, or nil if embeddings are not available</li>
</ul>
<p>Raises:</p>
<ul>
<li>Llama::Context::Error if embeddings mode is not enabled</li>
</ul>
        </div>
      
      <br/>
      <div>
        
          [<a href="https://github.com/kojix2/llama.cr/blob/85572b7dcb2cf4bca0b3f881f0546a93e2892ef1/src/llama/context.cr#L770" target="_blank">View source</a>]
        
      </div>
    </div>
  
    <div class="entry-detail" id="get_embeddings_ith(i:Int32):Array(Float32)|Nil-instance-method">
      <div class="signature">
        
        def <strong>get_embeddings_ith</strong>(i : Int32) : Array(Float32) | Nil

        <a class="method-permalink" href="#get_embeddings_ith%28i%3AInt32%29%3AArray%28Float32%29%7CNil-instance-method">#</a>
      </div>
      
        <div class="doc">
          
          <p>Gets the embeddings for a specific token</p>
<p>Parameters:</p>
<ul>
<li>i: The token index (negative indices can be used to access in reverse order)</li>
</ul>
<p>Returns:</p>
<ul>
<li>An array of embedding values, or nil if not available</li>
</ul>
<p>Raises:</p>
<ul>
<li>Llama::Context::Error if embeddings mode is not enabled</li>
</ul>
        </div>
      
      <br/>
      <div>
        
          [<a href="https://github.com/kojix2/llama.cr/blob/85572b7dcb2cf4bca0b3f881f0546a93e2892ef1/src/llama/context.cr#L801" target="_blank">View source</a>]
        
      </div>
    </div>
  
    <div class="entry-detail" id="get_embeddings_seq(seq_id:Int32):Array(Float32)|Nil-instance-method">
      <div class="signature">
        
        def <strong>get_embeddings_seq</strong>(seq_id : Int32) : Array(Float32) | Nil

        <a class="method-permalink" href="#get_embeddings_seq%28seq_id%3AInt32%29%3AArray%28Float32%29%7CNil-instance-method">#</a>
      </div>
      
        <div class="doc">
          
          <p>Gets the embeddings for a specific sequence</p>
<p>Parameters:</p>
<ul>
<li>seq_id: The sequence ID</li>
</ul>
<p>Returns:</p>
<ul>
<li>An array of embedding values, or nil if not available</li>
</ul>
<p>Raises:</p>
<ul>
<li>Llama::Context::Error if embeddings mode is not enabled</li>
</ul>
        </div>
      
      <br/>
      <div>
        
          [<a href="https://github.com/kojix2/llama.cr/blob/85572b7dcb2cf4bca0b3f881f0546a93e2892ef1/src/llama/context.cr#L827" target="_blank">View source</a>]
        
      </div>
    </div>
  
    <div class="entry-detail" id="logits:Pointer(Float32)-instance-method">
      <div class="signature">
        
        def <strong>logits</strong> : Pointer(Float32)

        <a class="method-permalink" href="#logits%3APointer%28Float32%29-instance-method">#</a>
      </div>
      
        <div class="doc">
          
          <p>Gets the logits for the last token</p>
<p>Returns:</p>
<ul>
<li>A pointer to the logits array</li>
</ul>
        </div>
      
      <br/>
      <div>
        
          [<a href="https://github.com/kojix2/llama.cr/blob/85572b7dcb2cf4bca0b3f881f0546a93e2892ef1/src/llama/context.cr#L409" target="_blank">View source</a>]
        
      </div>
    </div>
  
    <div class="entry-detail" id="memory:Memory-instance-method">
      <div class="signature">
        
        def <strong>memory</strong> : <a href="../Llama/Memory.html">Memory</a>

        <a class="method-permalink" href="#memory%3AMemory-instance-method">#</a>
      </div>
      
        <div class="doc">
          
          <p>Returns the memory for this context (modern API)</p>
<p>The memory system provides unified access to various memory types:</p>
<ul>
<li>Standard KV cache (llama_kv_cache_unified)</li>
<li>SWA (Sliding Window Attention) cache</li>
<li>Recurrent layer memory</li>
<li>Hybrid attention/recurrent models</li>
</ul>
<p>Returns:</p>
<ul>
<li>A Memory instance</li>
</ul>
        </div>
      
      <br/>
      <div>
        
          [<a href="https://github.com/kojix2/llama.cr/blob/85572b7dcb2cf4bca0b3f881f0546a93e2892ef1/src/llama/context.cr#L72" target="_blank">View source</a>]
        
      </div>
    </div>
  
    <div class="entry-detail" id="n_batch:UInt32-instance-method">
      <div class="signature">
        
        def <strong>n_batch</strong> : UInt32

        <a class="method-permalink" href="#n_batch%3AUInt32-instance-method">#</a>
      </div>
      
        <div class="doc">
          
          <p>Returns the logical batch size (n_batch)</p>
        </div>
      
      <br/>
      <div>
        
          [<a href="https://github.com/kojix2/llama.cr/blob/85572b7dcb2cf4bca0b3f881f0546a93e2892ef1/src/llama/context.cr#L82" target="_blank">View source</a>]
        
      </div>
    </div>
  
    <div class="entry-detail" id="n_ctx:UInt32-instance-method">
      <div class="signature">
        
        def <strong>n_ctx</strong> : UInt32

        <a class="method-permalink" href="#n_ctx%3AUInt32-instance-method">#</a>
      </div>
      
        <div class="doc">
          
          <p>Returns the context window size (n_ctx)</p>
        </div>
      
      <br/>
      <div>
        
          [<a href="https://github.com/kojix2/llama.cr/blob/85572b7dcb2cf4bca0b3f881f0546a93e2892ef1/src/llama/context.cr#L77" target="_blank">View source</a>]
        
      </div>
    </div>
  
    <div class="entry-detail" id="n_seq_max:UInt32-instance-method">
      <div class="signature">
        
        def <strong>n_seq_max</strong> : UInt32

        <a class="method-permalink" href="#n_seq_max%3AUInt32-instance-method">#</a>
      </div>
      
        <div class="doc">
          
          <p>Returns the maximum number of sequence IDs per token (n_seq_max)</p>
        </div>
      
      <br/>
      <div>
        
          [<a href="https://github.com/kojix2/llama.cr/blob/85572b7dcb2cf4bca0b3f881f0546a93e2892ef1/src/llama/context.cr#L92" target="_blank">View source</a>]
        
      </div>
    </div>
  
    <div class="entry-detail" id="n_threads:Int32-instance-method">
      <div class="signature">
        
        def <strong>n_threads</strong> : Int32

        <a class="method-permalink" href="#n_threads%3AInt32-instance-method">#</a>
      </div>
      
        <div class="doc">
          
          <p>Returns the number of threads used for generation</p>
        </div>
      
      <br/>
      <div>
        
          [<a href="https://github.com/kojix2/llama.cr/blob/85572b7dcb2cf4bca0b3f881f0546a93e2892ef1/src/llama/context.cr#L97" target="_blank">View source</a>]
        
      </div>
    </div>
  
    <div class="entry-detail" id="n_threads_batch:Int32-instance-method">
      <div class="signature">
        
        def <strong>n_threads_batch</strong> : Int32

        <a class="method-permalink" href="#n_threads_batch%3AInt32-instance-method">#</a>
      </div>
      
        <div class="doc">
          
          <p>Returns the number of threads used for batch processing</p>
        </div>
      
      <br/>
      <div>
        
          [<a href="https://github.com/kojix2/llama.cr/blob/85572b7dcb2cf4bca0b3f881f0546a93e2892ef1/src/llama/context.cr#L102" target="_blank">View source</a>]
        
      </div>
    </div>
  
    <div class="entry-detail" id="n_ubatch:UInt32-instance-method">
      <div class="signature">
        
        def <strong>n_ubatch</strong> : UInt32

        <a class="method-permalink" href="#n_ubatch%3AUInt32-instance-method">#</a>
      </div>
      
        <div class="doc">
          
          <p>Returns the micro-batch size (n_ubatch)</p>
        </div>
      
      <br/>
      <div>
        
          [<a href="https://github.com/kojix2/llama.cr/blob/85572b7dcb2cf4bca0b3f881f0546a93e2892ef1/src/llama/context.cr#L87" target="_blank">View source</a>]
        
      </div>
    </div>
  
    <div class="entry-detail" id="pooling_type:LibLlama::LlamaPoolingType-instance-method">
      <div class="signature">
        
        def <strong>pooling_type</strong> : LibLlama::LlamaPoolingType

        <a class="method-permalink" href="#pooling_type%3ALibLlama%3A%3ALlamaPoolingType-instance-method">#</a>
      </div>
      
        <div class="doc">
          
          <p>Gets the pooling type used for embeddings</p>
<p>Returns:</p>
<ul>
<li>The pooling type as a PoolingType enum</li>
</ul>
        </div>
      
      <br/>
      <div>
        
          [<a href="https://github.com/kojix2/llama.cr/blob/85572b7dcb2cf4bca0b3f881f0546a93e2892ef1/src/llama/context.cr#L758" target="_blank">View source</a>]
        
      </div>
    </div>
  
    <div class="entry-detail" id="print_perf-instance-method">
      <div class="signature">
        
        def <strong>print_perf</strong>

        <a class="method-permalink" href="#print_perf-instance-method">#</a>
      </div>
      
        <div class="doc">
          
          <p>Print performance information for this context</p>
<p>This method prints performance statistics about the context to STDERR.
It's useful for debugging and performance analysis.</p>
        </div>
      
      <br/>
      <div>
        
          [<a href="https://github.com/kojix2/llama.cr/blob/85572b7dcb2cf4bca0b3f881f0546a93e2892ef1/src/llama/context.cr#L650" target="_blank">View source</a>]
        
      </div>
    </div>
  
    <div class="entry-detail" id="process_embeddings(embeddings:Array(Array(Float32)),seq_ids:Array(Int32)|Nil=nil,n_seq_max:Int32=8):Int32-instance-method">
      <div class="signature">
        
        def <strong>process_embeddings</strong>(embeddings : Array(Array(Float32)), seq_ids : Array(Int32) | Nil = <span class="n">nil</span>, n_seq_max : Int32 = <span class="n">8</span>) : Int32

        <a class="method-permalink" href="#process_embeddings%28embeddings%3AArray%28Array%28Float32%29%29%2Cseq_ids%3AArray%28Int32%29%7CNil%3Dnil%2Cn_seq_max%3AInt32%3D8%29%3AInt32-instance-method">#</a>
      </div>
      
        <div class="doc">
          
          <p>Process embeddings</p>
<p>Parameters:</p>
<ul>
<li>embeddings: Array of embedding vectors</li>
<li>seq_ids: Sequence IDs to use for all embeddings</li>
<li>n_seq_max: Maximum number of sequence IDs per token (default: 8)</li>
</ul>
<p>Returns:</p>
<ul>
<li>The result of the decode operation (0 on success)</li>
</ul>
<p>Raises:</p>
<ul>
<li>Llama::Batch::Error on error</li>
</ul>
        </div>
      
      <br/>
      <div>
        
          [<a href="https://github.com/kojix2/llama.cr/blob/85572b7dcb2cf4bca0b3f881f0546a93e2892ef1/src/llama/context.cr#L270" target="_blank">View source</a>]
        
      </div>
    </div>
  
    <div class="entry-detail" id="process_prompts(prompts:Array(String)):Array(Int32)-instance-method">
      <div class="signature">
        
        def <strong>process_prompts</strong>(prompts : Array(String)) : Array(Int32)

        <a class="method-permalink" href="#process_prompts%28prompts%3AArray%28String%29%29%3AArray%28Int32%29-instance-method">#</a>
      </div>
      
        <div class="doc">
          
          <p>Process multiple prompts in batch</p>
<p>Parameters:</p>
<ul>
<li>prompts: Array of text prompts to process</li>
<li>compute_logits_for_last: Whether to compute logits only for the last token of each prompt</li>
</ul>
<p>Returns:</p>
<ul>
<li>Array of decode operation results (0 on success)</li>
</ul>
<p>Raises:</p>
<ul>
<li>Llama::Batch::Error on error</li>
<li>Llama::TokenizationError if a prompt cannot be tokenized</li>
</ul>
        </div>
      
      <br/>
      <div>
        
          [<a href="https://github.com/kojix2/llama.cr/blob/85572b7dcb2cf4bca0b3f881f0546a93e2892ef1/src/llama/context.cr#L220" target="_blank">View source</a>]
        
      </div>
    </div>
  
    <div class="entry-detail" id="process_tokens(tokens:Array(Int32),compute_logits_for_last:Bool=true,seq_ids:Array(Int32)|Nil=nil,n_seq_max:Int32=8):Int32-instance-method">
      <div class="signature">
        
        def <strong>process_tokens</strong>(tokens : Array(Int32), compute_logits_for_last : Bool = <span class="n">true</span>, seq_ids : Array(Int32) | Nil = <span class="n">nil</span>, n_seq_max : Int32 = <span class="n">8</span>) : Int32

        <a class="method-permalink" href="#process_tokens%28tokens%3AArray%28Int32%29%2Ccompute_logits_for_last%3ABool%3Dtrue%2Cseq_ids%3AArray%28Int32%29%7CNil%3Dnil%2Cn_seq_max%3AInt32%3D8%29%3AInt32-instance-method">#</a>
      </div>
      
        <div class="doc">
          
          <p>Process a sequence of tokens</p>
<p>Parameters:</p>
<ul>
<li>tokens: Array of token IDs to process</li>
<li>compute_logits_for_last: Whether to compute logits only for the last token</li>
<li>seq_ids: Sequence IDs to use for all tokens</li>
<li>n_seq_max: Maximum number of sequence IDs per token (default: 8)</li>
</ul>
<p>Returns:</p>
<ul>
<li>The result of the decode operation (0 on success)</li>
</ul>
<p>Raises:</p>
<ul>
<li>Llama::Batch::Error on error</li>
</ul>
        </div>
      
      <br/>
      <div>
        
          [<a href="https://github.com/kojix2/llama.cr/blob/85572b7dcb2cf4bca0b3f881f0546a93e2892ef1/src/llama/context.cr#L199" target="_blank">View source</a>]
        
      </div>
    </div>
  
    <div class="entry-detail" id="reset_perf-instance-method">
      <div class="signature">
        
        def <strong>reset_perf</strong>

        <a class="method-permalink" href="#reset_perf-instance-method">#</a>
      </div>
      
        <div class="doc">
          
          <p>Reset performance counters for this context</p>
<p>This method resets all performance counters for the context.</p>
        </div>
      
      <br/>
      <div>
        
          [<a href="https://github.com/kojix2/llama.cr/blob/85572b7dcb2cf4bca0b3f881f0546a93e2892ef1/src/llama/context.cr#L657" target="_blank">View source</a>]
        
      </div>
    </div>
  
    <div class="entry-detail" id="set_embeddings(enabled:Bool)-instance-method">
      <div class="signature">
        
        def <strong>set_embeddings</strong>(enabled : Bool)

        <a class="method-permalink" href="#set_embeddings%28enabled%3ABool%29-instance-method">#</a>
      </div>
      
        <div class="doc">
          
          <p>Sets whether the model is in embeddings mode or not
If true, embeddings will be returned but logits will not</p>
<p>Parameters:</p>
<ul>
<li>enabled: Whether to enable embeddings mode</li>
</ul>
        </div>
      
      <br/>
      <div>
        
          [<a href="https://github.com/kojix2/llama.cr/blob/85572b7dcb2cf4bca0b3f881f0546a93e2892ef1/src/llama/context.cr#L750" target="_blank">View source</a>]
        
      </div>
    </div>
  
    <div class="entry-detail" id="state:State-instance-method">
      <div class="signature">
        
        def <strong>state</strong> : <a href="../Llama/State.html">State</a>

        <a class="method-permalink" href="#state%3AState-instance-method">#</a>
      </div>
      
        <div class="doc">
          
          <p>Returns the state manager for this context
Lazily initializes the state if it doesn't exist yet</p>
        </div>
      
      <br/>
      <div>
        
          [<a href="https://github.com/kojix2/llama.cr/blob/85572b7dcb2cf4bca0b3f881f0546a93e2892ef1/src/llama/context.cr#L108" target="_blank">View source</a>]
        
      </div>
    </div>
  
    <div class="entry-detail" id="to_unsafe:Pointer(Llama::LibLlama::LlamaContext)-instance-method">
      <div class="signature">
        
        def <strong>to_unsafe</strong> : Pointer(Llama::LibLlama::LlamaContext)

        <a class="method-permalink" href="#to_unsafe%3APointer%28Llama%3A%3ALibLlama%3A%3ALlamaContext%29-instance-method">#</a>
      </div>
      
        <div class="doc">
          
          <p>Returns the raw pointer to the underlying llama_context structure</p>
        </div>
      
      <br/>
      <div>
        
          [<a href="https://github.com/kojix2/llama.cr/blob/85572b7dcb2cf4bca0b3f881f0546a93e2892ef1/src/llama/context.cr#L637" target="_blank">View source</a>]
        
      </div>
    </div>
  



</div>

</body>
</html>
