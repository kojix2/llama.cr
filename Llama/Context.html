<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="utf-8" />
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1.0">
<meta name="generator" content="Crystal Docs 1.15.1">
<meta name="crystal_docs.project_version" content="main">
<meta name="crystal_docs.project_name" content="llama">



<link href="../css/style.css" rel="stylesheet" type="text/css" />
<script type="text/javascript" src="../js/doc.js"></script>

  <meta name="repository-name" content="llama">
  <title>Llama::Context - llama main</title>
  <script type="text/javascript">
    CrystalDocs.base_path = "../";
  </script>
</head>
<body>

<svg class="hidden">
  <symbol id="octicon-link" viewBox="0 0 16 16">
    <path fill="currentColor" fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path>
  </symbol>
</svg>
<input type="checkbox" id="sidebar-btn">
<label for="sidebar-btn" id="sidebar-btn-label">
  <svg class="open" xmlns="http://www.w3.org/2000/svg" height="2em" width="2em" viewBox="0 0 512 512"><title>Open Sidebar</title><path fill="currentColor" d="M80 96v64h352V96H80zm0 112v64h352v-64H80zm0 112v64h352v-64H80z"></path></svg>
  <svg class="close" xmlns="http://www.w3.org/2000/svg" width="2em" height="2em" viewBox="0 0 512 512"><title>Close Sidebar</title><path fill="currentColor" d="m118.6 73.4-45.2 45.2L210.7 256 73.4 393.4l45.2 45.2L256 301.3l137.4 137.3 45.2-45.2L301.3 256l137.3-137.4-45.2-45.2L256 210.7Z"></path></svg>
</label>
<div class="sidebar">
  <div class="sidebar-header">
    <div class="search-box">
      <input type="search" class="search-input" placeholder="Search..." spellcheck="false" aria-label="Search">
    </div>

    <div class="project-summary">
      <h1 class="project-name">
        <a href="../index.html">
          llama
        </a>
      </h1>

      <span class="project-version">
        main
      </span>
    </div>
  </div>

  <div class="search-results hidden">
    <ul class="search-list"></ul>
  </div>

  <div class="types-list">
    <ul>
  
  <li class="parent open current" data-id="llama/Llama" data-name="llama">
      <a href="../Llama.html">Llama</a>
      
        <ul>
  
  <li class=" " data-id="llama/Llama/Batch" data-name="llama::batch">
      <a href="../Llama/Batch.html">Batch</a>
      
    </li>
  
  <li class=" " data-id="llama/Llama/BatchError" data-name="llama::batcherror">
      <a href="../Llama/BatchError.html">BatchError</a>
      
    </li>
  
  <li class=" " data-id="llama/Llama/ChatMessage" data-name="llama::chatmessage">
      <a href="../Llama/ChatMessage.html">ChatMessage</a>
      
    </li>
  
  <li class=" current" data-id="llama/Llama/Context" data-name="llama::context">
      <a href="../Llama/Context.html">Context</a>
      
    </li>
  
  <li class=" " data-id="llama/Llama/ContextError" data-name="llama::contexterror">
      <a href="../Llama/ContextError.html">ContextError</a>
      
    </li>
  
  <li class=" " data-id="llama/Llama/DistSampler" data-name="llama::distsampler">
      <a href="../Llama/DistSampler.html">DistSampler</a>
      
    </li>
  
  <li class=" " data-id="llama/Llama/Error" data-name="llama::error">
      <a href="../Llama/Error.html">Error</a>
      
    </li>
  
  <li class=" " data-id="llama/Llama/GrammarLazyPatternsSampler" data-name="llama::grammarlazypatternssampler">
      <a href="../Llama/GrammarLazyPatternsSampler.html">GrammarLazyPatternsSampler</a>
      
    </li>
  
  <li class=" " data-id="llama/Llama/GrammarSampler" data-name="llama::grammarsampler">
      <a href="../Llama/GrammarSampler.html">GrammarSampler</a>
      
    </li>
  
  <li class=" " data-id="llama/Llama/InfillSampler" data-name="llama::infillsampler">
      <a href="../Llama/InfillSampler.html">InfillSampler</a>
      
    </li>
  
  <li class=" " data-id="llama/Llama/KvCache" data-name="llama::kvcache">
      <a href="../Llama/KvCache.html">KvCache</a>
      
    </li>
  
  <li class=" " data-id="llama/Llama/KvCacheError" data-name="llama::kvcacheerror">
      <a href="../Llama/KvCacheError.html">KvCacheError</a>
      
    </li>
  
  <li class=" " data-id="llama/Llama/MinPSampler" data-name="llama::minpsampler">
      <a href="../Llama/MinPSampler.html">MinPSampler</a>
      
    </li>
  
  <li class=" " data-id="llama/Llama/MirostatSampler" data-name="llama::mirostatsampler">
      <a href="../Llama/MirostatSampler.html">MirostatSampler</a>
      
    </li>
  
  <li class=" " data-id="llama/Llama/MirostatV2Sampler" data-name="llama::mirostatv2sampler">
      <a href="../Llama/MirostatV2Sampler.html">MirostatV2Sampler</a>
      
    </li>
  
  <li class=" " data-id="llama/Llama/Model" data-name="llama::model">
      <a href="../Llama/Model.html">Model</a>
      
    </li>
  
  <li class=" " data-id="llama/Llama/ModelError" data-name="llama::modelerror">
      <a href="../Llama/ModelError.html">ModelError</a>
      
    </li>
  
  <li class=" " data-id="llama/Llama/PenaltiesSampler" data-name="llama::penaltiessampler">
      <a href="../Llama/PenaltiesSampler.html">PenaltiesSampler</a>
      
    </li>
  
  <li class=" " data-id="llama/Llama/Sampler" data-name="llama::sampler">
      <a href="../Llama/Sampler.html">Sampler</a>
      
    </li>
  
  <li class=" " data-id="llama/Llama/SamplerChain" data-name="llama::samplerchain">
      <a href="../Llama/SamplerChain.html">SamplerChain</a>
      
    </li>
  
  <li class=" " data-id="llama/Llama/SamplingError" data-name="llama::samplingerror">
      <a href="../Llama/SamplingError.html">SamplingError</a>
      
    </li>
  
  <li class=" " data-id="llama/Llama/State" data-name="llama::state">
      <a href="../Llama/State.html">State</a>
      
    </li>
  
  <li class=" " data-id="llama/Llama/StateError" data-name="llama::stateerror">
      <a href="../Llama/StateError.html">StateError</a>
      
    </li>
  
  <li class=" " data-id="llama/Llama/TempExtSampler" data-name="llama::tempextsampler">
      <a href="../Llama/TempExtSampler.html">TempExtSampler</a>
      
    </li>
  
  <li class=" " data-id="llama/Llama/TempSampler" data-name="llama::tempsampler">
      <a href="../Llama/TempSampler.html">TempSampler</a>
      
    </li>
  
  <li class=" " data-id="llama/Llama/TokenizationError" data-name="llama::tokenizationerror">
      <a href="../Llama/TokenizationError.html">TokenizationError</a>
      
    </li>
  
  <li class=" " data-id="llama/Llama/TopKSampler" data-name="llama::topksampler">
      <a href="../Llama/TopKSampler.html">TopKSampler</a>
      
    </li>
  
  <li class=" " data-id="llama/Llama/TopNSigmaSampler" data-name="llama::topnsigmasampler">
      <a href="../Llama/TopNSigmaSampler.html">TopNSigmaSampler</a>
      
    </li>
  
  <li class=" " data-id="llama/Llama/TopPSampler" data-name="llama::toppsampler">
      <a href="../Llama/TopPSampler.html">TopPSampler</a>
      
    </li>
  
  <li class=" " data-id="llama/Llama/TypicalSampler" data-name="llama::typicalsampler">
      <a href="../Llama/TypicalSampler.html">TypicalSampler</a>
      
    </li>
  
  <li class=" " data-id="llama/Llama/Vocab" data-name="llama::vocab">
      <a href="../Llama/Vocab.html">Vocab</a>
      
    </li>
  
  <li class=" " data-id="llama/Llama/XtcSampler" data-name="llama::xtcsampler">
      <a href="../Llama/XtcSampler.html">XtcSampler</a>
      
    </li>
  
</ul>

      
    </li>
  
</ul>

  </div>
</div>


<div class="main-content">
<h1 class="type-name">

  <span class="kind">
    class
  </span> Llama::<wbr>Context

</h1>


  <ul class="superclass-hierarchy"><li class="superclass"><a href="../Llama/Context.html">Llama::Context</a></li><li class="superclass">Reference</li><li class="superclass">Object</li></ul>




  <h2>
    <a id="overview" class="anchor" href="#overview">
  <svg class="octicon-link" aria-hidden="true">
    <use href="#octicon-link"/>
  </svg>
</a>
    Overview
  </h2>

  <p>Wrapper for the llama_context structure</p>
















  <h2>
    <a id="defined-in" class="anchor" href="#defined-in">
  <svg class="octicon-link" aria-hidden="true">
    <use href="#octicon-link"/>
  </svg>
</a>
    Defined in:
  </h2>
  
    
      <a href="https://github.com/kojix2/llama.cr/blob/55f06ea473618e52179cc122c32bbf3aaee8805d/src/llama/context.cr#L3" target="_blank">
        llama/context.cr
      </a>
    
    <br/>
  





  
  <h2>
    <a id="constructors" class="anchor" href="#constructors">
  <svg class="octicon-link" aria-hidden="true">
    <use href="#octicon-link"/>
  </svg>
</a>
    Constructors
  </h2>
  <ul class="list-summary">
    
      <li class="entry-summary">
        <a href="#new%28model%3AModel%2Cparams%3Dnil%29-class-method" class="signature"><strong>.new</strong>(model : Model, params = <span class="n">nil</span>)</a>
        
          <div class="summary"><p>Creates a new Context instance for a model</p></div>
        
      </li>
    
  </ul>


  
  <h2>
    <a id="class-method-summary" class="anchor" href="#class-method-summary">
  <svg class="octicon-link" aria-hidden="true">
    <use href="#octicon-link"/>
  </svg>
</a>
    Class Method Summary
  </h2>
  <ul class="list-summary">
    
      <li class="entry-summary">
        <a href="#default_params%3ALibLlama%3A%3ALlamaContextParams-class-method" class="signature"><strong>.default_params</strong> : LibLlama::LlamaContextParams</a>
        
          <div class="summary"><p>Returns the default context parameters</p></div>
        
      </li>
    
  </ul>


  

  
  <h2>
    <a id="instance-method-summary" class="anchor" href="#instance-method-summary">
  <svg class="octicon-link" aria-hidden="true">
    <use href="#octicon-link"/>
  </svg>
</a>
    Instance Method Summary
  </h2>
  <ul class="list-summary">
    
      <li class="entry-summary">
        <a href="#chat%28messages%3AArray%28ChatMessage%29%2Cmax_tokens%3AInt32%3D128%2Ctemperature%3AFloat32%3D0.8%2Ctemplate%3AString%7CNil%3Dnil%29%3AString-instance-method" class="signature"><strong>#chat</strong>(messages : Array(ChatMessage), max_tokens : Int32 = <span class="n">128</span>, temperature : Float32 = <span class="n">0.8</span>, template : String | Nil = <span class="n">nil</span>) : String</a>
        
          <div class="summary"><p>Generates a response in a chat conversation</p></div>
        
      </li>
    
      <li class="entry-summary">
        <a href="#cleanup-instance-method" class="signature"><strong>#cleanup</strong></a>
        
          <div class="summary"><p>Explicitly clean up resources This can be called manually to release resources before garbage collection</p></div>
        
      </li>
    
      <li class="entry-summary">
        <a href="#decode%28batch%3ALibLlama%3A%3ALlamaBatch%7CBatch%29%3AInt32-instance-method" class="signature"><strong>#decode</strong>(batch : LibLlama::LlamaBatch | Batch) : Int32</a>
        
          <div class="summary"><p>Processes a batch of tokens with the decoder part of the model</p></div>
        
      </li>
    
      <li class="entry-summary">
        <a href="#encode%28batch%3ALibLlama%3A%3ALlamaBatch%7CBatch%29%3AInt32-instance-method" class="signature"><strong>#encode</strong>(batch : LibLlama::LlamaBatch | Batch) : Int32</a>
        
          <div class="summary"><p>Processes a batch of tokens with the encoder part of the model</p></div>
        
      </li>
    
      <li class="entry-summary">
        <a href="#finalize-instance-method" class="signature"><strong>#finalize</strong></a>
        
          <div class="summary"><p>Frees the resources associated with this context</p></div>
        
      </li>
    
      <li class="entry-summary">
        <a href="#generate%28prompt%3AString%2Cmax_tokens%3AInt32%3D128%2Ctemperature%3AFloat32%3D0.8%29%3AString-instance-method" class="signature"><strong>#generate</strong>(prompt : String, max_tokens : Int32 = <span class="n">128</span>, temperature : Float32 = <span class="n">0.8</span>) : String</a>
        
          <div class="summary"><p>Generates text from a prompt</p></div>
        
      </li>
    
      <li class="entry-summary">
        <a href="#generate_with_sampler%28prompt%3AString%2Csampler%3ASamplerChain%2Cmax_tokens%3AInt32%3D128%29%3AString-instance-method" class="signature"><strong>#generate_with_sampler</strong>(prompt : String, sampler : SamplerChain, max_tokens : Int32 = <span class="n">128</span>) : String</a>
        
          <div class="summary"><p>Generates text using a sampler chain</p></div>
        
      </li>
    
      <li class="entry-summary">
        <a href="#get_embeddings%3AArray%28Float32%29%7CNil-instance-method" class="signature"><strong>#get_embeddings</strong> : Array(Float32) | Nil</a>
        
          <div class="summary"><p>Gets all output token embeddings Only available when embeddings mode is enabled</p></div>
        
      </li>
    
      <li class="entry-summary">
        <a href="#get_embeddings_ith%28i%3AInt32%29%3AArray%28Float32%29%7CNil-instance-method" class="signature"><strong>#get_embeddings_ith</strong>(i : Int32) : Array(Float32) | Nil</a>
        
          <div class="summary"><p>Gets the embeddings for a specific token</p></div>
        
      </li>
    
      <li class="entry-summary">
        <a href="#get_embeddings_seq%28seq_id%3AInt32%29%3AArray%28Float32%29%7CNil-instance-method" class="signature"><strong>#get_embeddings_seq</strong>(seq_id : Int32) : Array(Float32) | Nil</a>
        
          <div class="summary"><p>Gets the embeddings for a specific sequence</p></div>
        
      </li>
    
      <li class="entry-summary">
        <a href="#kv_cache%3AKvCache-instance-method" class="signature"><strong>#kv_cache</strong> : KvCache</a>
        
          <div class="summary"><p>Returns the KV cache for this context Lazily initializes the KV cache if it doesn't exist yet</p></div>
        
      </li>
    
      <li class="entry-summary">
        <a href="#logits%3APointer%28Float32%29-instance-method" class="signature"><strong>#logits</strong> : Pointer(Float32)</a>
        
          <div class="summary"><p>Gets the logits for the last token</p></div>
        
      </li>
    
      <li class="entry-summary">
        <a href="#pooling_type%3ALibLlama%3A%3ALlamaPoolingType-instance-method" class="signature"><strong>#pooling_type</strong> : LibLlama::LlamaPoolingType</a>
        
          <div class="summary"><p>Gets the pooling type used for embeddings</p></div>
        
      </li>
    
      <li class="entry-summary">
        <a href="#process_embeddings%28embeddings%3AArray%28Array%28Float32%29%29%2Cseq_id%3AInt32%3D0%29%3AInt32-instance-method" class="signature"><strong>#process_embeddings</strong>(embeddings : Array(Array(Float32)), seq_id : Int32 = <span class="n">0</span>) : Int32</a>
        
          <div class="summary"><p>Process embeddings</p></div>
        
      </li>
    
      <li class="entry-summary">
        <a href="#process_prompts%28prompts%3AArray%28String%29%29%3AArray%28Int32%29-instance-method" class="signature"><strong>#process_prompts</strong>(prompts : Array(String)) : Array(Int32)</a>
        
          <div class="summary"><p>Process multiple prompts in batch</p></div>
        
      </li>
    
      <li class="entry-summary">
        <a href="#process_tokens%28tokens%3AArray%28Int32%29%2Ccompute_logits_for_last%3ABool%3Dtrue%2Cseq_id%3AInt32%3D0%29%3AInt32-instance-method" class="signature"><strong>#process_tokens</strong>(tokens : Array(Int32), compute_logits_for_last : Bool = <span class="n">true</span>, seq_id : Int32 = <span class="n">0</span>) : Int32</a>
        
          <div class="summary"><p>Process a sequence of tokens</p></div>
        
      </li>
    
      <li class="entry-summary">
        <a href="#set_embeddings%28enabled%3ABool%29-instance-method" class="signature"><strong>#set_embeddings</strong>(enabled : Bool)</a>
        
          <div class="summary"><p>Sets whether the model is in embeddings mode or not If true, embeddings will be returned but logits will not</p></div>
        
      </li>
    
      <li class="entry-summary">
        <a href="#state%3AState-instance-method" class="signature"><strong>#state</strong> : State</a>
        
          <div class="summary"><p>Returns the state manager for this context Lazily initializes the state if it doesn't exist yet</p></div>
        
      </li>
    
      <li class="entry-summary">
        <a href="#to_unsafe%3APointer%28Llama%3A%3ALibLlama%3A%3ALlamaContext%29-instance-method" class="signature"><strong>#to_unsafe</strong> : Pointer(Llama::LibLlama::LlamaContext)</a>
        
          <div class="summary"><p>Returns the raw pointer to the underlying llama_context structure</p></div>
        
      </li>
    
  </ul>



  <div class="methods-inherited">
    
      


      


      


      


    
      


      


      


      


    
  </div>

  
  <h2>
    <a id="constructor-detail" class="anchor" href="#constructor-detail">
  <svg class="octicon-link" aria-hidden="true">
    <use href="#octicon-link"/>
  </svg>
</a>
    Constructor Detail
  </h2>
  
    <div class="entry-detail" id="new(model:Model,params=nil)-class-method">
      <div class="signature">
        
        def self.<strong>new</strong>(model : <a href="../Llama/Model.html">Model</a>, params = <span class="n">nil</span>)

        <a class="method-permalink" href="#new%28model%3AModel%2Cparams%3Dnil%29-class-method">#</a>
      </div>
      
        <div class="doc">
          
          <p>Creates a new Context instance for a model</p>
<p>Parameters:</p>
<ul>
<li>model: The Model to create a context for</li>
<li>params: Optional context parameters</li>
</ul>
<p>Raises:</p>
<ul>
<li>Llama::ContextError if the context cannot be created</li>
</ul>
        </div>
      
      <br/>
      <div>
        
          [<a href="https://github.com/kojix2/llama.cr/blob/55f06ea473618e52179cc122c32bbf3aaee8805d/src/llama/context.cr#L20" target="_blank">View source</a>]
        
      </div>
    </div>
  


  
  <h2>
    <a id="class-method-detail" class="anchor" href="#class-method-detail">
  <svg class="octicon-link" aria-hidden="true">
    <use href="#octicon-link"/>
  </svg>
</a>
    Class Method Detail
  </h2>
  
    <div class="entry-detail" id="default_params:LibLlama::LlamaContextParams-class-method">
      <div class="signature">
        
        def self.<strong>default_params</strong> : LibLlama::LlamaContextParams

        <a class="method-permalink" href="#default_params%3ALibLlama%3A%3ALlamaContextParams-class-method">#</a>
      </div>
      
        <div class="doc">
          
          <p>Returns the default context parameters</p>
<p>Returns:</p>
<ul>
<li>Default context parameters</li>
</ul>
        </div>
      
      <br/>
      <div>
        
          [<a href="https://github.com/kojix2/llama.cr/blob/55f06ea473618e52179cc122c32bbf3aaee8805d/src/llama/context.cr#L8" target="_blank">View source</a>]
        
      </div>
    </div>
  


  

  
  <h2>
    <a id="instance-method-detail" class="anchor" href="#instance-method-detail">
  <svg class="octicon-link" aria-hidden="true">
    <use href="#octicon-link"/>
  </svg>
</a>
    Instance Method Detail
  </h2>
  
    <div class="entry-detail" id="chat(messages:Array(ChatMessage),max_tokens:Int32=128,temperature:Float32=0.8,template:String|Nil=nil):String-instance-method">
      <div class="signature">
        
        def <strong>chat</strong>(messages : Array(<a href="../Llama/ChatMessage.html">ChatMessage</a>), max_tokens : Int32 = <span class="n">128</span>, temperature : Float32 = <span class="n">0.8</span>, template : String | Nil = <span class="n">nil</span>) : String

        <a class="method-permalink" href="#chat%28messages%3AArray%28ChatMessage%29%2Cmax_tokens%3AInt32%3D128%2Ctemperature%3AFloat32%3D0.8%2Ctemplate%3AString%7CNil%3Dnil%29%3AString-instance-method">#</a>
      </div>
      
        <div class="doc">
          
          <p>Generates a response in a chat conversation</p>
<p>Parameters:</p>
<ul>
<li>messages: Array of chat messages</li>
<li>max_tokens: Maximum number of tokens to generate</li>
<li>temperature: Sampling temperature</li>
<li>template: Optional chat template (nil to use model's default)</li>
</ul>
<p>Returns:</p>
<ul>
<li>The generated response text</li>
</ul>
<p>Raises:</p>
<ul>
<li>ArgumentError if parameters are invalid</li>
<li>Llama::ContextError if text generation fails</li>
<li>Llama::TokenizationError if the prompt cannot be tokenized</li>
</ul>
        </div>
      
      <br/>
      <div>
        
          [<a href="https://github.com/kojix2/llama.cr/blob/55f06ea473618e52179cc122c32bbf3aaee8805d/src/llama/context.cr#L80" target="_blank">View source</a>]
        
      </div>
    </div>
  
    <div class="entry-detail" id="cleanup-instance-method">
      <div class="signature">
        
        def <strong>cleanup</strong>

        <a class="method-permalink" href="#cleanup-instance-method">#</a>
      </div>
      
        <div class="doc">
          
          <p>Explicitly clean up resources
This can be called manually to release resources before garbage collection</p>
        </div>
      
      <br/>
      <div>
        
          [<a href="https://github.com/kojix2/llama.cr/blob/55f06ea473618e52179cc122c32bbf3aaee8805d/src/llama/context.cr#L54" target="_blank">View source</a>]
        
      </div>
    </div>
  
    <div class="entry-detail" id="decode(batch:LibLlama::LlamaBatch|Batch):Int32-instance-method">
      <div class="signature">
        
        def <strong>decode</strong>(batch : LibLlama::LlamaBatch | <a href="../Llama/Batch.html">Batch</a>) : Int32

        <a class="method-permalink" href="#decode%28batch%3ALibLlama%3A%3ALlamaBatch%7CBatch%29%3AInt32-instance-method">#</a>
      </div>
      
        <div class="doc">
          
          <p>Processes a batch of tokens with the decoder part of the model</p>
<p>Parameters:</p>
<ul>
<li>batch: The batch to process (can be a LibLlama::LlamaBatch or a Batch instance)</li>
</ul>
<p>Returns:</p>
<ul>
<li>0 on success</li>
<li>1 if no KV slot was found for the batch</li>
<li>&lt; 0 on error</li>
</ul>
<p>Raises:</p>
<ul>
<li>Llama::BatchError on error</li>
</ul>
        </div>
      
      <br/>
      <div>
        
          [<a href="https://github.com/kojix2/llama.cr/blob/55f06ea473618e52179cc122c32bbf3aaee8805d/src/llama/context.cr#L332" target="_blank">View source</a>]
        
      </div>
    </div>
  
    <div class="entry-detail" id="encode(batch:LibLlama::LlamaBatch|Batch):Int32-instance-method">
      <div class="signature">
        
        def <strong>encode</strong>(batch : LibLlama::LlamaBatch | <a href="../Llama/Batch.html">Batch</a>) : Int32

        <a class="method-permalink" href="#encode%28batch%3ALibLlama%3A%3ALlamaBatch%7CBatch%29%3AInt32-instance-method">#</a>
      </div>
      
        <div class="doc">
          
          <p>Processes a batch of tokens with the encoder part of the model</p>
<p>This function is used for encoder-decoder models to encode the input
before generating text with the decoder.</p>
<p>Parameters:</p>
<ul>
<li>batch: The batch to process (can be a LibLlama::LlamaBatch or a Batch instance)</li>
</ul>
<p>Returns:</p>
<ul>
<li>0 on success</li>
<li>&lt; 0 on error</li>
</ul>
<p>Raises:</p>
<ul>
<li>Llama::BatchError on error</li>
</ul>
        </div>
      
      <br/>
      <div>
        
          [<a href="https://github.com/kojix2/llama.cr/blob/55f06ea473618e52179cc122c32bbf3aaee8805d/src/llama/context.cr#L304" target="_blank">View source</a>]
        
      </div>
    </div>
  
    <div class="entry-detail" id="finalize-instance-method">
      <div class="signature">
        
        def <strong>finalize</strong>

        <a class="method-permalink" href="#finalize-instance-method">#</a>
      </div>
      
        <div class="doc">
          
          <p>Frees the resources associated with this context</p>
        </div>
      
      <br/>
      <div>
        
          [<a href="https://github.com/kojix2/llama.cr/blob/55f06ea473618e52179cc122c32bbf3aaee8805d/src/llama/context.cr#L587" target="_blank">View source</a>]
        
      </div>
    </div>
  
    <div class="entry-detail" id="generate(prompt:String,max_tokens:Int32=128,temperature:Float32=0.8):String-instance-method">
      <div class="signature">
        
        def <strong>generate</strong>(prompt : String, max_tokens : Int32 = <span class="n">128</span>, temperature : Float32 = <span class="n">0.8</span>) : String

        <a class="method-permalink" href="#generate%28prompt%3AString%2Cmax_tokens%3AInt32%3D128%2Ctemperature%3AFloat32%3D0.8%29%3AString-instance-method">#</a>
      </div>
      
        <div class="doc">
          
          <p>Generates text from a prompt</p>
<p>Parameters:</p>
<ul>
<li>prompt: The input prompt</li>
<li>max_tokens: Maximum number of tokens to generate (must be positive)</li>
<li>temperature: Sampling temperature (0.0 = greedy, 1.0 = more random)</li>
</ul>
<p>Returns:</p>
<ul>
<li>The generated text</li>
</ul>
<p>Raises:</p>
<ul>
<li>ArgumentError if parameters are invalid</li>
<li>Llama::ContextError if text generation fails</li>
<li>Llama::TokenizationError if the prompt cannot be tokenized</li>
</ul>
        </div>
      
      <br/>
      <div>
        
          [<a href="https://github.com/kojix2/llama.cr/blob/55f06ea473618e52179cc122c32bbf3aaee8805d/src/llama/context.cr#L381" target="_blank">View source</a>]
        
      </div>
    </div>
  
    <div class="entry-detail" id="generate_with_sampler(prompt:String,sampler:SamplerChain,max_tokens:Int32=128):String-instance-method">
      <div class="signature">
        
        def <strong>generate_with_sampler</strong>(prompt : String, sampler : <a href="../Llama/SamplerChain.html">SamplerChain</a>, max_tokens : Int32 = <span class="n">128</span>) : String

        <a class="method-permalink" href="#generate_with_sampler%28prompt%3AString%2Csampler%3ASamplerChain%2Cmax_tokens%3AInt32%3D128%29%3AString-instance-method">#</a>
      </div>
      
        <div class="doc">
          
          <p>Generates text using a sampler chain</p>
<p>Parameters:</p>
<ul>
<li>prompt: The input prompt</li>
<li>sampler: The sampler chain to use</li>
<li>max_tokens: Maximum number of tokens to generate (must be positive)</li>
</ul>
<p>Returns:</p>
<ul>
<li>The generated text</li>
</ul>
<p>Raises:</p>
<ul>
<li>ArgumentError if parameters are invalid</li>
<li>Llama::ContextError if text generation fails</li>
<li>Llama::TokenizationError if the prompt cannot be tokenized</li>
<li>Llama::SamplingError if sampling fails</li>
</ul>
        </div>
      
      <br/>
      <div>
        
          [<a href="https://github.com/kojix2/llama.cr/blob/55f06ea473618e52179cc122c32bbf3aaee8805d/src/llama/context.cr#L259" target="_blank">View source</a>]
        
      </div>
    </div>
  
    <div class="entry-detail" id="get_embeddings:Array(Float32)|Nil-instance-method">
      <div class="signature">
        
        def <strong>get_embeddings</strong> : Array(Float32) | Nil

        <a class="method-permalink" href="#get_embeddings%3AArray%28Float32%29%7CNil-instance-method">#</a>
      </div>
      
        <div class="doc">
          
          <p>Gets all output token embeddings
Only available when embeddings mode is enabled</p>
<p>Returns:</p>
<ul>
<li>An array of embeddings, or nil if embeddings are not available</li>
</ul>
<p>Raises:</p>
<ul>
<li>Llama::ContextError if embeddings mode is not enabled</li>
</ul>
        </div>
      
      <br/>
      <div>
        
          [<a href="https://github.com/kojix2/llama.cr/blob/55f06ea473618e52179cc122c32bbf3aaee8805d/src/llama/context.cr#L618" target="_blank">View source</a>]
        
      </div>
    </div>
  
    <div class="entry-detail" id="get_embeddings_ith(i:Int32):Array(Float32)|Nil-instance-method">
      <div class="signature">
        
        def <strong>get_embeddings_ith</strong>(i : Int32) : Array(Float32) | Nil

        <a class="method-permalink" href="#get_embeddings_ith%28i%3AInt32%29%3AArray%28Float32%29%7CNil-instance-method">#</a>
      </div>
      
        <div class="doc">
          
          <p>Gets the embeddings for a specific token</p>
<p>Parameters:</p>
<ul>
<li>i: The token index (negative indices can be used to access in reverse order)</li>
</ul>
<p>Returns:</p>
<ul>
<li>An array of embedding values, or nil if not available</li>
</ul>
<p>Raises:</p>
<ul>
<li>Llama::ContextError if embeddings mode is not enabled</li>
</ul>
        </div>
      
      <br/>
      <div>
        
          [<a href="https://github.com/kojix2/llama.cr/blob/55f06ea473618e52179cc122c32bbf3aaee8805d/src/llama/context.cr#L649" target="_blank">View source</a>]
        
      </div>
    </div>
  
    <div class="entry-detail" id="get_embeddings_seq(seq_id:Int32):Array(Float32)|Nil-instance-method">
      <div class="signature">
        
        def <strong>get_embeddings_seq</strong>(seq_id : Int32) : Array(Float32) | Nil

        <a class="method-permalink" href="#get_embeddings_seq%28seq_id%3AInt32%29%3AArray%28Float32%29%7CNil-instance-method">#</a>
      </div>
      
        <div class="doc">
          
          <p>Gets the embeddings for a specific sequence</p>
<p>Parameters:</p>
<ul>
<li>seq_id: The sequence ID</li>
</ul>
<p>Returns:</p>
<ul>
<li>An array of embedding values, or nil if not available</li>
</ul>
<p>Raises:</p>
<ul>
<li>Llama::ContextError if embeddings mode is not enabled</li>
</ul>
        </div>
      
      <br/>
      <div>
        
          [<a href="https://github.com/kojix2/llama.cr/blob/55f06ea473618e52179cc122c32bbf3aaee8805d/src/llama/context.cr#L675" target="_blank">View source</a>]
        
      </div>
    </div>
  
    <div class="entry-detail" id="kv_cache:KvCache-instance-method">
      <div class="signature">
        
        def <strong>kv_cache</strong> : <a href="../Llama/KvCache.html">KvCache</a>

        <a class="method-permalink" href="#kv_cache%3AKvCache-instance-method">#</a>
      </div>
      
        <div class="doc">
          
          <p>Returns the KV cache for this context
Lazily initializes the KV cache if it doesn't exist yet</p>
        </div>
      
      <br/>
      <div>
        
          [<a href="https://github.com/kojix2/llama.cr/blob/55f06ea473618e52179cc122c32bbf3aaee8805d/src/llama/context.cr#L42" target="_blank">View source</a>]
        
      </div>
    </div>
  
    <div class="entry-detail" id="logits:Pointer(Float32)-instance-method">
      <div class="signature">
        
        def <strong>logits</strong> : Pointer(Float32)

        <a class="method-permalink" href="#logits%3APointer%28Float32%29-instance-method">#</a>
      </div>
      
        <div class="doc">
          
          <p>Gets the logits for the last token</p>
<p>Returns:</p>
<ul>
<li>A pointer to the logits array</li>
</ul>
        </div>
      
      <br/>
      <div>
        
          [<a href="https://github.com/kojix2/llama.cr/blob/55f06ea473618e52179cc122c32bbf3aaee8805d/src/llama/context.cr#L352" target="_blank">View source</a>]
        
      </div>
    </div>
  
    <div class="entry-detail" id="pooling_type:LibLlama::LlamaPoolingType-instance-method">
      <div class="signature">
        
        def <strong>pooling_type</strong> : LibLlama::LlamaPoolingType

        <a class="method-permalink" href="#pooling_type%3ALibLlama%3A%3ALlamaPoolingType-instance-method">#</a>
      </div>
      
        <div class="doc">
          
          <p>Gets the pooling type used for embeddings</p>
<p>Returns:</p>
<ul>
<li>The pooling type as a PoolingType enum</li>
</ul>
        </div>
      
      <br/>
      <div>
        
          [<a href="https://github.com/kojix2/llama.cr/blob/55f06ea473618e52179cc122c32bbf3aaee8805d/src/llama/context.cr#L606" target="_blank">View source</a>]
        
      </div>
    </div>
  
    <div class="entry-detail" id="process_embeddings(embeddings:Array(Array(Float32)),seq_id:Int32=0):Int32-instance-method">
      <div class="signature">
        
        def <strong>process_embeddings</strong>(embeddings : Array(Array(Float32)), seq_id : Int32 = <span class="n">0</span>) : Int32

        <a class="method-permalink" href="#process_embeddings%28embeddings%3AArray%28Array%28Float32%29%29%2Cseq_id%3AInt32%3D0%29%3AInt32-instance-method">#</a>
      </div>
      
        <div class="doc">
          
          <p>Process embeddings</p>
<p>Parameters:</p>
<ul>
<li>embeddings: Array of embedding vectors</li>
<li>seq_id: Sequence ID to use for all embeddings</li>
</ul>
<p>Returns:</p>
<ul>
<li>The result of the decode operation (0 on success)</li>
</ul>
<p>Raises:</p>
<ul>
<li>Llama::BatchError on error</li>
</ul>
        </div>
      
      <br/>
      <div>
        
          [<a href="https://github.com/kojix2/llama.cr/blob/55f06ea473618e52179cc122c32bbf3aaee8805d/src/llama/context.cr#L209" target="_blank">View source</a>]
        
      </div>
    </div>
  
    <div class="entry-detail" id="process_prompts(prompts:Array(String)):Array(Int32)-instance-method">
      <div class="signature">
        
        def <strong>process_prompts</strong>(prompts : Array(String)) : Array(Int32)

        <a class="method-permalink" href="#process_prompts%28prompts%3AArray%28String%29%29%3AArray%28Int32%29-instance-method">#</a>
      </div>
      
        <div class="doc">
          
          <p>Process multiple prompts in batch</p>
<p>Parameters:</p>
<ul>
<li>prompts: Array of text prompts to process</li>
<li>compute_logits_for_last: Whether to compute logits only for the last token of each prompt</li>
</ul>
<p>Returns:</p>
<ul>
<li>Array of decode operation results (0 on success)</li>
</ul>
<p>Raises:</p>
<ul>
<li>Llama::BatchError on error</li>
<li>Llama::TokenizationError if a prompt cannot be tokenized</li>
</ul>
        </div>
      
      <br/>
      <div>
        
          [<a href="https://github.com/kojix2/llama.cr/blob/55f06ea473618e52179cc122c32bbf3aaee8805d/src/llama/context.cr#L160" target="_blank">View source</a>]
        
      </div>
    </div>
  
    <div class="entry-detail" id="process_tokens(tokens:Array(Int32),compute_logits_for_last:Bool=true,seq_id:Int32=0):Int32-instance-method">
      <div class="signature">
        
        def <strong>process_tokens</strong>(tokens : Array(Int32), compute_logits_for_last : Bool = <span class="n">true</span>, seq_id : Int32 = <span class="n">0</span>) : Int32

        <a class="method-permalink" href="#process_tokens%28tokens%3AArray%28Int32%29%2Ccompute_logits_for_last%3ABool%3Dtrue%2Cseq_id%3AInt32%3D0%29%3AInt32-instance-method">#</a>
      </div>
      
        <div class="doc">
          
          <p>Process a sequence of tokens</p>
<p>Parameters:</p>
<ul>
<li>tokens: Array of token IDs to process</li>
<li>compute_logits_for_last: Whether to compute logits only for the last token</li>
<li>seq_id: Sequence ID to use for all tokens</li>
</ul>
<p>Returns:</p>
<ul>
<li>The result of the decode operation (0 on success)</li>
</ul>
<p>Raises:</p>
<ul>
<li>Llama::BatchError on error</li>
</ul>
        </div>
      
      <br/>
      <div>
        
          [<a href="https://github.com/kojix2/llama.cr/blob/55f06ea473618e52179cc122c32bbf3aaee8805d/src/llama/context.cr#L139" target="_blank">View source</a>]
        
      </div>
    </div>
  
    <div class="entry-detail" id="set_embeddings(enabled:Bool)-instance-method">
      <div class="signature">
        
        def <strong>set_embeddings</strong>(enabled : Bool)

        <a class="method-permalink" href="#set_embeddings%28enabled%3ABool%29-instance-method">#</a>
      </div>
      
        <div class="doc">
          
          <p>Sets whether the model is in embeddings mode or not
If true, embeddings will be returned but logits will not</p>
<p>Parameters:</p>
<ul>
<li>enabled: Whether to enable embeddings mode</li>
</ul>
        </div>
      
      <br/>
      <div>
        
          [<a href="https://github.com/kojix2/llama.cr/blob/55f06ea473618e52179cc122c32bbf3aaee8805d/src/llama/context.cr#L598" target="_blank">View source</a>]
        
      </div>
    </div>
  
    <div class="entry-detail" id="state:State-instance-method">
      <div class="signature">
        
        def <strong>state</strong> : <a href="../Llama/State.html">State</a>

        <a class="method-permalink" href="#state%3AState-instance-method">#</a>
      </div>
      
        <div class="doc">
          
          <p>Returns the state manager for this context
Lazily initializes the state if it doesn't exist yet</p>
        </div>
      
      <br/>
      <div>
        
          [<a href="https://github.com/kojix2/llama.cr/blob/55f06ea473618e52179cc122c32bbf3aaee8805d/src/llama/context.cr#L48" target="_blank">View source</a>]
        
      </div>
    </div>
  
    <div class="entry-detail" id="to_unsafe:Pointer(Llama::LibLlama::LlamaContext)-instance-method">
      <div class="signature">
        
        def <strong>to_unsafe</strong> : Pointer(Llama::LibLlama::LlamaContext)

        <a class="method-permalink" href="#to_unsafe%3APointer%28Llama%3A%3ALibLlama%3A%3ALlamaContext%29-instance-method">#</a>
      </div>
      
        <div class="doc">
          
          <p>Returns the raw pointer to the underlying llama_context structure</p>
        </div>
      
      <br/>
      <div>
        
          [<a href="https://github.com/kojix2/llama.cr/blob/55f06ea473618e52179cc122c32bbf3aaee8805d/src/llama/context.cr#L582" target="_blank">View source</a>]
        
      </div>
    </div>
  



</div>

</body>
</html>
